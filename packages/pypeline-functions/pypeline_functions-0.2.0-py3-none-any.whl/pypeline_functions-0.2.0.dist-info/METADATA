Metadata-Version: 2.4
Name: pypeline-functions
Version: 0.2.0
Summary: A compiliation of data pipeline scripts in Python
Author-email: Miguel Habana <mighabana@gmail.com>
License: MIT
License-File: LICENSE.txt
Keywords: ELT,data pipelines,dlt,pypeline
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries
Requires-Python: >=3.11
Requires-Dist: dlt>=1.0.0
Requires-Dist: fastparquet>=2024.5.0
Requires-Dist: feedparser>=6.0.11
Requires-Dist: google-auth>=2.34.0
Requires-Dist: google-cloud-bigquery-storage>=2.26.0
Requires-Dist: google-cloud-bigquery>=3.25.0
Requires-Dist: google-cloud-storage>=2.18.0
Requires-Dist: pandas>=2.2.0
Requires-Dist: psycopg2-binary>=2.9.10
Requires-Dist: pydantic>=2.9.0
Requires-Dist: python-magic>=0.4.27
Requires-Dist: stream-unzip>=0.0.90
Provides-Extra: dev
Requires-Dist: memray; extra == 'dev'
Requires-Dist: pre-commit; extra == 'dev'
Requires-Dist: ruff; extra == 'dev'
Description-Content-Type: text/markdown

# About

This is a compilation of my data pipeline scripts written in Python.

### Conventions

Each pipeline function is an executable python file that accepts flags to modify the specific configurations of the pipeline (i.e. MSSQL DB Name, GCS Bucket Name).

When loading data from a third-party source you can set the temporary destination of the data to the `data/` folder. After the data has been successfully ingested remove the file from the data folder.

### Folder Structure

- config/
    - contains any specific configurations that need to be modified within the Docker container
-  data/
    - a temporary landing zone for any data that is ingested from a third-party source
- functions/
    - contains all pipeline functions
- functions/utils/
    - contains all reusable code and can be organized futher as either a Source (where data is pulled from), or a Target (where data is placed)

# Setup

[//]: # (TODO: Review environment and dependency management best practices when using Hatch)

1. Create and activate a python virtual environment.

```
python3 -m venv venv
```

```
source /venv/bin/activate
```


2. Install python dependencies

```
pip install -r requirements.txt
```