import asyncio
import os
from datetime import datetime
import platform
from . import phishdb_schema
from . import phishdb_layer
import tldextract
from pathlib import Path
import hashlib
from .phish_logger import Phish_Logger
from .additional_info import whois_info
import importlib
import pyppeteer

import shutil

dir_path = os.path.abspath(os.path.dirname(__file__))

logger=Phish_Logger.get_phish_logger('phish_logs')


help_desc = '''
Crawler, visits a url (Chrome Browser) or executes a HTML code in the browser, follows redirects and logs data about the crawled infrastructure
'''

# We need to replace the network_manager within pyppeteer
pyppeteer_path = os.path.dirname(pyppeteer.__file__)
source_file = os.path.join(dir_path,"network_manager.py")
destination_file = os.path.join(pyppeteer_path, "network_manager.py")
shutil.copy(source_file, destination_file)
importlib.reload(pyppeteer)



async def crawl(htmlfile=None,phish_url=None,phish_id: str=None,source_type=None,emailrecord=None,session=None):
    if phish_url:
        source=phish_url.replace('\x00', '')
    elif htmlfile:
        source=htmlfile.decode(errors='ignore').replace('\x00', '') if isinstance(htmlfile,bytes) else htmlfile.replace('\x00', '')

    #Create the Crawler Instance, and link it to the Malicious Email Message
    crawler_instance=phishdb_schema.Crawler_Instance(source_type=source_type,crawling_time=datetime.now(),source=source)
    phishdb_layer.add_element(crawler_instance,session)
    phishdb_layer.add_element(phishdb_schema.Generates_Crawler_Instance(malmsg=emailrecord,instance=crawler_instance),session)


    if platform.system()=='Windows':
        browser_path=r"C:\Program Files\Google\Chrome\Application\chrome.exe"
    elif platform.system()=='Linux':
        browser_path='/usr/bin/google-chrome'

    browser = await pyppeteer.launch({'headless':False,
                            'executablePath': browser_path,
                            'args': [
                                '--disable-blink-features=AutomationControlled',
                                '--window-size=1366,768'
                                ]
                            },
                           handleSIGINT=False,
                           handleSIGTERM=False,
                           handleSIGHUP=False
                           )

    #pup_pages = await browser.pages()
    #pup_page = pup_pages[0]
    pup_page=await browser.newPage()

    async def handle_request(rq):
        try:
            fqdn= tldextract.extract(rq.url).fqdn
            request_record=phishdb_layer.request_exists(rq,crawler_instance,session)
            if fqdn:
                fqdn_record=phishdb_layer.domain_exists(fqdn,emailrecord,session)
                phishdb_layer.requests_domain(request_record,fqdn_record,session)
                phishdb_layer.umbrella_enrichment(fqdn,fqdn_record,session)
                w=whois_info(fqdn)
                if w:
                    phishdb_layer.create_whoisrecord(w,fqdn_record,emailrecord,session)

            try:
                if rq.response:
                    #This request is generated by a redirection response
                    redirectresponse_record=phishdb_layer.check_response(rq.response,session)
                    phishdb_layer.add_redirection(redirectresponse_record,request_record,session)

            except Exception as e:
                logger.error('[!] handle_request: Exception in handling redirection %s :: %s',str(phish_id), str(e))
        except Exception as e:
            logger.error('[!] handle_request: Exception %s :: %s',str(phish_id),str(e))
            pass




    async def handle_response(res):
        try:
            request=phishdb_layer.request_exists(res.request,crawler_instance,session)
        except Exception as e:
            logger.error('[!] handle_response: Exception in mapping request from response %s :: %s',str(phish_id),str(e))
            pass
        try:
            fqdn=tldextract.extract(res.url).fqdn
            if fqdn:
                fqdn_record=phishdb_layer.domain_exists(fqdn,emailrecord,session)
                phishdb_layer.umbrella_enrichment(fqdn,fqdn_record,session)
                response= phishdb_layer.add_response(res,request,fqdn_record,session)
                w=whois_info(fqdn)
                if w:
                    phishdb_layer.create_whoisrecord(w,fqdn_record,emailrecord,session)
                security_details=res.securityDetails
                if security_details !={}:
                    phishdb_layer.check_domaincertificate(fqdn_record,security_details,emailrecord,session)
            else:
                response= phishdb_layer.add_response(res,request,None,session)

            try:
                response_body= await res.buffer()
                if isinstance(response_body,str):
                    response_body=response_body.encode()
                bodyhash=hashlib.sha256(response_body).hexdigest()
                if bodyhash and response:
                    phishdb_layer.add_responsebody(bodyhash,response_body,response,session)

            except Exception:
                pass

        except Exception as e:
            logger.error('[!] handle_response: Exception in mapping response %s :: %s', str(phish_id),str(e))
            #domain_certificate=phishdb_schema.Domain_Certificate(Error=str(e))





    ### Intercept and handle requests and responses from the pages
    await pup_page.setRequestInterception(False) #remove Cache Control and Pragma from headers
    pup_page.on('request', lambda req: asyncio.ensure_future(handle_request(req)))
    pup_page.on('response',  lambda res: asyncio.ensure_future(handle_response(res)))



    ### Visit the page
    try:
        #await asyncio.sleep(3)
        await pup_page.setViewport({'width':1366, 'height':768})
        if phish_url:
            logger.info('[+] crawl_page_info %s :: Visiting URL :: %s',str(phish_id),phish_url)
            await pup_page.goto(phish_url)
        elif htmlfile:
            logger.info('[+] crawl_page_info :: opening the HTML file in a browser :: phish_id: %s', str(phish_id))
            await pup_page.setContent(htmlfile)
        await pup_page.mouse.move(100, 100)
        await pup_page.mouse.move(200, 200)

    except Exception:
        pass
        #logger.error('[!] Exception while setting page: %s', str(e))


    try:
        try:
            ###Wait for the page to navigate to a new URL or to reload
            try:
                page_source=await pup_page.content()
                phishdb_layer.add_page_source(page_source,crawler_instance,session)
            except Exception:
                pass

            try:
                navigationPromise = asyncio.ensure_future(pup_page.waitForNavigation({'timeout':5000}))
                #await pup_page.waitForNavigation({'timeout':5000}) #timeout 30s
                try:
                    page_source=await pup_page.content()
                    phishdb_layer.add_page_source(page_source,crawler_instance,session)
                except Exception as e:
                    logger.error('[!] crawl_page_info :: phish_id: %s Exception [1] in page source-- %s',str(phish_id),str(e))

                await navigationPromise

            except Exception:
                pass

            try:
                page_source=await pup_page.content()
                phishdb_layer.add_page_source(page_source,crawler_instance,session)
            except Exception as e:
                logger.error('[!] crawl_page_info :: phish_id: %s Exception [2] in page source-- %s',phish_id,str(e))

            try:
                Path( os.path.join( dir_path, "screenshots")).mkdir(parents=True, exist_ok=True)
                t=datetime.now().strftime("%Y-%m-%d_%H%M%S")
                screenshot_name= phish_id+'_'+t+'.png'
                path = os.path.join(dir_path, 'screenshots', screenshot_name)
                await pup_page.screenshot({'path': path, 'fullPage':True})
                logger.info('[+] Screenshot taken!')
                page_url=pup_page.url
                phishdb_layer.add_screenshot(screenshot_name,path,crawler_instance,t,page_url,session)
            except Exception as e:
                logger.error('[!] crawl_page: %s :: Exception in screenshot :: %s',phish_id,str(e))


            #await asyncio.sleep(10)

        except Exception as e:
            logger.error('[!] Exception Occured phish_id: %s :: %s',phish_id,str(e))

    except Exception as oe:
        logger.error('[!] crawl_page_info :: phish_id: %s Exception -- %s',phish_id,str(oe))

    finally:

        logger.info('[+] crawl_page_info :: phish_id: %s : Crawling Complete. Browser Closed ',phish_id)

        try:
            await pup_page.close()
            await browser.close()
        except Exception as e:
            logger.error('[!] crawl_page_info :: phish_id: %s Exception -- %s',phish_id,str(e))

        logger.info('[+] Browser Closed!')











