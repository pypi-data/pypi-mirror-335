Metadata-Version: 2.4
Name: bs-scheduler
Version: 1.0.2
Summary: A PyTorch Dataloader compatible batch size scheduler library.
Author-email: George Stoica <george.stoica@info.uaic.ro>
Maintainer-email: George Stoica <george.stoica@info.uaic.ro>
License: BSD 3-Clause License
        
        Copyright (c) 2023, ancestor-mithril
        
        Redistribution and use in source and binary forms, with or without
        modification, are permitted provided that the following conditions are met:
        
        1. Redistributions of source code must retain the above copyright notice, this
           list of conditions and the following disclaimer.
        
        2. Redistributions in binary form must reproduce the above copyright notice,
           this list of conditions and the following disclaimer in the documentation
           and/or other materials provided with the distribution.
        
        3. Neither the name of the copyright holder nor the names of its
           contributors may be used to endorse or promote products derived from
           this software without specific prior written permission.
        
        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
        FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
        DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
        SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
        CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
        OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
        
Project-URL: Repository, https://github.com/ancestor-mithril/bs-scheduler
Project-URL: Issues, https://github.com/ancestor-mithril/bs-scheduler/issues
Keywords: deep learning,pytorch,batch size scheduler,dynamic batch size
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: BSD License
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=1.1.0
Provides-Extra: docs
Requires-Dist: mkdocs; extra == "docs"
Requires-Dist: mkdocstrings[python]; extra == "docs"
Requires-Dist: mkdocs-material; extra == "docs"
Provides-Extra: dev
Requires-Dist: matplotlib; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Dynamic: license-file

# bs-scheduler

A Batch Size Scheduler library compatible with PyTorch DataLoaders.

*** 

## Documentation

* [API Reference](https://ancestor-mithril.github.io/bs-scheduler).

* [Examples](https://ancestor-mithril.github.io/bs-scheduler/tutorials).

<!--For Release Notes, see TODO. -->

***

## Why use a Batch Size Scheduler?

* Using a big batch size has several advantages:
  * Better hardware utilization.
  * Enhanced parallelism.
  * Faster training.
* However, using a big batch size from the start may lead to a generalization gap.
* Therefore, the solution is to gradually increase the batch size, similar to a learning rate decay policy.
* See [Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489).


## Available Schedulers

### Batch Size Schedulers

1. `LambdaBS` - sets the batch size to the base batch size times a given lambda.
2. `MultiplicativeBS` - sets the batch size to the current batch size times a given lambda.
3. `StepBS` - multiplies the batch size with a given factor at a given number of steps.
4. `MultiStepBS` - multiplies the batch size with a given factor each time a milestone is reached.
5. `ConstantBS` - multiplies the batch size by a given factor once and decreases it again to its base value after a
   given number of steps.
6. `LinearBS` - increases the batch size by a linearly changing multiplicative factor for a given number of steps.
7. `ExponentialBS` - increases the batch size by a given $\gamma$ each step.
8. `PolynomialBS` - increases the batch size using a polynomial function in a given number of steps.
9. `CosineAnnealingBS` - increases the batch size to a maximum batch size and decreases it again following a cyclic
   cosine curve.
10. `IncreaseBSOnPlateau` - increases the batch size each time a given metric has stopped improving for a given number
    of steps.
11. `CyclicBS` - cycles the batch size between two boundaries with a constant frequency, while also scaling the
    distance between boundaries.
12. `CosineAnnealingBSWithWarmRestarts` - increases the batch size to a maximum batch size following a cosine curve,
    then restarts while also scaling the number of iterations until the next restart.
13. `OneCycleBS` - decreases the batch size to a minimum batch size then increases it to a given maximum batch size,
    following a linear or cosine annealing strategy.
14. `SequentialBS` - calls a list of schedulers sequentially given a list of milestone points which reflect which
    scheduler should be called when.
15. `ChainedBSScheduler` - chains a list of batch size schedulers and calls them together each step.

<!--

## Quick Start

TODO.

-->

## Installation

Please install [PyTorch](https://github.com/pytorch/pytorch) first before installing this repository.

```
pip install bs-scheduler
```

## Licensing

The library is licensed under the [BSD-3-Clause license](LICENSE).

## Citation

To be added...

<!--Citation: TODO. -->
