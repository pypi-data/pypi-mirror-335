{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcc13af",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\" id=\"part-1\">Getting Started with NLTK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60428372-b5e4-4923-aeb9-9740f224667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "#!conda install anaconda::nltk --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd4405",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# before import you should install nltk\n",
    "import nltk\n",
    "# download all datasets in nltk\n",
    "download_dir = '/home/jovyan/work/nltk_data/'\n",
    "nltk.data.path.append('/home/jovyan/work/nltk_data/')\n",
    "nltk.download('all', download_dir=download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebd727",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# from NLTK's book module, load all items.\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f06be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# enter their names at the Python prompt\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084b9cb",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Frequency Distributions</h2>\n",
    "\n",
    "Frequency distribution tells us the frequency of each vocabulary in the text. It is a \"distribution\" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items. Since we often need frequency distributions in language processing, NLTK provides built-in support for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: create a frequency distribution for text1 (Moby-Dick text)\n",
    "\n",
    "# A frequency distribution for the outcomes of an experiment. A frequency distribution \n",
    "# records the number of times each outcome of an experiment has occurred. For example, \n",
    "# a frequency distribution could be used to record the frequency of each word type in \n",
    "# a document. \n",
    "\n",
    "fdist1 = FreqDist(text1)\n",
    "print(fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find the 50 most frequent words of text1\n",
    "print(fdist1.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find the frequency of word 'whale' in text1\n",
    "print(fdist1['whale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff8ae1-f634-4c21-976c-7a84f458f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::matplotlib --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: plot Probability Density Function\n",
    "fdist1.plot(50, cumulative=False)\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d073e15",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Fine-grained Selection of Words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: look at the long words of the book Moby-Dick (text1)\n",
    "#       find all words that have at least 15 chars.\n",
    "V = set(text1)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "print(sorted(long_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8541128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: look at the long words of the Inaugural Address Corpus (text4)\n",
    "#       find all words that have at least 15 chars.\n",
    "V = set(text4)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "print(sorted(long_words))\n",
    "\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a53aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find all long words of text5\n",
    "print(sorted([w for w in set(text5) if len(w) > 15]))\n",
    "\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ec0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find frequently occurring long words. \n",
    "fdist5 = FreqDist(text5)\n",
    "print(sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7))\n",
    "\n",
    "# do you find ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb493a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find hapaxes (hapax legomenon, 孤立词，文本中出现一次)\n",
    "fdist1 = FreqDist(text1)\n",
    "hapax = fdist1.hapaxes()\n",
    "print(hapax[:10])\n",
    "\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab106692",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Collocations and Bigrams</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: generate bigrams from a word list\n",
    "# A collocation is a sequence of words that occur together unusually often. \n",
    "# Thus red wine is a collocation,  whereas the wine is not.\n",
    "list(bigrams(['more', 'is', 'said', 'than', 'done']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find frequent bigrams of text4 (Inaugural Address Corpus)\n",
    "text4.collocations(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find frequent bigrams of text8 (Personals Corpus \n",
    "#       comes from personal ads posted on various online \n",
    "#       dating sites)\n",
    "text8.collocations() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1d648",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Counting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eed3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find and cluster frequency of word length\n",
    "\n",
    "# For example, we can look at the distribution of word lengths \n",
    "# in a text, by creating a FreqDist out of a long list of numbers, \n",
    "# where each number is the length of the corresponding word in the text:\n",
    "\n",
    "fdist = FreqDist(len(w) for w in text1)\n",
    "print(fdist)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: show all frequent of the different lengths of words\n",
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find a specific frequency the most frequency length\n",
    "print(fdist.max())\n",
    "print(fdist[3])\n",
    "print(fdist.freq(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(w for w in set(text1) if w.endswith('ableness'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(term for term in set(text4) if 'gnt' in term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(item for item in set(text6) if item.istitle())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(item for item in set(sent7) if item.isdigit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(w for w in set(text7) if '-' in w and 'index' in w))\n",
    "print(sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10))\n",
    "print(sorted(w for w in set(sent7) if not w.islower()))\n",
    "print(sorted(t for t in set(text2) if 'cie' in t or 'cei' in t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "captical_words = [w.upper() for w in text1]\n",
    "print(captical_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75680d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1)\n",
    "print(len(text1))\n",
    "print(len(set(text1)))\n",
    "print(len(set(word.lower() for word in text1)))\n",
    "# merge words like The the."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab38235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate numbers and punctuation from the vocabulary count by \n",
    "# filtering out any non-alphabetic items:\n",
    "print(len(set(word.lower() for word in text1 if word.isalpha())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8772ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the word type for sent1\n",
    "for token in sent1:\n",
    "    if token.islower():\n",
    "        print(f'{token:10} is a lowercase word')\n",
    "    elif token.istitle():\n",
    "        print(f'{token:10} is a titlecase word')\n",
    "    else:\n",
    "        print(f'{token:10} is punctuation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of cie and cei words, \n",
    "# then we loop over each item and print it. \n",
    "tricky = sorted(w for w in set(text2) if 'cie' in w or 'cei' in w)\n",
    "for word in tricky:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1a0b9",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\" id=\"part-2\">Searching Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae10b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f37d38-2354-44f9-bd07-8de74d1c7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: look up the context of word \"monstrous\" in Moby Dick (text1) \n",
    "print('-'*17)\n",
    "text1.concordance(\"monstrous\")\n",
    "print('-'*17)\n",
    "text1.concordance(\"fudan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05889bfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: search Sense and Sensibility (text2) for the word \"affection\"\n",
    "print('-'*17)\n",
    "text2.concordance(\"affection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73aa758",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: search the book of Genesis (text3) to find out how long some people lived\n",
    "text3.concordance(\"lived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22b96b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: look at text4, the Inaugural Address Corpus, to see examples of English going back to 1789, \n",
    "print('-'*17)\n",
    "text4.concordance(\"nation\")\n",
    "print('-'*17)\n",
    "text4.concordance(\"terror\")\n",
    "# see how these words have been used differently over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8c695",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: find lol context in text5, the NPS Chat Corpus: \n",
    "#       search this for unconventional words like im, ur, lol.\n",
    "text5.concordance(\"lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e8559",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: find similar context,\n",
    "#.      we saw that monstrous occurred in contexts such as \n",
    "#.      the ___ pictures and a ___ size. What other words \n",
    "#.      appear in a similar range of contexts?\n",
    "text1.concordance(\"monstrous\")\n",
    "print('-'*17)\n",
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cfa28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: find similar context of monstrous in text2\n",
    "text2.concordance(\"monstrous\")\n",
    "print('-'*17)\n",
    "text2.similar(\"monstrous\")\n",
    "\n",
    "# Observe that we get different results for different texts. \n",
    "# Austen uses this word quite differently from Melville; \n",
    "# for her, monstrous has positive connotations, and sometimes \n",
    "# functions as an intensifier like the word very."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196f1ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: find common context of two words\n",
    "# The term \"common_contexts\" allows us to examine \n",
    "# just the contexts that are shared by two or more words\n",
    "text1.common_contexts([\"monstrous\", \"very\"])\n",
    "text2.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff1d3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Task: show the dispersion plot for Elinor, Edward, Marianne, Willoughby\n",
    "text2.dispersion_plot([\"Elinor\", \"Edward\", \"Marianne\", \"Willoughby\"])\n",
    "# TODO: There is a bug in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: generate some random text in the various styles we have just seen.\n",
    "text3.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d191a29-a170-4f85-923b-761ef7632ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::wordcloud --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ca6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: generate word cloud\n",
    "#.      you may need to install wordcloud first\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in text1]\n",
    "fd = nltk.FreqDist(words).most_common()\n",
    "wc = WordCloud(background_color='white', max_words=2000, stopwords=STOPWORDS, max_font_size=50,\n",
    "              random_state=17)\n",
    "wc.generate(' '.join(words))\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad251e-8c95-4c12-8ae0-02f9bc2e8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis = nltk.FreqDist(text1)\n",
    "filter_words = dict([(m,n) for m,n in data_analysis.items() if len(m) > 3])\n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(25,cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fcacdc",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Web and Chat Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f023d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a26cf-bce2-48c4-98b5-7ebe387939ae",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">spaCy </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9c4ca-1525-4950-a268-5141d0d8aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::spacy --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d866a7-50ea-40ef-8409-9f9243a33656",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5568b4-6fde-409f-8948-eb4b63b6eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcf0da",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Regular Expression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a739df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install anaconda::seaborn --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python, there is a built in lib re, we can import them\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c720fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Find woodchuck or Woodchuck : Disjunction\n",
    "test_str = \"This string contains Woodchuck and woodchuck.\"\n",
    "result=re.search(pattern=\"[wW]oodchuck\", string=test_str)\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[wW]ooodchuck\", string=test_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the word \"woodchuck\" in the following test string\n",
    "test_str = \"interesting links to woodchucks ! and lemurs!\"\n",
    "re.search(pattern=\"woodchuck\", string=test_str)\n",
    "\n",
    "# Find !, it follows the same way:\n",
    "print(re.search(pattern=\"!\", string=test_str))\n",
    "print(re.search(pattern=\"!!\", string=test_str))\n",
    "assert re.search(pattern=\"!!\", string=test_str) == None # match nothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any single digit in a string.\n",
    "result=re.search(pattern=r\"[0123456789]\", string=\"plenty of 7 to 5\")\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[0-9]\", string=\"plenty of 7 to 5\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6af07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation: If the caret ^ is the first symbol after [,\n",
    "# the resulting pattern is negated. For example, the pattern \n",
    "# [^a] matches any single character (including special characters) except a.\n",
    "\n",
    "# -- not an upper case letter\n",
    "print(re.search(pattern=r\"[^A-Z]\", string=\"Oyfn pripetchik\"))\n",
    "\n",
    "# -- neither 'S' nor 's'\n",
    "print(re.search(pattern=r\"[^Ss]\", string=\"I have no exquisite reason for't\"))\n",
    "\n",
    "# -- not a period\n",
    "print(re.search(pattern=r\"[^.]\", string=\"our resident Djinn\"))\n",
    "\n",
    "# -- either 'e' or '^'\n",
    "print(re.search(pattern=r\"[e^]\", string=\"look up ^ now\"))\n",
    "\n",
    "# -- the pattern ‘a^b’\n",
    "print(re.search(pattern=r'a\\^b', string=r'look up a^b now'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More disjuncations\n",
    "str1 = \"Woodchucks is another name for groundhog!\"\n",
    "result = re.search(pattern=\"groundhog|woodchuck\",string=str1)\n",
    "print(result)\n",
    "\n",
    "str1 = \"Find all woodchuckk Woodchuck Groundhog groundhogxxx!\"\n",
    "result = re.findall(pattern=\"[gG]roundhog|[Ww]oodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a33475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some special chars\n",
    "\n",
    "# ?: Optional previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou?r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# *: 0 or more of previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou*r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# +: 1 or more of previous char\n",
    "str1 = \"baa baaa baaaa baaaaa\"\n",
    "result = re.findall(pattern=\"baa+\",string=str1)\n",
    "print(result)\n",
    "# .: any char\n",
    "str1 = \"begin begun begun beg3n\"\n",
    "result = re.findall(pattern=\"beg.n\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end.\"\n",
    "result = re.findall(pattern=\"\\.$\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end? The end. #t\"\n",
    "result = re.findall(pattern=\".$\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all \"the\" in a raw text.\n",
    "text = \"If two sequences in an alignment share a common ancestor, \\\n",
    "mismatches can be interpreted as point mutations and gaps as indels (that \\\n",
    "is, insertion or deletion mutations) introduced in one or both lineages in \\\n",
    "the time since they diverged from one another. In sequence alignments of \\\n",
    "proteins, the degree of similarity between amino acids occupying a \\\n",
    "particular position in the sequence can be interpreted as a rough \\\n",
    "measure of how conserved a particular region or sequence motif is \\\n",
    "among lineages. The absence of substitutions, or the presence of \\\n",
    "only very conservative substitutions (that is, the substitution of \\\n",
    "amino acids whose side chains have similar biochemical properties) in \\\n",
    "a particular region of the sequence, suggest [3] that this region has \\\n",
    "structural or functional importance. Although DNA and RNA nucleotide bases \\\n",
    "are more similar to each other than are amino acids, the conservation of \\\n",
    "base pairs can indicate a similar functional or structural role.\"\n",
    "matches = re.findall(\"[^a-zA-Z][tT]he[^a-zA-Z]\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ec072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer way is to do the following\n",
    "\n",
    "matches = re.findall(r\"\\b[tT]he\\b\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47cfe9",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Words and Corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ecdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to download some corpus\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import indian\n",
    "from nltk.corpus import conll2007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055737b7",
   "metadata": {},
   "source": [
    "### Word types and word instances (tokens)\n",
    "\n",
    "- **Word types** are the number of distinct words in a corpus; if the set of words in the vocabulary is $V$, the number of types is the vocabulary size $|V|$. \n",
    "\n",
    "- **Word instances** are the total number $N$ of running words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words())\n",
    "print(f\"total number of tokens in Brown corpus: {len(brown.words())}\")\n",
    "for cat in brown.categories():\n",
    "    print(f\"category {cat} has {len(brown.words(categories=cat))} tokens\")\n",
    "print(f\"It has {len(nltk.FreqDist(w.lower() for w in brown.words()))} case-insensitive types\")\n",
    "print(f\"It has {len(nltk.FreqDist(w for w in brown.words()))} case-senstive types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = len(nltk.FreqDist(w.lower() for w in news_text))\n",
    "fdist_case_sensitive = len(nltk.FreqDist(w for w in news_text))\n",
    "print(f\"there are {fdist} different words in news category!\")\n",
    "print(f\"there are {fdist_case_sensitive} case sensitive words in news category!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = len(nltk.FreqDist(w.lower() for w in brown.words()))\n",
    "fdist_case_sensitive = len(nltk.FreqDist(w for w in brown.words()))\n",
    "print(f\"there are {fdist} different words among all category!\")\n",
    "print(f\"there are {fdist_case_sensitive} case sensitive words among all category!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daab13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "print(f\"all categories of brown: {brown.categories()}\")\n",
    "print(f\"all words in news: {brown.words(categories='news')}\")\n",
    "print(brown.words(fileids=['cg22']))\n",
    "print(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630dd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"brown corpus has {len(brown.fileids())} files in total, it belongs to {len(brown.categories())} categories\")\n",
    "print(f\"first 10 file names: {brown.fileids()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07df937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8feef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "type(emma_words)\n",
    "print(gutenberg.words('austen-emma.txt'))\n",
    "# How many tokens in the text:\n",
    "print(\"Token count:\", len(emma_words))\n",
    "\n",
    "# What is the token at index 1000?\n",
    "print(\"token at index 1000:\", emma_words[1000])\n",
    "\n",
    "# Slice from token 1400 to 1500\n",
    "print(\"slice from 1400 to 1500:\", emma_words[1400:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4445f38",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Word Tokenization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01c1dd",
   "metadata": {},
   "source": [
    "There are two type of tokenizations\n",
    "\n",
    "- **Top-down tokenization**: We define a standard and implement rules to implement that kind of tokenization.\n",
    "  - word tokenization\n",
    "  - charater tokenization\n",
    "- **Bottom-up tokenization**: We use simple statistics of letter sequences to break up words into subword tokens.\n",
    "  - subword tokenization (modern LLMs use this type!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7fd73",
   "metadata": {},
   "source": [
    "### Top-down (rule-based) tokenization - word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use split method via the whitespace \" \"\n",
    "text = \"\"\"While the Unix command sequence just removed all the numbers and punctuation\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But, we have punctuations, icons, and many other small issues.\n",
    "text = \"\"\"Don't you love 🤗 Transformers? We sure do.\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d09288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-down tokenization by using regular expression\n",
    "pattern = r'''(?x) # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A. \n",
    "| \\w+(?:-\\w+)* # words with optional internal hyphens \n",
    "| \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82% \n",
    "| \\.\\.\\. # ellipsis \n",
    "| [][.,;\"'?():_`-] # these are separate tokens; includes ], [\n",
    "'''\n",
    "print(f'pattern needs to match is: \\n\\n{pattern}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Don't you love 🤗 Transformers? We sure do.\"\"\"\n",
    "print(f\"tokenized words after pattern matching: \\n\\n{nltk.regexp_tokenize(text, pattern)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ead275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy works much better\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"While the Unix command sequence just removed all the numbers and punctuation,\n",
    "for most NLP applications we’ll . But we’ll often want\n",
    "to keep the punctuation that occurs word internally, in examples like m.p.h., Ph.D.,\n",
    "AT&T, and cap’n. Special characters and numbers will need to be kept in prices\n",
    "($45.55) and dates (01/02/06); we don’t want to segment that price into separate\n",
    "tokens of “45” and “55”. And there are URLs (https://www.stanford.edu),\n",
    "Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).\n",
    "Number expressions introduce other complications as well; while commas normally\n",
    "appear at word boundaries, commas are used inside numbers in English, every\n",
    "three digits: 555,500.50. (or sometimes periods)\n",
    "where English puts commas, for example, 555 500,50.\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()\n",
    "print(f\"tokenized words after pattern matching: \\n\\n{nltk.regexp_tokenize(text, pattern)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15249178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy works much better\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc[:10]: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is more complex in languages like written Chinese, Japanese.\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "text = '姚明进入总决赛'\n",
    "t = TreebankWordTokenizer()\n",
    "toks = t.tokenize(text)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faa588",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::jieba --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StanfordSegmenter for Chinese \n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "# Note, it needs to install jar file.\n",
    "# Alternative way to tokenize Chinese words\n",
    "# install jieba via conda as: conda install conda-forge::jieba\n",
    "# Website: https://github.com/fxsjy/jieba\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '姚明进入总决赛'\n",
    "seg_list = jieba.cut(text)\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933379b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download zh_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ba975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = '姚明进入总决赛'\n",
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff407831",
   "metadata": {},
   "source": [
    "### Top-down (rule-based) tokenization - character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2faede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "nlp_ch = Chinese()\n",
    "print(*nlp_ch(text), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1518d2b",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Word Tokenization: BPE</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fc1db",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding: A Bottom-up Tokenization Algorithm\n",
    "- It has been adopted from all modern LLMs including ChatGPT, GPT-series, and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301387b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::tiktoken --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a39504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, install GPT-4's tiktoken via: conda install conda-forge::tiktoken\n",
    "import tiktoken\n",
    "# Load an encoding\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# Use tiktoken.encoding_for_model() to automatically load the correct encoding for a given model name.\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "print(encoding.encode(\"tiktoken is great!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74531989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens by counting the length of the list returned by .encode().\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"tiktoken is great!\"\n",
    "print(f'\\\"{text}\\\" has been encoded into {num_tokens_from_string(text, \"cl100k_base\")} subwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .decode() converts a list of token integers to a string.\n",
    "encode_ids = [83, 1609, 5963, 374, 2294, 0]\n",
    "print(f'the decoded string is: \\\"{encoding.decode(encode_ids)}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ea379",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Chapters 5 to 8 teach the basics of 🤗 Datasets and 🤗 Tokenizers before diving into classic NLP tasks.\\\n",
    "By the end of this part, you will be able to tackle the most common NLP problems by yourself. \\\n",
    "By the end of this part, you will be ready to apply 🤗 Transformers to (almost) any machine \\\n",
    "learning problem! E=mc^2. f(x) = x^2+y^2, print('hello world!’) baojianzhou. asdasfasdgasdg\n",
    "\"\"\"\n",
    "print(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ids = encoding.encode(text)\n",
    "print(encoding.decode(encode_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c07c2",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Word Normalization, Lemmatization and Stemming</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07838dbd",
   "metadata": {},
   "source": [
    "### Lemmatization (词形还原)\n",
    "\n",
    "- Lemmatization is the task of determining that two words have the same root, despite their surface differences.\n",
    "- **Motivation**: For some NLP situations, we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages\n",
    "that mention woodchuck with no s.\n",
    "- **Example 1**: The words am, are, and is have the shared lemma be.\n",
    "- **Example 2**: The words dinner and dinners both have the lemma dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "text = \"\"\"\n",
    "The Brown Corpus, a text corpus of American English that was compiled in the 1960s at Brown University, \\\n",
    "is widely used in the field of linguistics and natural language processing. It contains about 1 million \\\n",
    "words (or \"tokens\") across a diverse range of texts from 500 sources, categorized into 15 genres, such \\\n",
    "as news, editorial, and fiction, to provide a comprehensive resource for studying the English language. \\\n",
    "This corpus has been instrumental in the development and evaluation of various computational linguistics \\\n",
    "algorithms and tools.\n",
    "\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print(doc[0], type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in doc]\n",
    "for ori,lemma in zip(doc[:10], lemmas[:10]):\n",
    "    print(ori, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f644420",
   "metadata": {},
   "source": [
    "### Stemming (词干提取): The Porter-Stemmer method\n",
    "\n",
    "Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off words final affixes. This naive version of morphological analysis is called stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy does not provide stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"\\\n",
    "This was not the map we found in Billy Bones's chest, but \\\n",
    "an accurate copy, complete in all things-names and heights \\\n",
    "and soundings-with the single exception of the red crosses \\\n",
    "and the written notes.\\\n",
    "\"\"\"   \n",
    "porter_stemmer = PorterStemmer()\n",
    "words = word_tokenize(text)\n",
    "for word in words[:10]:\n",
    "    print(word, porter_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec4ff0",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Sentence Segmentation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: use nltk package\n",
    "# Install nltk\n",
    "import nltk\n",
    "# Download the required models\n",
    "nltk.download('punkt')  \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a90106",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In the first part of the book we introduce the fundamental suite of algorithmic \\\n",
    "tools that make up the modern neural language model that is the heart of end-to-end \\\n",
    "NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms \\\n",
    "like computing edit distance, and then proceed to the tasks of classification, \\\n",
    "logistic regression, neural networks, proceeding through feedforward networks, recurrent \\\n",
    "networks, and then transformers. We’ll also see the role of embeddings as a \\\n",
    "model of word meaning.\"\n",
    "sentences = sent_tokenize(text)\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb526c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: A modern and fast NLP library that includes support for sentence segmentation. \n",
    "# spaCy uses a statistical model to predict sentence boundaries, which can be more accurate \n",
    "# than rule-based approaches for complex texts.\n",
    "# Install via conda: conda install conda-forge::spacy\n",
    "# Install via pip:   pip install -U spacy\n",
    "# Download data: python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Here is a sentence. Here is another one! And the last one.\")\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install it via: python -m spacy download zh_core_web_sm\n",
    "from spacy.lang.zh.examples import sentences \n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "text = \"\"\"\\\n",
    "时光荏苒，自 2003 年我师从吴立德教授，开启自然语言处理学习与研究之路，转眼已近二十\\\n",
    "载春秋。回想当年第一次听到自然语言处理的目标 ──“让机器理解人类语言”时的兴奋，第一次\\\n",
    "看到《大规模中文文本处理》教材时的茫然，仿佛黄萱菁教授对我研究生入学的电话面试就在昨\\\n",
    "天，每周与吴老师固定交流前的紧张感依然清晰。从求学到任教，深刻感受到自然语言处理的快\\\n",
    "速发展，从基于特征的统计机器学习方法到深度神经网络模型，再到大规模预训练方法，自然语\\\n",
    "言处理研究范式的更新迭代速度也在不断加快。在本科生和研究生的自然语言处理课程教学过程\\\n",
    "中，虽然通过不断补充国际国内的近期研究进展，将最新的理论和方法通过课件和面授的形式介\\\n",
    "绍给同学们，但是系统全面的书籍仍然是不可或缺的重要资料。于是，自 2020 年起与黄萱菁教授\\\n",
    "和桂韬研究员一起开始着手本书的准备，在经过几十次的讨论和大纲和结构反复修改后，自 2021\\\n",
    "年暑假起开始了本书的写作。2022 年本书入选复旦大学七大系列百本精品教材项目和复旦大学研\\\n",
    "究生规划系列教材项目，进一步督促我们加快进度。从规划到完成，历时近三年之久，这本拙作\\\n",
    "终于完成。\"\"\"\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fc03f-382c-485f-b7ec-e386ad9e697b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
