{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">N-gram Language Models</h1>\n",
    "\n",
    "> In this notebook, we illustrate some basic ideas of N-gram and LM evaluation by using the perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy==1.24.2\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import math\n",
    "import nltk\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# make plots in center\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">$N$-Grams</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1 = \\\n",
    "\"\"\"\n",
    "<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I do not like green eggs and ham </s>\n",
    "\"\"\"\n",
    "print(raw_text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the raw text:\n",
    "\n",
    "We tokenize it as a list of words by whitespace, you may use tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = raw_text1.strip().split('\\n')\n",
    "print(f'All sentences:')\n",
    "for sent in sentences:\n",
    "    print(sent)\n",
    "print('---')\n",
    "tokens = raw_text1.replace(\"\\n\", \" \").split()\n",
    "print(f'All unigram tokens:\\n{tokens}\\n---')\n",
    "vocab = set(tokens)\n",
    "print(f'set of vocabulary: \\n{vocab}')\n",
    "print(f'{len(vocab)} words in total.')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bigrams and the counts of these bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_list = [unigram for sent in sentences \n",
    "           for unigram in nltk.ngrams(sent.split(),n=1)]\n",
    "bigrams_list = [bigram for sent in sentences \n",
    "           for bigram in nltk.bigrams(sent.split())]\n",
    "print('All unigrams:\\n', unigrams_list)\n",
    "print('All bigrams:\\n', bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreqDist can help to get counts\n",
    "freq_dist = FreqDist(unigrams_list + bigrams_list)\n",
    "for gram, freq in freq_dist.items():\n",
    "    if len(gram) == 2:\n",
    "        print(f'gram: {gram}', freq, f', bigram prob.: {freq / freq_dist[(gram[0],)]:.6f}')\n",
    "    else:\n",
    "        print(f'gram: {gram}', freq)\n",
    "\n",
    "# most of the bigrams have zero probabilities.\n",
    "print(f'the following bigrams have zero probability: ')\n",
    "zeros = 0\n",
    "for v1, v2 in product(vocab, vocab):\n",
    "    if (v1, v2) not in freq_dist:\n",
    "        zeros += 1\n",
    "        print(f'{(v1, v2)} ', end=' ')\n",
    "\n",
    "print(f'\\nThere is {zeros} zero-prob bigrams in total!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the frequency distribution\n",
    "freq_dist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">3.2 Building and evaluating LMs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all books written by Jane Austen and split it as train and test dataset\n",
    "\n",
    "> The main part of section follows from [Joshua Floehr's code](https://github.com/joshualoehr/ngram-language-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/dataset_jane-austen_all.txt', 'r') as file:\n",
    "    raw_text_jane = file.read()\n",
    "raw_text_jane = raw_text_jane.replace(\"\\n\", \" \")\n",
    "print(raw_text_jane[:200], raw_text_jane[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_preprocess(raw_text, n):\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    sos = \"<s> \" * (n-1) if n > 1 else \"<s> \"\n",
    "    tokenized_sentences = ['{}{} {}'.format(sos, sent, \"</s>\").split() for sent in sentences]\n",
    "    n = len(tokenized_sentences)\n",
    "    tr_ind = np.random.permutation(n)[:n-1000]\n",
    "    tr_sentences = [tokenized_sentences[_] for _ in tr_ind]\n",
    "    te_ind = np.random.permutation(n)[n-1000:]\n",
    "    te_sentences = [tokenized_sentences[_] for _ in te_ind]\n",
    "    tr_tokens = [token for sublist in tr_sentences for token in sublist]\n",
    "    te_tokens = [token for sublist in te_sentences for token in sublist]\n",
    "    # Replace tokens which appear only once in the corpus with <UNK>\n",
    "    vocab = nltk.FreqDist(tr_tokens)\n",
    "    tr_tokens = [token if vocab[token] > 1 else \"<UNK>\" for token in tr_tokens]\n",
    "    return tr_tokens, te_tokens, tr_sentences, te_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self, tr_tokens, n, laplace=1):\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self.tokens = tr_tokens\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.model  = self._create_model()\n",
    "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
    "        self.special_words = ['<s>', '</s>', '<UNK>']\n",
    "\n",
    "    def _smooth(self):\n",
    "        \"\"\" Apply Laplace smoothing to N-gram frequency distribution. \"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create a probability distribution for the vocabulary of the training corpus.\n",
    "        If building a unigram model, the probabilities are simple relative frequencies\n",
    "        of each token with the entire corpus.\n",
    "        Otherwise, the probabilities are Laplace-smoothed relative frequencies.\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            return self._smooth()\n",
    "\n",
    "    def _convert_oov(self, ngram):\n",
    "        \"\"\"Convert, if necessary, a given n-gram to one which is known by the model.\n",
    "        Starting with the unmodified ngram, check each possible permutation of the n-gram\n",
    "        with each index of the n-gram containing either the original token or <UNK>. Stop\n",
    "        when the model contains an entry for that permutation.\n",
    "        This is achieved by creating a 'bitmask' for the n-gram tuple, and swapping out\n",
    "        each flagged token for <UNK>. Thus, in the worst case, this function checks 2^n\n",
    "        possible n-grams before returning.\n",
    "\n",
    "        Returns:\n",
    "            The n-gram with <UNK> tokens in certain positions such that the model\n",
    "            contains an entry for it.\n",
    "\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "    def perplexity(self, te_tokens):\n",
    "        \"\"\"Calculate the perplexity of the model against a given test corpus.\n",
    "        Args:\n",
    "            test_data (list of str): sentences comprising the testing corpus.\n",
    "        Returns:\n",
    "            The perplexity of the model as a float.\n",
    "        \n",
    "        \"\"\"\n",
    "        test_ngrams = nltk.ngrams(te_tokens, self.n)\n",
    "        N = len(te_tokens)\n",
    "        known_ngrams  = (self._convert_oov(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev, i, without=[]):\n",
    "        \"\"\"Choose the most likely next token given the previous (n-1) tokens.\n",
    "        If selecting the first word of the sentence (after the SOS tokens),\n",
    "        the i'th best candidate will be selected, to create variety.\n",
    "        If no candidates are found, the EOS token is returned with probability 1.\n",
    "        Args:\n",
    "            prev (tuple of str): the previous n-1 tokens of the sentence.\n",
    "            i (int): which candidate to select if not the most probable one.\n",
    "            without (list of str): tokens to exclude from the candidates list.\n",
    "        Returns:\n",
    "            A tuple with the next most probable token and its corresponding probability.\n",
    "\n",
    "        \"\"\"\n",
    "        blacklist  = [\"<UNK>\"] + without\n",
    "        candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n",
    "        candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
    "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
    "        if len(candidates) == 0:\n",
    "            return (\"</s>\", 1)\n",
    "        else:\n",
    "            return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "     \n",
    "    def generate_sentences(self, num, min_len=12, max_len=24):\n",
    "        \"\"\"Generate num random sentences using the language model.\n",
    "        Sentences always begin with the SOS token and end with the EOS token.\n",
    "        While unigram model sentences will only exclude the UNK token, n>1 models\n",
    "        will also exclude all other words already in the sentence.\n",
    "\n",
    "        Args:\n",
    "            num (int): the number of sentences to generate.\n",
    "            min_len (int): minimum allowed sentence length.\n",
    "            max_len (int): maximum allowed sentence length.\n",
    "        Yields:\n",
    "            A tuple with the generated sentence and the combined probability\n",
    "            (in log-space) of all of its n-grams.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = self._best_candidate(prev, i, without=blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -1/math.log(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LMs with Add-one smoothing testing on Jane Austen's books\n",
    "\n",
    "> 1. Tokenization for raw text of Jane Austen's books.\n",
    "> 2. Build and Train LMs using add-one smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "laplace = .1 # the parameter of add-one smoothing.\n",
    "num_sent_gen = 10 # number of sentences is generated.\n",
    "\n",
    "for n in [1,2,3]:\n",
    "    print(\"-\" * 50)\n",
    "    tr_tokens, te_tokens, tr_sentences, te_sentences = raw_text_preprocess(raw_text_jane, n)\n",
    "    \n",
    "    print(\"Loading {}-gram model...\".format(n))\n",
    "    lm = LanguageModel(tr_tokens, n, laplace=laplace)\n",
    "    print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "    \n",
    "    print(\"Generating sentences...\")\n",
    "    for sentence, prob in lm.generate_sentences(num_sent_gen):\n",
    "        print(\"{} ({:.5f})\".format(sentence, prob))\n",
    "    \n",
    "    perplexity = lm.perplexity(te_tokens)\n",
    "    print(\"Model perplexity: {:.3f}\".format(perplexity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all Shakespeare's work\n",
    "\n",
    "> We downloaded Shakespeare's work at [Gutenberg](https://www.gutenberg.org/ebooks/100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/dataset_shakespeare_all.txt', 'r') as file:\n",
    "    raw_text_shakespeare = file.read()\n",
    "raw_text_shakespeare = raw_text_shakespeare.replace(\"\\n\", \" \")\n",
    "print(raw_text_shakespeare[:200], raw_text_shakespeare[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace = .1 # the parameter of add-one smoothing.\n",
    "num_sent_gen = 10 # number of sentences is generated.\n",
    "\n",
    "for n in [1,2,3]:\n",
    "    print(\"-\" * 50)\n",
    "    tr_tokens, te_tokens, tr_sentences, te_sentences = raw_text_preprocess(raw_text_jane + raw_text_shakespeare, n)\n",
    "    \n",
    "    print(\"Loading {}-gram model...\".format(n))\n",
    "    lm = LanguageModel(tr_tokens, n, laplace=laplace)\n",
    "    print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "    \n",
    "    print(\"Generating sentences...\")\n",
    "    for sentence, prob in lm.generate_sentences(num_sent_gen):\n",
    "        print(\"{} ({:.5f})\".format(sentence, prob))\n",
    "    \n",
    "    perplexity = lm.perplexity(te_tokens)\n",
    "    print(\"Model perplexity: {:.3f}\".format(perplexity))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
