{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bedddd1",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Pytorch Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbeb8d",
   "metadata": {},
   "source": [
    "**WHAT IS PYTORCH?**\n",
    "\n",
    ">It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "> 1. A replacement for NumPy to use the power of GPUs\n",
    "> 2. a deep learning research platform that provides maximum flexibility and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9dbc69",
   "metadata": {},
   "source": [
    "**1.1 Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbbab1-b147-4834-8a34-9a524339d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075844ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-01 Construct a 5x3 matrix, uninitialized:\n",
    "from __future__ import print_function\n",
    "x = torch.empty(5, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f671f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-02 Construct a randomly initialized matrix:\n",
    "\n",
    "x = torch.rand(5,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6711aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-03 Construct a matrix filled zeros and of dtype long:\n",
    "\n",
    "x = torch.zeros(5, 4, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-04 Construct a tensor directly from data:\n",
    "\n",
    "x = torch.tensor([5.5, 3, 4., -1.])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f181c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-05 These methods will reuse properties of the \n",
    "# input tensor, e.g. dtype, unless new values are provided by user\n",
    "\n",
    "x = x.new_ones(5, 4, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "print(type(x))\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-06 Get its size:\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077cafb",
   "metadata": {},
   "source": [
    "**1.2 Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f17afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-07 Addition using +\n",
    "\n",
    "y = torch.rand(5, 4)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-08 Addition using add() method\n",
    "\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b51d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-09 Addition: providing an output tensor as argument\n",
    "\n",
    "result = torch.empty(5, 4)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-10 Addition: in-place\n",
    "# Any operation that mutates a tensor in-place is post-fixed with an _. \n",
    "# For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee384a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-11 You can use standard NumPy-like indexing with all bells and whistles!\n",
    "\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbf4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-12 Resizing: If you want to resize/reshape tensor, you can use torch.view\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ab92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-12 Get numerical value: \n",
    "# If you have a one element tensor, use .item() \n",
    "# to get the value as a Python number\n",
    "\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "print(y[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce62f9",
   "metadata": {},
   "source": [
    "**1.3 Converting a Torch Tensor to a NumPy Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-13 Converting a Torch Tensor to a NumpPy Array\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d941c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-14 See how the numpy array changed in value.\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# They share the same memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-15 Converting NumPy Array to Torch Tensor\n",
    "\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-16 Tensors can be moved onto any device using the .to method.\n",
    "\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8abed",
   "metadata": {},
   "source": [
    "**1.4 Autograd: automatic differentiation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2666c2",
   "metadata": {},
   "source": [
    "Central to all neural networks in PyTorch is the *autograd* package. Let’s first briefly visit this, and we will then go to training our first neural network.\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-17 Create a tensor and set requires_grad=True to track computation with it\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aaf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d02204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-18 do operations on y\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac6f3a",
   "metadata": {},
   "source": [
    "**1.5 Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23c0ec",
   "metadata": {},
   "source": [
    "Let’s backprop now Because out contains a single scalar, \n",
    "out.backward() is equivalent to out.backward(torch.tensor(1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-19 do the backprop\n",
    "\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b69010",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55112c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-20 the autograd operation could complicated\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8441801",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-21 Use requires_grad()\n",
    "# You can also stop autograd from tracking history on Tensors with \n",
    "# .requires_grad=True by wrapping the code block in with torch.no_grad():\n",
    "\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-22 create a neural network model\n",
    "\n",
    "# A typical training procedure for a neural network is as follows:\n",
    "\n",
    "# Define the neural network that has some learnable parameters (or weights)\n",
    "# Iterate over a dataset of inputs\n",
    "# Process input through the network\n",
    "# Compute the loss (how far is the output from being correct)\n",
    "# Propagate gradients back into the network’s parameters\n",
    "# Update the weights of the network, typically using a simple update rule: \n",
    "# weight = weight - learning_rate * gradient\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-23 The learnable parameters of a model are returned by net.parameters()\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4821bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-24 \n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-25 Zero the gradient buffers of all parameters and backprops with random gradients:\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-26 Loss function \n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ea0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdd4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-27 Backprop\n",
    "\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weights\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac558d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1b295-5feb-482f-96ad-bf5729a84411",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-2.0, requires_grad=True)\n",
    "y = torch.tensor(5.0, requires_grad=True)\n",
    "z = torch.tensor(-4.0, requires_grad=True)\n",
    "f = (x+y)*z # Define the computation graph\n",
    "f.backward() # PyTorch’s internal backward gradient computation\n",
    "print('Gradients after backpropagation:', x.grad, y.grad, z.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
