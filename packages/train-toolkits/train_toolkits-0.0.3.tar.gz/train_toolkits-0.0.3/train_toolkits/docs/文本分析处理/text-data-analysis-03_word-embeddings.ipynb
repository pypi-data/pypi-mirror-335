{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446821ed-e61d-4e3e-82a6-1974db06f481",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Word Embedding - word2vec</h1>\n",
    "\n",
    "> In this notebook, we illustrate some basic ideas of word embeddings via the word2vec model.\n",
    "> **If it has an error when you install gensim, to patch the problem temporarily without downgrading, I replaced the import inside gensim/matutils.py in my venv with from numpy import triu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65286f20-5185-49fd-b711-fd61a5c24a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a8d52-35cb-4b78-b830-a8ee6b41075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2db639-fd10-4d47-bca2-603bdf764dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tempfile\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3982d0-9796-427e-9cab-48439e1342d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume there's one document per line, tokens separated by whitespace\n",
    "corpus_path = datapath('lee_background.cor')\n",
    "for ind, line in enumerate(open(corpus_path)):\n",
    "    print(ind, line[0:100])\n",
    "    if ind > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec4fb2-35b1-411e-9b9e-c034d83f5111",
   "metadata": {},
   "source": [
    "### Build corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fcdac-8d9c-4a78-91a0-c5b349b9ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a602d7-e2cb-4c72-ac12-544bfe0b3863",
   "metadata": {},
   "source": [
    "### Training word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0a968-af19-45e0-b43a-772afb64bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of sentences\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=100, alpha=0.025, window=5, \n",
    "                               min_count=5, sample=0.001, seed=1, workers=3, min_alpha=0.0001, \n",
    "                               sg=1, negative=5, ns_exponent=0.75, epochs=5, sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c22eef-3075-4e9c-b645-83ac7e8919ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['king']\n",
    "print(vec_king)\n",
    "for index, word in enumerate(model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46bf34-d209-4d94-9918-ad64561b955c",
   "metadata": {},
   "source": [
    "### Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cf1eb-fe22-42b9-82f7-7b2a0f4017cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    # To load a saved model:\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecea424-e787-4dd7-92b0-78150f405c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# cleaning up temporary file\n",
    "import os\n",
    "os.remove(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956ce87-f545-43d8-86dd-9cf7ad9c6aec",
   "metadata": {},
   "source": [
    "### Visualising Word Embeddings using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63ea6f-bc1c-4f41-b912-c3f44c565d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    random.seed(0)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "        \n",
    "plot_with_matplotlib(x_vals, y_vals, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
