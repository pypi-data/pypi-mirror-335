{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each section defines how we construct each component in the training. Most corresponds to a PyTorch component (e.g. *model* for `torch.nn.Module`, *data.dataset* for `torch.utils.data.Dataset`). Here are the custom components in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may check their __init__ method to see how they are constructed\n",
      "[data.augment] SegmentationAugment : Default data augmentations for semantic segmentation\n",
      "[trainer] Trainer : Repeatedly run training and validation loop, provide detailed logs\n",
      "[paths] don't have associated class\n",
      "[log.wandb] WandbLogger : Save log results and optionally snapshots to wandb\n",
      "[log.tensorboard] TensorboardLogger : Save log results and snapshots to Tensorboard\n"
     ]
    }
   ],
   "source": [
    "from src.pixseg.pipeline import TensorboardLogger, Trainer, WandbLogger\n",
    "from src.pixseg.utils.transform import SegmentationAugment\n",
    "\n",
    "print(\"You may check their __init__ method to see how they are constructed\")\n",
    "print(\"[data.augment]\", SegmentationAugment.__name__, \":\", SegmentationAugment.__doc__)\n",
    "print(\"[trainer]\", Trainer.__name__, \":\", Trainer.__doc__.splitlines()[0])  # type: ignore\n",
    "print(\"[paths] don't have associated class\")\n",
    "print(\"[log.wandb]\", WandbLogger.__name__, \":\", WandbLogger.__doc__)\n",
    "print(\"[log.tensorboard]\", TensorboardLogger.__name__, \":\", TensorboardLogger.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fields\n",
    "Most section have the field *\"params\"*. This will be sent as kwargs to the component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructor field\n",
    "Some sections allow user to choose the constructor. Here is the list of such sections and the builtin options (you may register more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[model.model] 20 items\n",
      "['bisenet_resnet18', 'bisenet_resnet50', 'bisenet_xception', 'deeplabv3_mobilenet_v3_large',\n",
      " 'deeplabv3_resnet101', 'deeplabv3_resnet18', 'deeplabv3_resnet50', 'enet', 'fcn_resnet101',\n",
      " 'fcn_resnet50', 'fcn_vgg16', 'lraspp_mobilenet_v3_large', 'lraspp_resnet18', 'pspnet_resnet50',\n",
      " 'sfnet_lite_resnet101', 'sfnet_lite_resnet18', 'sfnet_resnet101', 'sfnet_resnet18',\n",
      " 'upernet_resnet101', 'upernet_resnet18']\n",
      "\n",
      "[data.dataset.dataset] 11 items\n",
      "['ADE20K', 'BDD100K', 'Cityscapes', 'Cityscapes{category}', 'Cityscapes{full}', 'LIP', 'Mapillary',\n",
      " 'SBD', 'VOC', 'coco', 'coco{voc}']\n",
      "\n",
      "[criterion.criterion] 3 items\n",
      "['CrossEntropyLoss', 'DiceLoss', 'FocalLoss']\n",
      "\n",
      "[criterion.class_weight] 5 items\n",
      "['class_frequency', 'effective_number', 'log_frequency', 'none', 'sqrt_frequency']\n",
      "\n",
      "[optimizer.optimizer] 3 items\n",
      "['Adam', 'Padam', 'SGD']\n",
      "\n",
      "[lr_scheduler.lr_scheduler] 4 items\n",
      "['CosineAnnealingLR', 'OneCycleLR', 'PolynomialLR', 'StepLR']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from src.pixseg.datasets import DATASET_ZOO\n",
    "from src.pixseg.learn import (\n",
    "    CLASS_WEIGHTINGS,\n",
    "    CRITERION_ZOO,\n",
    "    LR_SCHEDULER_ZOO,\n",
    "    OPTIMIZER_ZOO,\n",
    ")\n",
    "from src.pixseg.models import MODEL_ZOO\n",
    "\n",
    "# fmt: off\n",
    "zoos: dict[str, dict] = {\n",
    "    \"[model.model]\": MODEL_ZOO, \"[data.dataset.dataset]\": DATASET_ZOO, \n",
    "    \"[criterion.criterion]\": CRITERION_ZOO, \"[criterion.class_weight]\": CLASS_WEIGHTINGS, \n",
    "    \"[optimizer.optimizer]\": OPTIMIZER_ZOO, \"[lr_scheduler.lr_scheduler]\": LR_SCHEDULER_ZOO,\n",
    "}\n",
    "# fmt: on\n",
    "\n",
    "for k, v in zoos.items():\n",
    "    print(f\"\\n{k} {len(v)} items\")\n",
    "    pprint(sorted(v.keys()), width=100, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[model.state_file]` path to model's state dict  \n",
    "`[data.dataset.pad_crop_size]` The size to pad or crop each image  \n",
    "`[data.loader.num_workers]` applies to both train and eval dataloader  \n",
    "`[criterion.class_weight]` weight given to each class in weighted loss. See above for valid name  \n",
    "`[criterion.aux_weight]` alpha for auxiliary weight. You may want to set *aux_loss=True* in *[model.params]* for this to take effect  \n",
    "`[optimizer.effective_batch_size]` the actual optimization batch size. Must be a multiple of *[data.loader.params.batch_size]*  \n",
    "`[trainer.device]` if *\"auto\"*, choose cpu or cuda automatically  \n",
    "`[paths.runs_folder]` folder to store logs, checkpoints and snapshots locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[data.dataset.pad_crop_size], [data.loader.params], [data.augment.params]` are only applied to the training dataset/loader. Evaluations are always done one by one with default augmentations. To use different test-time strategies, please see `tasks\\evaluation.ipynb` or `tasks\\inference.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
