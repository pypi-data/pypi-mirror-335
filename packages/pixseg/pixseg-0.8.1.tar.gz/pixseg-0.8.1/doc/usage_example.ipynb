{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.pixseg.datasets import ADE20K, resolve_metadata\n",
    "from src.pixseg.utils.transform import SegmentationTransform\n",
    "\n",
    "root = Path(r\"path/to/ADE20K\")\n",
    "# Preset metadata of ADE20K, which includes labels, background index, etc\n",
    "metadata = resolve_metadata(\"ADE20K\")\n",
    "# Make sure the data is formatted correctly for model input\n",
    "transforms = SegmentationTransform(size=(512, 512), mask_fill=metadata.ignore_index)\n",
    "dataset = ADE20K(root, split=\"training\", transforms=transforms)\n",
    "train_loader = DataLoader(dataset, batch_size=4, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "\n",
    "from src.pixseg.learn import DiceLoss, Padam\n",
    "from src.pixseg.models import PSPNET_ResNet50_Weights, pspnet_resnet50\n",
    "from src.pixseg.utils.transform import SegmentationAugment\n",
    "\n",
    "# Create PSPNet with ResNet-50 backbone and enable auxiliary loss\n",
    "model = pspnet_resnet50(num_classes=metadata.num_classes, aux_loss=True)\n",
    "# Or initialize with pretrained weights\n",
    "model = pspnet_resnet50(weights=PSPNET_ResNet50_Weights.DEFAULT)\n",
    "criterion = DiceLoss(ignore_index=metadata.ignore_index)\n",
    "optimizer = Padam(model.parameters(), lr=0.1, weight_decay=5e-4, partial=0.125)\n",
    "lr_scheduler = PolynomialLR(optimizer, total_iters=100, power=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "from src.pixseg.utils.metrics import MetricStore\n",
    "from src.pixseg.utils.rng import seed\n",
    "from src.pixseg.utils.visual import exhibit_figure, plot_confusion_matrix\n",
    "\n",
    "# Fix random seeds of random, numpy and pytorch\n",
    "seed(42)\n",
    "# This project separates data transforms and data augmentations, so that visualizations can show the original images\n",
    "train_augment = SegmentationAugment(hflip=0.5, mask_fill=metadata.ignore_index)\n",
    "for i in range(100):  # Set your number of epochs\n",
    "    model.train()\n",
    "    # Store and calculate metrics efficiently\n",
    "    train_ms = MetricStore(metadata.num_classes)\n",
    "    for j, (images, masks) in enumerate(train_loader):\n",
    "        images, masks = train_augment(images, masks)\n",
    "        ...\n",
    "        logits: dict[str, Tensor] = model(images)\n",
    "        train_ms.store_results(masks, logits[\"out\"].argmax(1))\n",
    "        ...\n",
    "\n",
    "    # Print training results\n",
    "    print(train_ms.summarize())\n",
    "    # Visualize confusion matrix and save as image\n",
    "    plot_confusion_matrix(train_ms.confusion_matrix, metadata.labels)\n",
    "    exhibit_figure(show=False, save_to=Path(\"confusion_matrix.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
