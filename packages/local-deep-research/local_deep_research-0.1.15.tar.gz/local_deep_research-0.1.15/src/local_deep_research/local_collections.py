# local_collections.py
"""
Configuration file for local document collections.
Each collection functions as an independent search engine.
"""

import os
from typing import Dict, Any

# Registry of local document collections
# Each collection appears as a separate search engine in the main configuration
LOCAL_COLLECTIONS = {
    # Project Documents Collection
    "project_docs": {
        "name": "Project Documents",
        "description": "Project documentation and specifications",
        "paths": [os.path.abspath("./local_search_files/project_documents")],
        "enabled": True,
        "embedding_model": "all-MiniLM-L6-v2",
        "embedding_device": "cpu",
        "embedding_model_type": "sentence_transformers",
        "max_results": 20,
        "max_filtered_results": 5,
        "chunk_size": 1000,
        "chunk_overlap": 200,
        "cache_dir": ".cache/local_search/project_docs"
    },
    
    # Research Papers Collection
    "research_papers": {
        "name": "Research Papers",
        "description": "Academic research papers and articles",
        "paths": [os.path.abspath("local_search_files/research_papers")],
        "enabled": True,
        "embedding_model": "all-MiniLM-L6-v2",
        "embedding_device": "cpu",
        "embedding_model_type": "sentence_transformers",
        "max_results": 20,
        "max_filtered_results": 5,
        "chunk_size": 800,  # Smaller chunks for academic content
        "chunk_overlap": 150,
        "cache_dir": ".cache/local_search/research_papers"
    },
    
    # Personal Notes Collection
    "personal_notes": {
        "name": "Personal Notes",
        "description": "Personal notes and documents",
        "paths": [os.path.abspath("./local_search_files/personal_notes")],
        "enabled": True,
        "embedding_model": "all-MiniLM-L6-v2",
        "embedding_device": "cpu",
        "embedding_model_type": "sentence_transformers",
        "max_results": 30,
        "max_filtered_results": 10,
        "chunk_size": 500,  # Smaller chunks for notes
        "chunk_overlap": 100,
        "cache_dir": ".cache/local_search/personal_notes"
    }
}

# Configuration for local search integration
LOCAL_SEARCH_CONFIG = {
    # General embedding options
    "DEFAULT_EMBEDDING_MODEL": "all-MiniLM-L6-v2",
    "DEFAULT_EMBEDDING_DEVICE": "cpu",  # "cpu" or "cuda" for GPU acceleration
    "DEFAULT_EMBEDDING_MODEL_TYPE": "sentence_transformers",  # or "ollama"
    
    # Ollama settings (only used if model type is "ollama")
    # Note: You must run 'ollama pull nomic-embed-text' first if using Ollama for embeddings
    "OLLAMA_BASE_URL": "http://localhost:11434",
    "OLLAMA_EMBEDDING_MODEL": "nomic-embed-text",
    
    # Default indexing options
    "FORCE_REINDEX": True,  # Force reindexing on startup
    "CACHE_DIR": ".cache/local_search",  # Base directory for cache
}

def register_local_collections(search_engines_dict: Dict[str, Any]) -> None:
    """
    Register all enabled local collections as search engines.
    
    Args:
        search_engines_dict: The main search engines dictionary to update
    """
    for collection_id, collection in LOCAL_COLLECTIONS.items():
        print(collection_id, collection)
        if collection.get("enabled", True):
            # Skip if already defined (don't override)
            if collection_id in search_engines_dict:
                continue
                
            # Validate paths exist
            paths = collection.get("paths", [])
            valid_paths = []
            for path in paths:
                if os.path.exists(path) and os.path.isdir(path):
                    valid_paths.append(path)
                else:
                    print(f"Warning: Collection '{collection_id}' contains non-existent folder: {path}")
            
            # Log warning if no valid paths
            if not valid_paths and paths:
                print(f"Warning: Collection '{collection_id}' has no valid folders. It will be registered but won't return results.")
                
            # Create a search engine entry for this collection
            search_engines_dict[collection_id] = {
                "module_path": "local_deep_research.web_search_engines.engines.search_engine_local",
                "class_name": "LocalSearchEngine",
                "requires_api_key": False,
                "reliability": 0.9,  # High reliability for local documents
                "strengths": ["personal documents", "offline access", 
                             collection.get("description", "local documents")],
                "weaknesses": ["requires indexing", "limited to specific folders"],
                "default_params": {
                    "folder_paths": collection.get("paths", []),
                    "embedding_model": collection.get(
                        "embedding_model", 
                        LOCAL_SEARCH_CONFIG["DEFAULT_EMBEDDING_MODEL"]
                    ),
                    "embedding_device": collection.get(
                        "embedding_device", 
                        LOCAL_SEARCH_CONFIG["DEFAULT_EMBEDDING_DEVICE"]
                    ),
                    "embedding_model_type": collection.get(
                        "embedding_model_type", 
                        LOCAL_SEARCH_CONFIG["DEFAULT_EMBEDDING_MODEL_TYPE"]
                    ),
                    "chunk_size": collection.get("chunk_size", 1000),
                    "chunk_overlap": collection.get("chunk_overlap", 200),
                    "cache_dir": collection.get(
                        "cache_dir", 
                        f"{LOCAL_SEARCH_CONFIG['CACHE_DIR']}/{collection_id}"
                    ),
                    "max_results": collection.get("max_results", 20),
                    "max_filtered_results": collection.get("max_filtered_results", 5),
                    "collection_name": collection.get("name", collection_id),
                    "collection_description": collection.get("description", "")
                },
                "requires_llm": True
            }