Metadata-Version: 2.4
Name: RLInventoryOpt
Version: 0.1.3
Summary: Launching over version 0.1.2. This just solve the typo in consumption.
Home-page: https://github.com/sebassaras02/rf_inventory_optimization
Author: Sebastian Sarasti
Author-email: Sebastian Sarasti <sebitas.alejo@hotmail.com>
License: MIT License
        
        Copyright (c) 2024 Sebastian Sarasti
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Project-URL: Homepage, https://github.com/sebassaras02/rf_inventory_optimization
Project-URL: Explanation, https://medium.com/@sebitas.alejo/reinforcement-learning-for-inventory-optimization-f63c26a59c19
Keywords: Reinforcement Learning,Q-Learning,Inventory Optimization
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy==1.26.4
Requires-Dist: pandas==2.2.2
Requires-Dist: plotly==5.22.0
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# ğŸ“Š Reinforcement Learning in Inventory Optimization using Q-Learning

This repository contains a Python implementation of an inventory optimization model using Q-Learning, a Reinforcement Learning (RL) algorithm. The model is designed to help manage inventory levels by making optimal decisions on order quantities based on forecasted demand, initial stock levels, and inventory capacity.

## ğŸš€ Introduction

Efficient inventory management is crucial for reducing costs and avoiding stockouts or overstocking. This project implements a Q-Learning-based optimizer that learns to make optimal inventory decisions over time. It considers factors such as forecasted demand, security stock, and inventory capacity to minimize costs and maintain optimal stock levels.

## âœ¨ Features

- ğŸ§  **Q-Learning Algorithm**: Implements Q-Learning for decision-making based on temporal difference learning.
- ğŸ”„ **Dynamic Inventory Management**: Adjusts inventory levels based on forecasted consumption dynamically without traditional rules.
- ğŸ› ï¸ **Customizable Parameters**: Adjustable learning rate, discount factor, and exploration rate.
- ğŸ“ˆ **Visualizations**: Plots inventory levels, forecast, and order amounts to provide insights into the optimization process.


## Installation ğŸš€

You can install the `RLInventoryOpt` library directly from GitHub. Follow the instructions below to get started!

### Install from GitHub ğŸŒŸ

To install the library directly from GitHub, use the following command:

```bash
pip install RLInventoryOpt
```


## ğŸ”§ Usage
Initialize the Model

Create an instance of the QLearningOptimizer class.

To create any model for inventory optimization, you have to follow this:

1. Create a forecasting model and predict the future consuption.
2. You have to know the limitations of your system such as security stock, maximum level of stock, initial stock, lead time, and the minimal order quantity.
3. The actions are limited based from 1 to 6 times the minimal order quantity. 


```python
from RLInventoryOpt.qlearning import QLearningOptimizer
import numpy as np

# Example forecasted demand for 6 months
forecast = np.array([400, 325, 356, 210, 150, 400])

# Initial conditions of the system
initial_state = {
 "stock": 800,
 "leadTime": 2,
 "minimumOrder": 100,
 "securityStock": 200,
 "maximalCapacity": 1000
}

# Define the actions
actions = ["no", "m", "2m", "3m", "4m", "5m", "6m"]

# Initialize the optimizer
model = QLearningOptimizer(
 forecast=forecast, 
 initial_stock=initial_state["stock"], 
 security_stock=initial_state["securityStock"],
 capacity=initial_state["maximalCapacity"],
 n_actions=actions, 
 min_order=initial_state["minimumOrder"], 
 lead_time=initial_state["leadTime"],
 alpha=0.1, 
 gamma=0.6, 
 epsilon=0.1
)

# Train the model
model.fit(epochs=1000)

# Predict inventory levels and actions
predictions = model.predict()

# Plot the results
model.plot("bar")
```

A detailed explanation can be found on this article on Medium: https://lnkd.in/gDEvav59

## âš™ï¸ Customization

You can customize the behavior of the agent modifying the parameters for training.

- alpha (float): Learning rate (default: 0.1)
- gamma (float): Discount factor (default: 0.6)
- epsilon (float): Exploration rate (default: 0.1)

## â˜• Support the Project

If you find this inventory optimization tool helpful and would like to support its continued development, consider buying me a coffee. Your support helps maintain and improve this project!

[![Buy Me A Coffee](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://www.paypal.com/paypalme/sebassarasti)

### Other Ways to Support
- â­ Star this repository
- ğŸ´ Fork it and contribute
- ğŸ“¢ Share it with others who might find it useful
- ğŸ› Report issues or suggest new features

Your support, in any form, is greatly appreciated! ğŸ™
