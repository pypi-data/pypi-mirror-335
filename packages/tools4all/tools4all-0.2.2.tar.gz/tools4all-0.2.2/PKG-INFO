Metadata-Version: 2.4
Name: tools4all
Version: 0.2.2
Summary: Function calling capabilities for LLMs that don't natively support them
Home-page: https://github.com/alfredwallace7/tools4all
Author: Alfred Wallace
Author-email: Alfred Wallace <alfred.wallace@netcraft.fr>
License: MIT
Project-URL: Homepage, https://github.com/alfredwallace7/tools4all
Project-URL: Issues, https://github.com/alfredwallace7/tools4all/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: ollama>=0.1.20
Requires-Dist: pydantic>=2.5.0
Requires-Dist: rich>=13.6.0
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# Tools4All

A drop-in replacement for the Ollama Python library that adds function calling capabilities to LLMs that don't natively support them.

## üéØ Goal

Tools4All attempts to enables function calling to non capable Ollama served LLMs. It works by:

1. Extending ollama.Client and overriding the chat method
2. Detecting if the LLM is capable of function calling
3. Injecting tool descriptions into the system prompt if the LLM is not capable of function calling
4. Parsing the LLM's response to extract tool calls and inject them into ollama.ChatResponse
5. Parsing role "tool" message items to extract tool results

This approach allows function calling with models that lack native tool support.

## Installation

```bash
pip install tools4all
```

Use as you would use ollama python library

### üì¶ Dependencies

- `ollama`: Python client for Ollama
- `pydantic`: Data validation and settings management
- `rich`: For pretty printing (optional)

## Usage

```python
from tools4all import Client

# Actual function that may be called
def get_weather(location):
    # Your implementation here
    return f"The weather in {location} is snowy"

# Define the tool description
tool = {
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the current weather",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g., San Francisco, CA"
                }
            },
            "required": ["location"]
        }
    }
}

# Create a Tools4All client
client = Client(host='http://127.0.0.1:11434')

# Process a user prompt
prompt = "What's the weather like in San Francisco?"
model = "phi4:latest" # set the ollama model to use

# Initialize conversation history
messages=[
    {"role": "user", "content": prompt}
]

# Query the model
response = client.chat(
    messages=messages,
    tools=[tool],
    model=model
)

# handle tool calls
if response.message.tool_calls:
    for i, tool_call in enumerate(response.message.tool_calls):
        # Generate tool call id
        tool_call_id = f"tool_call_{i}"
        
        # Handle get_weather tool call
        if tool_call.function.name == "get_weather":
            location = tool_call.function.arguments["location"]
            weather = get_weather(location)
            
            # Add tool result to messages
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "name": tool_call.function.name,
                    "content": weather
                }
            )

    # Query the model again with the updated messages
    response = client.chat(
        messages=messages,
        model=model
    )

# Print the response content
print(response.message.content)
```

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgements

- [Ollama](https://github.com/ollama/ollama) for providing the LLM backend
- [Pydantic](https://github.com/pydantic/pydantic) for data validation
