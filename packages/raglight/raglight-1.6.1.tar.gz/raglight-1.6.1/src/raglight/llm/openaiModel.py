from __future__ import annotations
from typing import Optional, Dict, Any
from typing_extensions import override
from ..config.settings import Settings
from .llm import LLM
from json import dumps
import logging
from openai import OpenAI


class OpenAIModel(LLM):
    """
    A subclass of LLM that uses OpenAI as the backend for text generation.

    This class provides an interface to interact with a local OpenAI server,
    enabling text generation with various models supported by OpenAI.

    Attributes:
        model_name (str): The name of the model to use with OpenAI.
        role (str): The role of the user in the conversation, typically "user".
        system_prompt (str): The system prompt to use for text generation.
        model (OpenAI): The OpenAI client configured to interact with the local OpenAI server.
    """

    def __init__(
        self,
        model_name: str,
        system_prompt: Optional[str] = None,
        system_prompt_file: Optional[str] = None,
        role: str = "user",
    ) -> None:
        """
        Initializes an instance of OpenAIModel.

        Args:
            model_name (str): The name of the model to use with OpenAI.
            system_prompt (Optional[str]): The system prompt to use. If not provided, it will be loaded from system_prompt_file or use the default value.
            system_prompt_file (Optional[str]): The path to the file to load the system prompt from. If provided, it takes precedence over system_prompt.
            role (str): The role of the user in the conversation, defaults to "user".
        """
        super().__init__(model_name)
        logging.info(f"Using OpenAI with {model_name} model ðŸ¤–")
        self.role: str = role
        self.system_prompt: str = ""
        if system_prompt_file is not None:
            self.system_prompt = self.load_system_prompt(system_prompt_file)
        elif system_prompt is not None:
            self.system_prompt = system_prompt
        else:
            self.system_prompt = Settings.DEFAULT_SYSTEM_PROMPT

    @override
    def load(self) -> OpenAI:
        """
        Loads the OpenAI client configured for the local OpenAI server.

        Returns:
            OpenAI: The client object to interact with OpenAI.
        """
        return OpenAI(
            base_url=Settings.DEFAULT_OPENAI_CLIENT, api_key=Settings.OPENAI_API_KEY
        )

    @override
    def generate(self, input: Dict[str, Any]) -> str:
        """
        Generates text using the OpenAI model.

        Args:
            input (Dict[str, Any]): The input data for text generation, which will be converted to JSON.

        Returns:
            str: The text generated by the model.
        """
        new_input = dumps(input)
        response = self.model.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": new_input},
            ],
            temperature=0.7,
        )
        return response.choices[0].message.content

    @staticmethod
    def load_system_prompt(filePath: str) -> str:
        """
        Loads the system prompt from a file.

        Args:
            filePath (str): The path to the file containing the system prompt.

        Returns:
            str: The content of the file as the system prompt.
        """
        with open(filePath, "r", encoding="utf-8") as file:
            prompt = file.read()
        return prompt
