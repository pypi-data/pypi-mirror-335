Metadata-Version: 2.4
Name: mingru-keras
Version: 0.1.3
Summary: This package contains a Keras 3 implementation of the minGRU layer, a minimal and parallelizable version of the gated recurrent unit (GRU).
Author-email: Boris Reuderink <boris@cortext.nl>
License-File: LICENSE
Requires-Python: >=3.10
Requires-Dist: keras>=3.5.0
Provides-Extra: jax
Requires-Dist: jax>=0.4.34; extra == 'jax'
Provides-Extra: metal
Requires-Dist: tensorflow-macos>=2.16.2; extra == 'metal'
Requires-Dist: tensorflow-metal>=1.1.0; extra == 'metal'
Provides-Extra: test
Requires-Dist: pytest>=8.3.3; extra == 'test'
Description-Content-Type: text/markdown

# MinGRU Implementation in Keras

This repository contains a Keras implementation of the minGRU model, a minimal
and parallelizable version of the traditional Gated Recurrent Unit (GRU)
architecture. The minGRU model is based on the research paper ["Were RNNs All We
Needed?"](https://arxiv.org/abs/2410.01201) that revisits traditional recurrent
neural networks and modifies them to be efficiently trained in parallel.

## Features

* Minimal GRU architecture with significantly fewer parameters than traditional GRUs.
* Fully parallelizable during training, achieving faster training times.
* Compatible with Keras 3.

## Installation

This project uses uv to manage dependencies. To install the required
dependencies, run:

```bash
pip install --upgrade mingru-keras
```

## Usage

To use the MinGRU model in your own project, simply import the `MinGRU` class
and use it as you would any other Keras layer.

## Example

```python
>>> import keras
>>> from mingru_keras import MinGRU
>>> layer = MinGRU(units=64)
>>> X = keras.random.normal((32, 1000, 8))
>>> layer(X).shape
(32, 1000, 64)

```

## Contributing

Contributions are welcome! If you'd like to report a bug or suggest a feature,
please open an issue or submit a pull request.