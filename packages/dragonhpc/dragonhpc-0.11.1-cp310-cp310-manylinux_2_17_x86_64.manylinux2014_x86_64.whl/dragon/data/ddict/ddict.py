"""
The Distributed Dictionary is a performant and distributed key-value store
that is available to applications and workflows written for the Dragon ecosystem.

This is Dragon's specialized implementation based on the Dragon file-like interface
which relies on Dragon Channels. The Distributed Dictionary works like a standard
Python dictionary except that the data that it holds may span multiple nodes and be
larger than any one node can hold.

The internals of the distributed dictionary rely on several processes include a
single orchestrator process and one or more manager processes. Each client attaches
to managers on an as-needed basis. Clients discover managers by attaching to the
serialized descriptor of a Distributed Dictionary. When using a Distributed Dictionary
in Python, the dictionary will be automatically pickled/serialized and sent to new
processes in the same way a Queue or other objects can be passed as parameters in
multiprocessing.

While the Distributed Dictionary does its best to evenly distributed data across all
managers, a localized wrapper class can be used to direct key/value pairs to user
chosen managers. See the Distributed Dictionary documentation for more details.

"""

import sys
import math
import logging
import traceback
import cloudpickle
import pickle
import time
import socket
import os
import copy
from dataclasses import dataclass, field
import multiprocessing as mp
import heapq
from types import FunctionType

from ...utils import b64decode, b64encode, hash as dragon_hash, get_local_kv
from ...infrastructure.parameters import this_process
from ...infrastructure import messages as dmsg
from ...infrastructure import policy
from ...channels import Channel
from ...native.process import Popen
from ...dlogging.util import setup_BE_logging, DragonLoggingServices as dls
from ...dlogging.logger import DragonLoggingError
from ...native.machine import Node
from dragon.native.process_group import ProcessGroup
from dragon.infrastructure.policy import Policy
from dragon.native.process import ProcessTemplate
from dragon.globalservices.node import query_all

from ... import fli
from dragon.fli import PickleWriteAdapter, PickleReadAdapter
from ...rc import DragonError

log = None
KEY_HINT = 1
VALUE_HINT = 2

# This is a default timeout value that is used for send/receive operations.
# Timeouts can be specified if needed by passing in a timeout on the
# distributed dictionary creation. The timeout applies to all operations that
# could timeout in the distributed dictionary. Likely causes of timeouts are
# a manager being overfilled, but some care is taken that does not occur.
# A timeout of None indicates to wait forever. Otherwise, positive timeout
# values are in seconds.
DDICT_DEFAULT_TIMEOUT = None

# This is the default size of a distributed dictionary which would normally be
# overridden.
DDICT_MIN_SIZE = 3 * 1024**2  # 3 MB


# This is the generic error that all other Distributed Dictionary specific
# errors inherit from. Other types of exceptions my be raised while using the
# Distributed Dictionary, but specific errors generated by this code
# are provided here.
class DDictError(DragonLoggingError):
    def __str__(self):
        return f"DDict Exception: {self.msg}\n*** Dragon C-level Traceback: ***\n{self.lib_msg}\n*** End C-level Traceback: ***\nDragon Error Code: {self.lib_err}"


# This will be raised when a Distributed Dictionary manager has filled to
# capacity. To rectify this you may need to increase the overall size of the
# dictionary and/or devise a better distributed hashing function.
class DDictManagerFull(DDictError):
    pass


# Timeout errors that occur may be either the generic TimeoutError or
# some exception that inherits from TimeoutError, including the
# DDictTimeoutError given below. If catching these errors in your program
# it is probably best to catch the generic TimeoutError so you catch
# all types of timeout errors.
class DDictTimeoutError(DDictError, TimeoutError):
    pass


# KeyErrors that can also be caught as DDictErrors.
class DDictKeyError(DDictError, KeyError):
    def __init__(self, err, msg, key):
        super().__init__(err, msg)
        super(KeyError, self).__init__(key)


# A Checkpoint Sync error can occur when a client is trying to work with
# a dictionary checkpoint that has now been retired from the working set
# of checkpoints.
class DDictCheckpointSync(DDictError):
    pass


class DDictUnableToCreateDDict(DDictError):
    pass


# Make the key a little more precise by stripping off pickled data.
def strip_pickled_bytes(byte_str):
    if byte_str[-2:] == b"\x94.":
        byte_str = byte_str[:-2]

    if byte_str[-1:] == b".":
        byte_str = byte_str[:-1]

    if byte_str[:3] == b"\x80\x05\x95":
        byte_str = byte_str[3:]

    return byte_str


@dataclass
class DDictManagerStats:
    """
    Included in manager stats are the manager identifier (0 to num_managers-1), the
    total number of bytes in the manager's pool, the total used bytes in the manager's pool,
    the number of key/value pairs stored in the manager, and the dictionary of free blocks. The
    free blocks has the block size and the number of free blocks of that size. If the pool is
    empty, then there will be one free block of the same size as the total number of bytes of the
    manager's pool. Otherwise, free blocks can be used to see the amount of fragmentation within
    the pool by looking at the various block sizes and number of blocks available. NOTE: Any
    larger block (except the smallest block size) can be split into two smaller blocks for
    smaller allocations.
    """

    manager_id: int
    hostname: str
    total_bytes: int
    total_used_bytes: int
    pool_free_space: int
    pool_utilization: float
    num_keys: int
    free_blocks: dict
    max_pool_allocations: int
    max_pool_allocations_used: int
    current_pool_allocations_used: int


# A SentinelQueue is a queue that raise EOFError when end of
# file is reached. It knows to do this by writing a sentinel
# into the queue when the queue is closed. A SentinelQueue
# contains a multiprocessing Queue. Other methods could
# be implemented for SentinelQueue, but are not needed in
# this example.
class SentinelQueue:
    _SENTINEL = "sentinel_queue_sentinel"

    def __init__(self):
        self._queue = mp.Queue()
        self._put_called = False
        self._get_called = False
        self._sentinel_sent = False
        self._closed = False
        self._sentinel_received = False

    def get(self):
        if self._closed:
            raise RuntimeError("Cannot get an item from a closed SentinelQueue")

        if not self._get_called:
            self._shutdown_event = self._queue.get()
            self._get_called = True

        item = self._queue.get()
        if item == SentinelQueue._SENTINEL:
            self._sentinel_received = True
            raise EOFError("SentinelQueue EOF")

        return item

    def put(self, item):
        if self._closed:
            raise RuntimeError("Cannot put an item on a closed SentinelQueue")

        if not self._put_called:
            self._shutdown_event = mp.Event()
            self._queue.put(self._shutdown_event)
            self._put_called = True

        if self._shutdown_event.is_set():
            raise EOFError("SentinelQueue receiving end closed.")

        self._queue.put(item)

    def close(self):
        if self._closed:
            return

        if self._put_called and not self._sentinel_sent:
            self._sentinel_sent = True
            self._queue.put(SentinelQueue._SENTINEL)

        if self._get_called and not self._sentinel_received:
            # This is a receiving end of a SentinelQueue,
            # so set the event and empty the queue.
            self._shutdown_event.set()
            try:
                while True:
                    self.get()
            except:
                pass

        try:
            self._queue.close()
        except:
            pass

        self._closed = True


# The PQEntry class is needed by the priority
# queue which is used to always know which queue
# to get the next value from. The __lt__ orders
# the priority queue elements by their original
# values. But the queue index of where the value
# came from is carried along with the value in the
# priority queue so the merging algorithm knows where
# to get the next value. In this way, the total number
# of entries in the priority queue is never more than the
# fanin value of the MergePool.
class PQEntry:
    def __init__(self, value, queue_index, comparator):
        self._value = value
        self._queue_index = queue_index
        self._comparator = comparator

    def __lt__(self, other):
        # The comparator is supplied by the user and must take
        # to arguments to be compared and return True or False.
        return self._comparator(self._value, other._value)

    @property
    def queue_index(self):
        return self._queue_index

    @property
    def value(self):
        return self._value

    def __repr__(self):
        return "PQEntry(" + str(self.value) + "," + str(self.queue_index) + "," + str(self._comparator) + ")"

    def __str__(self):
        return repr(self)


# This is here for type hinting below. It is redefined right below.
class DDict:
    pass


def filter_manager(dd, pickled_src_code, pickled_src_args, out_queue, manager_id):
    try:
        src_code = cloudpickle.loads(pickled_src_code)
        src_args = cloudpickle.loads(pickled_src_args)
        my_manager = dd.manager(manager_id)
        args = (my_manager, out_queue) + src_args
        src_code(*args)
    except Exception as ex:
        tb = traceback.format_exc()
        print(
            "There was an exception in filter_manager: %s\n Traceback: %s" % (ex, tb),
            flush=True,
            file=sys.stderr,
        )
        raise ex
    finally:
        out_queue.close()


def filter_aggregator(
    dd: DDict, managers_hosts, branching_factor, pickled_src_code, pickled_src_args, pickled_comparator, out_queue
):
    try:
        comparator = cloudpickle.loads(pickled_comparator)
        # managers_hosts is a list of tuples (manager_id, hostname)
        if len(managers_hosts) == 1:
            # It was already a in a process started on the manager node, so just call
            # the filter_manager code.
            return filter_manager(dd, pickled_src_code, pickled_src_args, out_queue, managers_hosts[0][0])

        # At larger scales, it is useful create more than one of these processes,
        # themselves in a process group where each of them is given an output queue and
        # a set of managers to merge (instead of merging all managers in one process).
        # Then the second-level merging would merge their managers, writing the merged
        # values to their output queues while a third level merge is done to merge all the
        # second level merges together. In this way, this algorithm can scale to whatever
        # size is necessary.

        filter_queues = []
        shutdown_events = []

        if len(managers_hosts) > branching_factor:

            num_divisions = len(managers_hosts) // branching_factor

            if num_divisions == 1:
                num_divisions = 2

            if num_divisions > branching_factor:
                num_divisions = branching_factor

            division_sz = len(managers_hosts) // num_divisions

            if division_sz * num_divisions < len(managers_hosts):
                division_sz += 1

            grp = ProcessGroup(restart=False)

            while len(managers_hosts) > 0:
                managers_subset = managers_hosts[:division_sz]
                managers_hosts = managers_hosts[division_sz:]
                filter_queue = SentinelQueue()
                filter_queues.append(filter_queue)

                node_name = managers_subset[0][1]
                local_policy = Policy(
                    placement=Policy.Placement.HOST_NAME,
                    host_name=node_name,
                )
                grp.add_process(
                    nproc=1,
                    template=ProcessTemplate(
                        target=filter_aggregator,
                        args=(
                            dd,
                            managers_subset,
                            branching_factor,
                            pickled_src_code,
                            pickled_src_args,
                            pickled_comparator,
                            filter_queue,
                        ),
                        policy=local_policy,
                    ),
                )
        else:

            grp = ProcessGroup(restart=False)

            for manager_id, node_name in managers_hosts:
                filter_queue = SentinelQueue()
                filter_queues.append(filter_queue)

                local_policy = Policy(
                    placement=Policy.Placement.HOST_NAME,
                    host_name=node_name,
                )
                grp.add_process(
                    nproc=1,
                    template=ProcessTemplate(
                        target=filter_manager,
                        args=(dd, pickled_src_code, pickled_src_args, filter_queue, manager_id),
                        policy=local_policy,
                    ),
                )

        grp.init()
        grp.start()
        # prime the priority queue. The values coming from the
        priority_queue = []

        try:
            for i in range(len(filter_queues)):
                try:
                    item = filter_queues[i].get()
                    heapq.heappush(priority_queue, PQEntry(item, i, comparator))
                except EOFError:
                    pass

            # merge the values from different procs

            while len(priority_queue) > 0:
                # If items are not in strictly decreasing order for values, then
                # you need to reverse the < to a > in the PQEntry __lt__ method.
                item = heapq.heappop(priority_queue)
                out_queue.put(item.value)

                try:
                    next = filter_queues[item.queue_index].get()
                    heapq.heappush(priority_queue, PQEntry(next, item.queue_index, comparator))
                except EOFError:
                    pass
        except EOFError:
            # This could happen if the outqueue is closed before this proc is done.
            pass
        except Exception as ex:
            tb = traceback.format_exc()
            print(
                "There was an exception in filter_aggregator priming: %s\n Traceback: %s" % (ex, tb),
                flush=True,
                file=sys.stderr,
            )
            raise ex

        out_queue.close()

        while len(filter_queues) > 0:
            filter_queue = filter_queues.pop()
            try:
                filter_queue.close()
                del filter_queue
            except:
                pass

        grp.join()
        grp.close()

    except Exception as ex:
        tb = traceback.format_exc()
        print(
            "There was an exception in filter_aggregator: %s\n Traceback: %s" % (ex, tb),
            flush=True,
            file=sys.stderr,
        )
        raise ex


class FilterIterator:
    def __init__(self, out_queue):
        self._queue = out_queue

    def __iter__(self):
        return self

    def __next__(self):
        try:
            value = self._queue.get()
            return value
        except EOFError:
            raise StopIteration("End of filter stream")

    def close(self):
        self._queue.close()


class FilterContextManager:
    def __init__(self, filter_proc, filter_queue):
        self._proc = filter_proc
        self._queue = filter_queue

    def __enter__(self):
        # Simulate resource acquisition
        self._iter = FilterIterator(self._queue)
        return self._iter

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Simulate resource release
        self._iter.close()
        self._proc.join()
        return False


class DDict:
    """
    The Distributed Dictionary provides a key/value store that is distributed across a series
    of managers and one nodes of a Dragon run-time.
    The goal is to evenly distribute data across all managers to provide a scalable
    implementation of a dictionary with a high degree of allowable parallelism. Clients attach
    to the Distributed Dictionary and store key/value pairs in it just like accessing a local
    dictionary in Python. However, the Distributed Dictionary goes beyond what the standard Python
    dictionary supports by including support for distributing data, checkpointing, and various
    other optimization opportunities for specific applications.
    """

    def __init__(
        self,
        managers_per_node: int = 1,
        n_nodes: int = 1,
        total_mem: int = DDICT_MIN_SIZE,
        *,
        working_set_size: int = 1,
        wait_for_keys: bool = False,
        wait_for_writers: bool = False,
        policy: policy.Policy or list[policy.Policy] = None,
        managers_per_policy: int = 1,
        persist_freq: int = 0,
        name: str = "",
        timeout: float = DDICT_DEFAULT_TIMEOUT,
        trace: bool = False,
        restart: bool = False,
    ) -> None:
        """
        Construct a Distributed Dictionary to be shared amongst distributed processes running
        in the Dragon Runtime. The distributed dictionary creates the specified number of managers
        and shards the data across all managers. The total memory of the dictionary is split
        across all the managers, so you want to allocate more space than is required by perhaps
        30 percent, but that should be determined via some experimentation and depends on the
        application being developed. See the Dragon documentation's section on the Distributed
        Dictionary design for more details about creating and using a distributed dictionary.

        :param managers_per_node: The number of managers on each node. The
             total_mem is divided up amongst the managers. If a list of
             policies is provided then this is the number of managers
             per policy. Each policy could be used to start more than
             one manager per node, in a potentially heterogeneous way.
             Defaults to 1.

        :param n_nodes: The number of nodes that will have managers
             deployed on them. This must be set to None if a list of policies is
             provided. Defaults to 1.

        :param total_mem: The total memory in bytes that will be
             sharded evenly across all managers. Defaults to DDICT_MIN_SIZE
             but this is really a minimum size for a single manager
             and should be specified by the user.

        :param working_set_size: Not implemented yet. This sets the
             size of the checkpoint, in memory, working set. This
             determines how much state each manager will keep
             internally. This is the number of different, simultaneous
             checkpoints that may be active at any point in time.
             Defaults to 1.

        :param wait_for_keys: Not implemented yet. Setting this to
             true means that each manager will keep track of a set of
             keys at each checkpoint level and clients advancing to a
             new checkpoint level will block until the set of keys at
             the oldest, retiring working set checkpoint are all
             written. By specifying this all clients will remain in
             sync with each other relative to the size of the working
             set. Defaults to False. It is also possible to store
             key/values that are not part of the checkpointing set of
             key/values. Those keys are called persistent keys and
             will not be affected by setting this argument to true.
             Specifying wait_for_keys also means that readers will block while
             waiting for a non-persistent key to be written until the
             key is found or a timeout occurs.

        :param wait_for_writers: Not implemented yet. Setting this
             to true means that each manager will wait for a set of
             clients to have all advanced their checkpoint id beyond
             the oldest checkpointing id before retiring a checkpoint
             from the working set. Setting this to true will cause
             clients that are advancing rapidly to block while others
             catch up. Defaults to False.

        :param policy: A policy
             can be supplied for starting the managers. Please read about policies in the
             Process Group documentation. Managers are started via a
             Process Group and placement of managers and other
             characteristics can be controlled via a policy or list of policies.
             If a list of policies is given then managers_per_node processes are
             started for each policy. Defaults to None which applies a
             Round-Robin policy.

        :param managers_per_policy: The number of managers started
             with each policy when a list of policies is provided. The total_mem
             is divided up evenly amongst the managers. This is the Defaults to
             1.

        :param persist_freq: Not implemented yet. This is the
             frequency that a checkpoint will be persisted to disk.
             This is independent of the working set size and can be
             any frequency desired. Defaults to 0 which means that no
             persisting will be done.

        :param name: Not implemented yet. This is a
             base file name to be applied to persisted state for the
             dictionary. This base name along with a checkpoint number
             is used to restore a distributed dictionary from a
             persisted checkpoint. Defaults to "".

        :param timeout: This is a timeout that will be used for
             all timeouts on the creating client and all managers
             during communication between the distributed components
             of the dictionary. New clients wishing to set their own
             timeout can use the attach method to specify their own
             local timeout. Defaults to None (block).

        :param trace: Defaults to False. If set to true, all
             interaction between clients and managers is logged. This results
             in large logs, but may help in debugging.

        :returns: None and a new instance of a distributed dictionary is initialized.

        :raises AttributeError: If incorrect parameters are supplied.
        :raises RuntimeError: If there was an unexpected error during initialization.
        """
        self.setup_logging()

        # This is the pattern used in the pydragon_perf.pyx file
        # It works, but may need review if it's the way we want to do it

        # This block turns on client log for the initial client that creates the dictionary
        try:
            if type(managers_per_node) is not int and type(managers_per_policy) is not int:
                raise AttributeError(
                    "When creating a Dragon Distributed Dict you must provide managers_per_node or managers_per_policy"
                )

            if type(total_mem) is not int:
                raise AttributeError("When creating a Dragon Distributed Dict you must provide total_mem")

            if type(policy) is not list and type(n_nodes) is not int:
                raise AttributeError(
                    "When creating a Dragon Distributed Dict if you provide a single policy you must provide n_nodes"
                )

            if type(policy) is list and (n_nodes is not None or managers_per_node is not None):
                raise AttributeError(
                    "When creating a Dragon Distributed Dict if you provide a list of policies you must provide n_nodes = None and managers_per_node=None"
                )

            if restart and policy is not None:
                raise AttributeError(
                    "When restarting a Dragon Distributed Dict if you specified restart you must not provide policy."
                )

            # we overwrite n_nodes and managers_per_node so that we get the correct division of the total memory among the managers that are launched as part of the process group.
            if isinstance(policy, list):
                n_nodes = len(policy)
                managers_per_node = managers_per_policy

            # Start the Orchestrator and capture its serialized descriptor so we can connect to it.
            self._orc_proc = Popen(
                executable=sys.executable,
                args=[
                    "-c",
                    f"import dragon.data.ddict.orchestrator as orc; orc.start({managers_per_node}, {n_nodes}, {total_mem}, {trace})",
                ],
                stdout=Popen.PIPE,
            )

            # Read the serialized FLI of the orchestrator.
            ddict = self._orc_proc.stdout.recv().strip()

            self._orc_connector = fli.FLInterface.attach(b64decode(ddict))
            self._args = (
                working_set_size,
                wait_for_keys,
                wait_for_writers,
                policy,
                persist_freq,
                name,
                timeout,
                restart,
            )

            self._wait_for_keys = wait_for_keys
            self._wait_for_writers = wait_for_writers
            self._init_props((True, ddict, timeout, trace))
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There is an exception initializing ddict: %s\nTraceback: %s", ex, tb)
            raise RuntimeError(f"There is an exception initializing ddict: {ex}\nTraceback: {tb}\n")

    def __setstate__(self, args):
        self._init_props(args)

    def _init_props(self, args):
        self._creator, serialized_orc, timeout, trace = args
        # -1 indicates to use the default timeout since the default
        # is sometimes None and None can't be passed through capnproto
        if timeout == -1:
            self._timeout = DDICT_DEFAULT_TIMEOUT
        else:
            self._timeout = timeout

        self.setup_logging()

        self._managers = dict()
        self._tag = 0
        self._chkpt_id = 0
        self._destroyed = False
        self._detached = False
        self._timeout = timeout
        self._trace = trace
        self._chosen_manager = None
        self._key_pickler = None
        self._value_pickler = None

        try:
            self._traceit("Connecting to ddict.")
            return_channel = Channel.make_process_local()
            buffered_return_channel = Channel.make_process_local()
            self._stream_channel = Channel.make_process_local()

            self._default_pool = return_channel.get_pool()

            self._return_connector = fli.FLInterface(main_ch=return_channel)
            self._serialized_return_connector = b64encode(self._return_connector.serialize())

            self._buffered_return_connector = fli.FLInterface(
                main_ch=buffered_return_channel, use_buffered_protocol=True
            )
            self._serialized_buffered_return_connector = b64encode(self._buffered_return_connector.serialize())

            self._serialized_orc = serialized_orc
            if self._creator:
                self._create(b64encode(cloudpickle.dumps((self._args))))
            else:
                self._orc_connector = fli.FLInterface.attach(b64decode(serialized_orc))

            self._client_id = None

            # if the client has a local manager, it is the main manager
            self._has_local_manager = False
            self._local_manager = None
            self._main_manager = None
            self._manager_nodes = []

            self._get_main_manager()
            self._register_client_to_main_manager(timeout)
        except DDictUnableToCreateDDict as ex:
            tb = traceback.format_exc()
            raise DDictUnableToCreateDDict(DragonError.FAILURE, "Failed to create dictionary.")
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There is an exception __setstate__ of ddict: %s\n Traceback: %s\n", ex, tb)
            except:
                pass
            raise RuntimeError(f"There is an exception __setstate__ of ddict.")

    def __getstate__(self):
        return (False, self.serialize(), self._timeout, self._trace)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        return

    def setup_logging(self):
        # This block turns on a client log for each client
        global log
        if log is None:
            fname = f"{dls.DD}_{socket.gethostname()}_client_{str(this_process.my_puid)}.log"
            setup_BE_logging(service=dls.DD, fname=fname)
            log = logging.getLogger(str(dls.DD))

    def __del__(self):
        try:
            self.detach()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(
                    "There was an exception while terminating the Distributed Dictionary. Exception is %s\n Traceback: %s\n",
                    ex,
                    tb,
                )
            except:
                pass

    def _create(self, pickled_args):
        msg = dmsg.DDCreate(self._tag_inc(), respFLI=self._serialized_buffered_return_connector, args=pickled_args)
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)

        if resp_msg.err == DragonError.FAILURE:
            raise DDictUnableToCreateDDict(resp_msg.err, resp_msg.errInfo)

        if resp_msg.err != DragonError.SUCCESS:
            self._cleanup()
            raise RuntimeError(f"Failed to create dictionary! {resp_msg.errInfo}")

    def _cleanup(self):
        if self._creator:
            try:
                self._orc_proc.wait()
                log.debug("joined orc proc")
            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug("There was an exception while joining orchestrator process: %s\n Traceback: %s", ex, tb)
                except:
                    pass

    def _traceit(self, *args, **kw_args):
        if self._trace:
            log.log(logging.INFO, *args, **kw_args)

    def _get_main_manager(self):  # SHGetKV
        try:
            serialized_main_manager = get_local_kv(key=self._serialized_orc)
            self._main_manager_connection = fli.FLInterface.attach(b64decode(serialized_main_manager))
            self._has_local_manager = True
        except KeyError as e:
            # no manager on the node, get a random manager from orchestrator
            try:
                log.info(
                    "Got KeyError during bringup, sending get random manager request to orchestrator. Exception was %s",
                    e,
                )
            except:
                pass
            msg = dmsg.DDRandomManager(self._tag_inc(), respFLI=self._serialized_buffered_return_connector)
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
            if resp_msg.err != DragonError.SUCCESS:
                raise RuntimeError("Client failed to get manager from orchestrator")
            self._main_manager_connection = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_to_main_manager(self, timeout):  # client ID assigned here
        msg = dmsg.DDRegisterClient(
            self._tag_inc(),
            respFLI=self._serialized_return_connector,
            bufferedRespFLI=self._serialized_buffered_return_connector,
        )  # register client to the manager (same node)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection)
        if resp_msg.err != DragonError.SUCCESS:
            raise RuntimeError("Client failed to connect to main manager.")
        # -1 indicates to use the default timeout since the default
        # is sometimes None and None can't be passed through capnproto
        if timeout == -1:
            self._timeout = resp_msg.timeout
        self._client_id = resp_msg.clientID
        self._num_managers = resp_msg.numManagers
        for serialized_node in resp_msg.managerNodes:
            self._manager_nodes.append(cloudpickle.loads(b64decode(serialized_node)))
        self._name = resp_msg.name
        self._managers[resp_msg.managerID] = self._main_manager_connection
        self._main_manager = resp_msg.managerID
        if self._has_local_manager:
            self._local_manager = resp_msg.managerID

    def _connect_to_manager(self, manager_id):
        msg = dmsg.DDConnectToManager(self._tag_inc(), clientID=self._client_id, managerID=manager_id)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection)
        if resp_msg.err != DragonError.SUCCESS:
            raise RuntimeError(f"Client {self._client_id} failed to coonect to manager {manager_id}")

        self._managers[manager_id] = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_ID_to_manager(self, manager_id):
        try:
            msg = dmsg.DDRegisterClientID(
                self._tag_inc(),
                clientID=self._client_id,
                respFLI=self._serialized_return_connector,
                bufferedRespFLI=self._serialized_buffered_return_connector,
            )
            resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id])
            if resp_msg.err != DragonError.SUCCESS:
                raise Exception(f"Failed to register client {self._client_id} to manager {manager_id}")
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(
                    "There was an exception registering client ID %s with manager: %s \n Traceback: %s",
                    self._client_id,
                    ex,
                    tb,
                )
            except:
                pass
            raise RuntimeError(
                f"There was an exception registering client ID {self._client_id=} with manager: {ex} \n Traceback: {tb}"
            )

    def _check_manager_connection(self, manager_id=None, all=False):
        if all:
            for manager_id in range(self._num_managers):
                if manager_id not in self._managers:
                    self._connect_to_manager(manager_id)
                    self._register_client_ID_to_manager(manager_id)
        elif not (manager_id in self._managers):
            self._connect_to_manager(manager_id)
            self._register_client_ID_to_manager(manager_id)

    def _choose_manager_pickle_key(self, key):
        # Check to see if there is a user-defined hash function. If so, then
        # assume it is deterministic and the same across all nodes and use it.
        if self._key_pickler is None:
            pickled_key = cloudpickle.dumps(key)
            stripped_key = strip_pickled_bytes(pickled_key)
        else:
            pickled_key = self._key_pickler.dumps(key)
            stripped_key = None  # Not used if the user provided a custom pickler

        if self._chosen_manager is None:
            if self._key_pickler is None:
                # We will try this first if no chosen manager. Might not be instance of
                # one of these. If not we fall down to code below which is what we want
                # when a manager is chosen.
                try:
                    if isinstance(key, int):
                        sz = round(math.log(key, 2))
                        return (dragon_hash(key.to_bytes(sz, sys.byteorder)) % self._num_managers, pickled_key)
                except:
                    pass
                try:
                    if isinstance(key, str):
                        return (dragon_hash(key.encode("utf-8")) % self._num_managers, pickled_key)
                except:
                    pass
            else:
                return (dragon_hash(pickled_key) % self._num_managers, pickled_key)

        if self._chosen_manager is not None:
            manager_id = self._chosen_manager
        else:
            if self._key_pickler is None:
                hash_val = dragon_hash(stripped_key)
            else:
                hash_val = dragon_hash(pickled_key)
            manager_id = hash_val % self._num_managers

        return (manager_id, pickled_key)

    def _tag_inc(self):
        tag = self._tag
        self._tag += 1
        return tag

    def _send(self, msglist, connection):
        self._traceit("Opening send handle.")

        if connection.is_buffered:
            strm = None
        else:
            strm = self._stream_channel

        with connection.sendh(stream_channel=strm, timeout=self._timeout) as sendh:
            for msg, arg in msglist:
                self._traceit("Sending msg: %s", msg)
                if arg is None:
                    sendh.send_bytes(msg.serialize(), timeout=self._timeout)
                else:
                    # It is a pickled value so don't call serialize.
                    sendh.send_bytes(msg, arg=arg, timeout=self._timeout)

    def _recv_resp(self, resp_set):
        self._traceit("About to open receive handle on fli to receive response.")
        done = False
        with self._buffered_return_connector.recvh(timeout=self._timeout) as recvh:
            while not done:
                resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
                msg = dmsg.parse(resp_ser_msg)
                if msg.ref not in resp_set:
                    log.info("Tossing lost/timed out response message in DDict Client: %s", msg)
                else:
                    resp_set.remove(msg.ref)
                    done = True

        self._traceit("Response: %s", msg)
        return msg

    def _recv_responses(self, resp_set, num_responses):
        msglist = []
        for _ in range(num_responses):
            resp_msg = self._recv_resp(set(resp_set))
            msglist.append(resp_msg)

        return msglist

    def _recv_dmsg_and_val(self, req_msg, key):
        self._traceit("About to open receive handle on fli to receive response and value.")
        with self._return_connector.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
            ref = -1
            while ref != req_msg.tag:
                resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
                resp_msg = dmsg.parse(resp_ser_msg)
                self._traceit("Response: %s", resp_msg)
                ref = resp_msg.ref
                if ref != req_msg.tag:
                    log.info("Tossing lost/timed out message in DDict Client: %s", resp_msg)

            if resp_msg.err != DragonError.SUCCESS:
                value = self.__missing__(key, err=resp_msg.err)
            else:
                try:
                    if self._value_pickler is None:
                        value = cloudpickle.load(
                            file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                        )
                    else:
                        value = self._value_pickler.load(
                            file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                        )
                except Exception as e:
                    tb = traceback.format_exc()
                    try:
                        log.info("Exception caught in cloudpickle load: %s \n Traceback: %s", e, tb)
                    except:
                        pass
                    raise RuntimeError(f"Exception caught in cloudpickle load: {e} \n Traceback: {tb}")

        return value

    def __missing__(self, key, *, err=DragonError.SUCCESS):
        raise DDictKeyError(err, "The key was not found", key)

    def _send_receive(self, msglist, connection):
        try:
            msg = msglist[0][0]
            tag = msg.tag
            self._send(msglist, connection)
            resp_msg = self._recv_resp(set([msg.tag]))
            return resp_msg

        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There was an exception in the _send_receive in ddict: %s\n Traceback: %s", ex, tb)
            except:
                pass
            raise RuntimeError(f"There was an exception in the _send_receive in ddict: {ex} \n Traceback: {tb}")

    def _put(self, msg, key, value):
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        try:
            self._traceit("Opening send handle to manager for put: %s", manager_id)
            with self._managers[manager_id].sendh(stream_channel=self._stream_channel, timeout=self._timeout) as sendh:
                self._traceit("Sending to manager: %s", msg)
                sendh.send_bytes(msg.serialize(), timeout=self._timeout)
                self._traceit("Sending pickled key to manager: %s", key)
                sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
                if self._value_pickler is not None:
                    self._value_pickler.dump(
                        value, file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout)
                    )
                else:
                    cloudpickle.dump(
                        value,
                        file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout),
                        protocol=pickle.HIGHEST_PROTOCOL,
                    )

        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

        try:
            resp_msg = self._recv_resp(set([msg.tag]))
        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

        if resp_msg.err == DragonError.MEMORY_POOL_FULL:
            raise DDictManagerFull(
                DragonError.MEMORY_POOL_FULL,
                f"Distributed Dictionary Manager {manager_id} is full. The key/value pair was not stored.",
            )

        elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
            raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)

        elif resp_msg.err != DragonError.SUCCESS:
            raise DDictError(
                resp_msg.err,
                "Failed to store key in the distributed dictionary.\nAdditional Information: %s" % resp_msg.errInfo,
            )

    def destroy(self, allow_restart=False) -> None:
        """
        Destroy a Distributed Dictionary instance, freeing all the resources that were allocated
        when it was created. Any clients that are still attached to the dictionary and try to do
        an operation on it will experience an exception when attempting subsequent operations.
        """
        if self._destroyed:
            return

        self._traceit("Destroying the ddict.")

        self._destroyed = True
        try:
            msg = dmsg.DDDestroy(
                self._tag_inc(),
                self._client_id,
                respFLI=self._serialized_buffered_return_connector,
                allowRestart=allow_restart,
            )
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There was an exception in the destroy: %s\n Traceback: %s", ex, tb)
            except:
                pass

        try:
            self._orc_connector.detach()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug("There was an exception while detaching orchestrator channel: %s \n Traceback: %s", ex, tb)
            except:
                pass

        # join on the orchestrator proc
        if self._creator:
            try:
                self._orc_proc.wait()
            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug("There was an exception while joining orchestrator process: %s \n Traceback: %s", ex, tb)
                except:
                    pass

    def serialize(self) -> str:
        """
        Returns a serialized, base64 encoded descriptor (i.e. string) that may be shared with other
        processes for attaching. This is especially useful when sharing with C/C++ or Fortran code
        though not all clients are available yet. Within Python you can pass the Distributed Dictionary
        to another process and it will be automatically serialized and attached so using this method
        is not needed when passing to another Python process.

        :returns: A serialized, base64 encoded string that may be used for attaching to the dictionary.
        """
        return self._serialized_orc

    @classmethod
    def attach(cls, serialized_dict: str, *, timeout: float = None, trace: bool = False) -> DDict:
        """
        Within Python you typically do not need to call this method explicitly. It will be done
        automatically when you pass a Distributed Dictionary from one process to another. However,
        you can do this explicitly if desired/needed.

        :param serialized_dict: A serialized distributed dictionary.

        :param timeout: None or a float or int value. A value of None means to wait forever. Otherwise
            it is the number of seconds to wait while an operation is performed. This timeout is applied
            to all subsequent client operations that are performed by the process that is attaching
            this DDict.

        :param trace: If True, specifies that all operations on the distributed dictionary should be
            logged in detail within the client log.

        :returns: An attached serialized dictionary.
        :rtype: DDict

        :raises TimeoutError: If the timeout expires.

        :raises Exception: Other exceptions are possible if for instance the serialized dictionary
            no longer exists.

        """
        new_client = cls.__new__(cls)
        new_client._init_props((False, serialized_dict, timeout, trace))
        return new_client

    def detach(self) -> None:
        """
        Detach from the Distributed Dictionary and free all local resources of this client. But leave
        in place the DDict for other clients and processes.
        """

        try:
            if self._destroyed or self._detached:
                return

            self._traceit("Detaching from ddict.")

            self._detached = True

            for manager_id in self._managers:
                try:
                    msg = dmsg.DDDeregisterClient(
                        self._tag_inc(), clientID=self._client_id, respFLI=self._serialized_buffered_return_connector
                    )
                    resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id])
                    if resp_msg.err != DragonError.SUCCESS:
                        log.debug("Error on response to deregister client %s", self._client_id)

                    self._managers[manager_id].detach()
                except:
                    pass

        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(
                    "There was an exception while detaching the client %s. Exception: %s\n Traceback: %s",
                    self._client_id,
                    ex,
                    tb,
                )
            except:
                pass

    @classmethod
    def synchronize_ddicts(cls, serialized_ddicts):
        """
        Synchronize managers across all parallel dictionaries.

        :param serialized_ddicts: a list of serialized dictionary.
        """

        if len(serialized_ddicts) == 0:
            raise ValueError("The list of serialized ddicts must not be empty.")

        for ser_ddict in serialized_ddicts:
            if not isinstance(ser_ddict, str):
                raise DDictError(DragonError.INVALID_ARGUMENT, "The serialized dictionary must be a string.")

        new_client = cls.__new__(cls)
        new_client.__setstate__((False, serialized_ddicts[0], DDICT_DEFAULT_TIMEOUT, False))

        tags = set()
        # send request to every ddict orchestrator
        for ser_ddict in serialized_ddicts:
            current_tag = new_client._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDGetManagers(current_tag, respFLI=new_client._serialized_buffered_return_connector)
            connection = fli.FLInterface.attach(b64decode(ser_ddict))
            new_client._send([(msg, None)], connection)
            connection.detach()

        # receive responses from all orchestrators
        resp_num = len(serialized_ddicts)
        msglist = new_client._recv_responses(tags, resp_num)
        # check each response
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise RuntimeError(resp_msg.err)

        # build empty and full manager list
        num_managers = len(msglist[0].managers)
        # Check managers with the same manager_id from all parallel dictionaries
        for manager_id in range(num_managers):
            empty_managers = []
            full_managers = []
            for resp_msg in msglist:
                # If the manager is empty, we append its fli to the empty_manager list.
                # Otherwise append its fli to full_managers.
                if resp_msg.emptyManagers[manager_id]:
                    empty_managers.append(resp_msg.managers[manager_id])
                else:
                    full_managers.append(resp_msg.managers[manager_id])

            if len(empty_managers) > 0:  # Sync manager only when there's any empty manager
                if len(full_managers) == 0:
                    raise RuntimeError(f"Failed to synchronize dictionary. No full manager for manager {manager_id}.")

                if len(full_managers) < len(empty_managers):
                    # guarantees the length of full managers is equal to or longer then empty managers.
                    full_managers = full_managers * int(1 + len(empty_managers) / len(full_managers))

                # Send sync request along with full managers fli to empty manager so that the empty manager can request data from full manager
                tags = set()
                for i in range(len(empty_managers)):
                    empty_fli = empty_managers[i]
                    full_fli = full_managers[i]
                    current_tag = new_client._tag_inc()
                    tags.add(current_tag)
                    msg = dmsg.DDManagerSync(
                        current_tag, fullManagerFLI=full_fli, respFLI=new_client._serialized_buffered_return_connector
                    )
                    connection = fli.FLInterface.attach(b64decode(empty_fli))
                    new_client._send([(msg, None)], connection)
                    connection.detach()

                # Receive sync responses from all empty managers
                resp_num = len(empty_managers)
                msglist = new_client._recv_responses(tags, resp_num)
                for resp_msg in msglist:
                    if resp_msg.err != DragonError.SUCCESS:
                        raise RuntimeError(f"Failed to synchronize dictionary! {resp_msg.errInfo}")

    def manager(self, id: int) -> DDict:
        """
        Return a version of the current ddict that will always choose the given manager for storing and retrieving data.

        :param id: The manager id of the chosen manager.
        :raises Exception: If the manager id is not a valid id.
        """

        if id < 0 or id >= self._num_managers:
            raise DDictError(DragonError.INVALID_ARGUMENT, f"The value {id} is not a valid manager id.")
        mgr_dd = copy.copy(self)
        mgr_dd._chosen_manager = id

        return mgr_dd

    def pickler(self, key_pickler=None, value_pickler=None) -> DDict:
        pickler_dd = copy.copy(self)
        pickler_dd._key_pickler = key_pickler
        pickler_dd._value_pickler = value_pickler

        return pickler_dd

    def __setitem__(self, key: object, value: object) -> None:
        """
        Store the key/value pair in the current checkpoint within the Distributed Dictionary.

        :param key: The key of the pair. It must be serializable.
        :param value: the value of the pair. It also must be serializable.
        :raises Exception: Various exceptions can be raised including TimeoutError.
        """
        msg = dmsg.DDPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=False)
        self._put(msg, key, value)

    def __getitem__(self, key: object) -> object:
        """
        Get the value that is associated with the given key.

        :param key: The key of a stored key/value pair.
        :returns: The value associated with the key.
        :raises Exception: Various exceptions can be raised including TimeoutError and KeyError.
        """
        msg = dmsg.DDGet(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None), (pickled_key, KEY_HINT)], self._managers[manager_id])

        value = self._recv_dmsg_and_val(msg, key)
        return value

    def __contains__(self, key: object) -> bool:
        """
        Returns True if key is in the Distributed Dictionary and False otherwise.

        :param key: A possible key stored in the DDict.
        :returns bool: True or False depending on if the key is there or not.
        :raises: Various exceptions can be raised including TimeoutError.
        """
        msg = dmsg.DDContains(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        resp_msg = self._send_receive([(msg, None), (pickled_key, KEY_HINT)], connection=self._managers[manager_id])

        if resp_msg.err == DragonError.SUCCESS:
            return True
        elif resp_msg.err == DragonError.KEY_NOT_FOUND:
            return False
        elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
            raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)

        raise RuntimeError(resp_msg.err)

    def __len__(self) -> int:
        """
        Returns the number of keys stored in the entire Distributed Dictionary.

        :returns int: The number of stored keys in the current checkpoint plus any persistent keys.
        :raises: Various exceptions can be raised including TimeoutError.
        """
        self._traceit("Getting length from ddict")
        tag = self._tag_inc()
        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDLength(
            tag,
            clientID=self._client_id,
            chkptID=self._chkpt_id,
            respFLI=self._serialized_buffered_return_connector,
            broadcast=broadcast,
        )
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        length = 0
        for resp_msg in msglist:
            if resp_msg.err == DragonError.SUCCESS:
                length += resp_msg.length
            elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)
            else:
                raise RuntimeError(resp_msg.err)
        return length

    def __delitem__(self, key: object) -> None:
        """
        Deletes a key/value pair from the Distributed Dictionary if it exists.

        :raises: Various exceptions can be raised including TimeoutError and KeyError.
        """
        self.pop(key)

    def _mark_as_drained(self, manager_id) -> None:
        """
        Notify orchestrator to mark the manager as drained for internal cross ddict synchronization testing.
        """
        msg = dmsg.DDMarkDrainedManager(
            self._tag_inc(), respFLI=self._serialized_buffered_return_connector, managerID=manager_id
        )
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
        if resp_msg.err != DragonError.SUCCESS:
            raise RuntimeError(f"Failed to mark manager {manager_id} as drained! {resp_msg.errInfo}")

    def pput(self, key: object, value: object) -> None:
        """
        Persistently store a key/value pair within the Distributed Dictionary. This is
        useful when checkpointing is employed in the dictionary. A persistent put of a
        key/value pair means that the key/value pair persists across checkpoints. Persistent
        key/value pairs are useful when putting constant values or other values that don't
        change across checkpoints.

        :param key: A serializable object that will be stored as the key in the DDict.
        :param value: A serializable object that will be stored as the value.
        """
        msg = dmsg.DDPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=True)
        self._put(msg, key, value)

    def keys(self) -> list[object]:
        """
        Return a list of the keys of the distributed dictionary. This is potentially a big
        list and should be used cautiously.

        :returns: A list of all the keys of the distributed dictionary for the current checkpoint.
        """

        keys = []
        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            managers = [self._chosen_manager]
        else:
            self._check_manager_connection(all=True)
            managers = range(self._num_managers)

        for manager_id in managers:
            msg = dmsg.DDKeys(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
            self._send([(msg, None)], self._managers[manager_id])
            self._traceit("About to open recv handle to retrieve keys from %s", manager_id)
            with self._return_connector.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                self._traceit("Got keys from %s", manager_id)
                resp_msg = dmsg.parse(resp_ser_msg)
                if resp_msg.ref == msg.tag:
                    if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                        raise DDictError(resp_msg.err, resp_msg.errInfo)
                    elif resp_msg.err != DragonError.SUCCESS:
                        raise DDictError(resp_msg.err, "Failed to get key list in the distributed dictionary.")
                    done = False
                    while not done:
                        try:
                            key_bytes, _ = recvh.recv_bytes(timeout=self._timeout)
                            if self._key_pickler is None:
                                key = cloudpickle.loads(key_bytes)
                            else:
                                key = self._key_pickler.loads(key_bytes)
                            keys.append(key)
                        except EOFError:
                            done = True

                # If the tag did not match, exiting the context manager will flush the rest of the response.
        if self._chosen_manager is not None:
            self._traceit("Keys from chosen manager are %s", keys)
        else:
            self._traceit("Keys from all managers are %s", keys)

        return keys

    def pop(self, key: object, default: object = None) -> object:
        """
        Pop the given key from the distributed dictionary and return the associated
        value. If the given key is not found in the dictionary, then KeyError is raised
        unless a default value is provided, in which case the default value is returned
        if the key is not found in the dictionary.

        :param key: A key to be popped from the distributed dictionary.

        :param default: A default value to be returned if the key is not in the
            distributed dictionary.

        :returns: The associated value if key is popped and the default value otherwise.
        """
        msg = dmsg.DDPop(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None), (pickled_key, KEY_HINT)], self._managers[manager_id])
        try:
            return self._recv_dmsg_and_val(msg, key)
        except KeyError as ex:
            if default is None:
                raise ex

            return default

    def clear(self) -> None:
        """
        Empty the distributed dictionary of all keys and values.
        """
        self._traceit("clearing dictionary for checkpoint %s", self._chkpt_id)
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDClear(
            tag, self._client_id, self._chkpt_id, self._serialized_buffered_return_connector, broadcast=broadcast
        )
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, "There was an error while clearing the distributed dictionary.")

    def get_name(self):
        return self._name

    def values(self) -> list[object]:
        """
        When called this returns a list of all values in the Distributed Dictionary.

        :returns list[object]: A list of all keys in the DDict.
        :raises NotImplementedError: Not implemented.

        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def items(self) -> list[tuple[object, object]]:
        """
        Returns a list of all key/value pairs in the Distributed Dictionary.

        :returns list[tuple[object,object]]: A list of all key/value pairs.
        :raises NotImplementedError: Not implemented.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def update(self, dict2: DDict) -> None:
        """
        Adds all key/value pairs from dict2 into this Distributed Dictionary.

        :param dict2: Another distributed dictionary.
        :raises NotImplementedError: Not implemented.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def popitem(self) -> tuple[object, object]:
        """
        Returns a random key/value pair from the Distributed Dictionary.

        :returns tuple[object,object]: A random key/value pair.
        :raises NotImplementedError: Not implemented.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def copy(self) -> DDict:
        """
        Returns a copy of the Distributed Dictionary.

        :returns DDict: A second DDict that is a copy of the first assuming that no
            other processes were concurrently using this DDict.

        :raises NotImplementedError: Not implemented.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    @property
    def stats(self) -> list[DDictManagerStats]:
        """
        Returns a list of manager stats, one for each manager of the distributed dictionary. See
        the DDictManagerStats structure for a description of its contents.
        """
        self._traceit("Getting stats from ddict")
        return list(self.dstats.values())

    @property
    def dstats(self) -> dict[int, DDictManagerStats]:
        """
        Returns a dict of manager stats, one for each manager of the distributed dictionary. See
        the DDictManagerStats structure for a description of its contents.
        """
        self._traceit("Getting stats from ddict")
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDManagerStats(tag, respFLI=self._serialized_buffered_return_connector, broadcast=broadcast)
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        data = {}
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(
                    resp_msg.err, "There was an error while getting stats from the distributed dictionary."
                )
            item = cloudpickle.loads(b64decode(resp_msg.data))
            data[item.manager_id] = item

        return data

    def checkpoint(self) -> None:
        """
        Calling checkpoint advances the checkpoint for the distributed dictionary. In
        subsequent calls to the distributed dictionary, like gets or puts, if the chosen
        manager does not have the current checkpoint in its working set, the get/put
        operations will block until the checkpoint becomes available. But, calling this
        operation itself does not block.
        """
        self._chkpt_id += 1

    def rollback(self) -> None:
        """
        Calling rollback decrements the checkpoint id to its previous value. Again this
        call does not block. If rollback causes the checkpoint id to roll back to a
        checkpoint that a chosen manager no longer has in its working set, then subsequent
        operations may fail with a exception indicating the Checkpoint is no longer available,
        raising a DDictCheckpointSync exception.

        """
        self._chkpt_id -= 1
        self._chkpt_id = max(0, self._chkpt_id)

    def sync_to_newest_checkpoint(self) -> None:
        """
        Advance the checkpoint identifier of this client to the newest checkpoint across all managers.
        This does not guarantee that all managers have advanced to the same checkpoint. That is up to the
        application which may guarantee all managers are at the same checkpoint by setting and getting
        values from managers in checkpoints and checkpoints advance. See the ddict_checkpoint_pi.py demo
        in examples/dragon_data/ddict for an example of an application that uses this method.
        """
        self._traceit("Syncing to newest checkpoint")
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        chkpt_id = 0
        self._check_manager_connection(selected_manager)
        msg = dmsg.DDManagerNewestChkptID(tag, respFLI=self._serialized_buffered_return_connector, broadcast=broadcast)
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(
                    resp_msg.err, "There was an error while getting stats from the distributed dictionary."
                )
            chkpt_id = max(chkpt_id, resp_msg.chkptID)

        self._chkpt_id = chkpt_id

    def filter(self, mgr_code: FunctionType, mgr_code_args: tuple, comparator: FunctionType, branching_factor: int = 5):
        """
        Calling this instantiates a tree of process groups where mgr_code is
        expected to be a function that is invoked as mgr_code(args) where
        args are (dd, out_queue)+mgr_code_args. For instance, if
        mgr_code_args are (x,) then mgr_code(dd, outqueue, x) is how
        mgr_code is invoked.

        The dd of the mgr_code arguments is this distributed dictionary directed
        toward one manager in the collection of dd managers. In other
        words, dd is as if the manager method had been invoked on this
        distributed dictionary so mgr_code only interacts with the
        manager it was provided. In addition, mgr_code is executed on the
        same node where the manager it is directed toward is running.
        This means that mgr code will get the best possible performance
        while filtering data that is associated with its manager. The
        mgr_code can do whatever computation is desired, but its chosen
        output is put into the outqueue.

        All data written to outqueue is aggregated with data coming from each
        manager in a tree-like fashion so as to be scalable to tens of
        thousands of nodes. All data put in the outqueue by mgr_code is
        assumed to be ordered from best to worst. When data is aggregated
        for sending up the tree, it is aggregated according to some kind
        of ordering which is determined by the comparator function. The
        comparator will be called as comparator(x,y) and should return
        True if x is better than y and False otherwise. If there is no
        ordering, or the ordering is not relevant to the filtering, then
        comparator(x,y) may return a constant value of False or True and
        there will be no ordering of the data.

        The branching_factor of the filtering tree has a default value, but may
        be provided by the user to create a tree of whatever width is
        desired. Note that branching_factor is the max branching factor.
        Depending on the number of managers, some nodes in the tree
        may/will have smaller numbers of children.

        The filter function returns a Context Manager that supplies an Iterator
        over which you can iterate on the filtered values.

        Example
        ---------

        ```
        candidate_list=[]
        with dd.filter(get_largest, (num_needed,), comparator) as candidates:
            for candidate in candidates:
                candidate_list.append(candidate)
                if len(candidate_list) == num_needed:
                    break
        ```

        Assuming your distributed dictionary is called dd, this will get
        num_needed elements from the result of filtering the distributed
        dictionary by calling the function get_largest on each
        distributed dictionary manager.

        :param mgr_code: A function taking arguments as described above that will
        run on the same node as a distributed dictionary manager and will
        be directed toward that manager.

        :param comparator: A function taking two arguments that should return
        True if the first argument of the values being filtered is
        "better" than the second and False otherwise. Note that returning
        a constant value of True or False will result in the filtering
        imposing no order.

        :param branching_factor: The maximum branching factor of any interior
        node in the filtering tree (i.e. any aggregator).

        :returns: A Context Manager that supplies an iterator over which you can
        iterate over the filtered values. See the example above.
        """
        stats = self.dstats
        nodes = query_all()
        if len(nodes) == 1:
            managers_hosts = [(manager_id, nodes[0].name) for manager_id in stats]
        else:
            managers_hosts = [(manager_id, stats[manager_id].hostname) for manager_id in stats]

        # Order by hostname so aggregators are close to other processes they will control.
        managers_hosts.sort(key=lambda tup: tup[1])
        filter_queue = SentinelQueue()
        pickled_mgr_code = cloudpickle.dumps(mgr_code)
        pickled_mgr_args = cloudpickle.dumps(mgr_code_args)
        pickled_comparator = cloudpickle.dumps(comparator)

        filter_proc = mp.Process(
            target=filter_aggregator,
            args=(
                self,
                managers_hosts,
                branching_factor,
                pickled_mgr_code,
                pickled_mgr_args,
                pickled_comparator,
                filter_queue,
            ),
        )
        filter_proc.start()

        cm = FilterContextManager(filter_proc, filter_queue)

        return cm

    @property
    def current_checkpoint_id(self) -> int:
        """
        Returns the current checkpoint id of the client.
        """
        return self._chkpt_id

    @property
    def local_managers(self) -> list[int]:
        """
        Returns all local manager ids of all managers that are local to this node.

        :raises NotImplementedError: Not implemented yet.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    @property
    def local_manager(self) -> int:
        """
        Returns a local manager id if one exists. This is manager designated as the main manager for the client. If
        no local manager exists, the None is returned.
        """
        return self._local_manager

    @property
    def main_manager(self) -> int:
        """
        Returns the main manager id. This will always exist and will be the same as the local manager
        id if a local manager exists. Otherwise, it will be the id of a random manager from another node.
        """
        return self._main_manager

    @property
    def manager_nodes(self) -> list[str]:
        """
        For each manager, the serialized, base64 encoded FLI of the manager is returned.
        """
        return self._manager_nodes

    @property
    def empty_managers(self):
        """
        Return a list of manager IDs that restarted on new nodes.
        """
        msg = dmsg.DDEmptyManagers(self._tag_inc(), respFLI=self._serialized_buffered_return_connector)
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)

        if resp_msg.err != DragonError.SUCCESS:
            self._cleanup()
            raise RuntimeError(f"Failed to create dictionary! {resp_msg.errInfo}")

        return resp_msg.managers
