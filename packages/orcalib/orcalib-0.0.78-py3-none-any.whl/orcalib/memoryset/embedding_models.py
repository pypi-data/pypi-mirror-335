from __future__ import annotations

import hashlib
import os
import random
from dataclasses import dataclass
from enum import Enum
from tempfile import TemporaryDirectory
from typing import Literal, NamedTuple, cast, overload

from cachetools import TTLCache
from datasets import Dataset
from sentence_transformers import SentenceTransformer
from torch import Tensor
from tqdm.auto import trange
from transformers import AutoConfig, CLIPConfig, PretrainedConfig

from ..dataset_utils import parse_dataset
from ..fs_utils import download_dir, download_file, is_using_blob_storage, upload_dir
from ..progress_utils import OnLogCallback, OnProgressCallback
from ..pydantic_utils import Vector
from ..torch_layers import SentenceEmbeddingGenerator
from .embedding_finetuning import (
    EmbeddingTrainingArguments,
    finetune_for_classification,
    finetune_with_batch_triplet_loss,
)
from .memory_types import InputType, InputTypeList

CACHE_TTL = 2 * 60 * 60  # 2h
CACHE_SIZE = 25000


class EmbeddingModelContext(NamedTuple):
    embeddings: Tensor
    hash: str


class EmbeddingFinetuningMethod(str, Enum):
    CLASSIFICATION = "classification"
    """Fine tune embeddings by adding a logistic regression head and training to predict labels"""

    BATCH_TRIPLET_LOSS = "batch_triplet_loss"
    """Fine tune embeddings via contrastive triplet loss based on batches"""

    # TODO: bring these back once they properly support on progress and can be safely exposed via Lighthouse
    # TRIPLETS = "triplets"
    # """Fine tune embeddings via contrastive triplet loss based on labels"""

    # CLASS_PROXY_LOSS = "class_proxy_loss"
    # """Fine tune embeddings via proxy-based loss, where each class is represented by a learnable proxy."""


@dataclass(frozen=True)
class _Config:
    """Schema for relevant parsed values from the AutoConfig of an embedding model"""

    name: str
    """Either the name of a HuggingFace model or a path to a local saved model"""

    max_seq_length: int
    """The maximum sequence length to use with this model"""

    embedding_dim: int
    """The dimension of the embeddings generated by this model"""

    query_prompt: str | None
    """Optional prompt prefix to use for queries with this model"""

    document_prompt: str | None
    """Optional prompt prefix to use for documents with this model"""

    transductive_context_length: int | None
    """The number of documents to use for the transductive context of contextual embedding models"""

    auto_model_compatible: bool
    """Whether this model can be used with AutoModel in addition to SentenceTransformer"""


class PretrainedEmbeddingModelName(Enum):
    # This enum is exposed over the lighthouse API, only add models that should appear on the API
    """Names of pretrained embedding models that are supported by OrcaCloud"""

    CLIP_BASE = "CLIP_BASE"
    """[CLIP-L14](https://huggingface.co/sentence-transformers/clip-ViT-L-14) embedding model"""

    GTE_BASE = "GTE_BASE"
    """[Alibaba GTE-Base v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) embedding model"""

    CDE_SMALL = "CDE_SMALL"
    """[CDE-Small](https://huggingface.co/jxm/cde-small-v1) embedding model"""

    DISTILBERT = "DISTILBERT"
    """[DistilBERT](https://huggingface.co/distilbert-base-uncased) embedding model for testing"""

    GTE_SMALL = "GTE_SMALL"
    """[GTE-Small](https://huggingface.co/Supabase/gte-small) embedding model"""

    @property
    def path(self) -> str:
        # this enum is exposed on the api and thus the names and values have to match, so we add
        # an additional path property that contains the huggingface name
        return {
            "CLIP_BASE": "OrcaDB/clip-ViT-L-14",
            "GTE_BASE": "OrcaDB/gte-base-en-v1.5",
            "CDE_SMALL": "OrcaDB/cde-small-v1",
            "DISTILBERT": "distilbert-base-uncased",
            "GTE_SMALL": "Supabase/gte-small",
        }[self.value]


class EmbeddingModelMeta(type):
    def __getattr__(cls, name: str) -> EmbeddingModel:
        if name in PretrainedEmbeddingModelName.__members__:
            return EmbeddingModel(PretrainedEmbeddingModelName[name].path)
        else:
            raise AttributeError(f"'{cls.__name__}' object has no attribute '{name}'")


class EmbeddingModel(metaclass=EmbeddingModelMeta):
    """Embedding models for use with memorysets"""

    @staticmethod
    def load_config(name: str, trust_remote_code: bool = False) -> _Config:
        if is_using_blob_storage(name):
            with TemporaryDirectory() as temp_dir:
                download_file(f"{name}/config.json", os.path.join(temp_dir, "config.json"))
                config: PretrainedConfig = AutoConfig.from_pretrained(temp_dir, trust_remote_code=True)
        else:
            config: PretrainedConfig = AutoConfig.from_pretrained(
                name, trust_remote_code=trust_remote_code or name.startswith("OrcaDB/") or os.path.isdir(name)
            )
        embedding_dim = getattr(config, "embedding_size", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "projection_dim", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "hidden_size", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "d_model", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "n_embed", None)
        if embedding_dim is None:
            raise ValueError(f"Could not determine embedding dimension from {config}")
        max_seq_length = getattr(config, "max_seq_length", None)
        if max_seq_length is None and hasattr(config, "text_config"):
            max_seq_length = getattr(config.text_config, "max_position_embeddings", None)
        if max_seq_length is None:
            max_seq_length = getattr(config, "max_position_embeddings", None)
        if max_seq_length is None:
            max_seq_length = getattr(config, "n_positions", None)
        if max_seq_length is None:
            raise ValueError(f"Could not determine max sequence length from {config}")
        query_prompt = getattr(config, "query_prompt", None)
        document_prompt = getattr(config, "document_prompt", None)
        transductive_context_length = getattr(config, "transductive_corpus_size", None)
        return _Config(
            name=name,
            embedding_dim=embedding_dim,
            max_seq_length=max_seq_length,
            query_prompt=query_prompt,
            document_prompt=document_prompt,
            transductive_context_length=transductive_context_length,
            auto_model_compatible=(
                not isinstance(config, CLIPConfig) and (type(config).__name__ != "ContextualModelConfig")
            ),
        )

    @staticmethod
    def _get_cache_key(
        value: InputType, value_kind: Literal["query", "document"], context: EmbeddingModelContext | None
    ) -> str:
        input_hash = hashlib.md5(value.encode("utf-8") if isinstance(value, str) else value.tobytes()).hexdigest()
        if value_kind != "query":
            input_hash += f"_{value_kind}"
        if context is not None:
            input_hash += f"_{context.hash}"
        return input_hash

    _instances: dict[tuple, EmbeddingModel] = {}

    def __new__(
        cls,
        model_name_or_path: str,
        *,
        trust_remote_code: bool = False,
        max_seq_length_override: int | None = None,
        cache_size: int = CACHE_SIZE,
        cache_ttl: int = CACHE_TTL,
    ):
        cache_key = (model_name_or_path, max_seq_length_override, cache_size, cache_ttl)
        if cache_key not in cls._instances:
            cls._instances[cache_key] = super().__new__(cls)
        return cls._instances[cache_key]

    def __init__(
        self,
        model_name_or_path: str,
        *,
        trust_remote_code: bool = False,
        max_seq_length_override: int | None = None,
        cache_size: int = CACHE_SIZE,
        cache_ttl: int = CACHE_TTL,
    ):
        """
        Initialize an embedding model

        Warning:
            Only the models that are available as class properties like `EmbeddingModel.CLIP_BASE` as
            well as fine-tuned versions of them are guaranteed to work.

        Note:
            This class will return singleton instances of the embedding model, i.e. calling it with
            the same arguments will return the same instance.

        Args:
            model_name_or_path: the name of the embedding model or a path to a local saved model
            trust_remote_code: whether to trust remote model implementation code from sources other
                than HuggingFace and Orca's organizations, don't st this to true in a server environment
            max_seq_length_override: optional override for the maximum sequence length (in tokens) that the
                model will accept, if `None` the model's default max sequence length will be used
            cache_size: the size of the cache to use for the embedding model
            cache_ttl: the time to live for the cache
        """
        self.name = model_name_or_path
        self.from_path = is_using_blob_storage(self.name) or os.path.isdir(self.name)
        self.trust_remote_code = trust_remote_code or self.name.startswith("OrcaDB/") or self.from_path
        self.config = self.load_config(model_name_or_path, trust_remote_code=self.trust_remote_code)

        if max_seq_length_override is not None and max_seq_length_override > self.config.max_seq_length:
            raise ValueError(
                f"Max sequence length {max_seq_length_override} is greater than what the model supports: {self.config.max_seq_length}"
            )

        self.max_seq_length_override = max_seq_length_override
        self._cache: TTLCache[str, Vector] = TTLCache(maxsize=cache_size, ttl=cache_ttl)

    @property
    def uses_context(self) -> bool:
        return self.config.transductive_context_length is not None

    @property
    def embedding_dim(self) -> int:
        return self.config.embedding_dim

    @property
    def max_seq_length(self) -> int:
        return self.max_seq_length_override or self.config.max_seq_length

    def __repr__(self) -> str:
        return f"EmbeddingModel({self.name}, embedding_dim={self.embedding_dim}, max_seq_length={self.max_seq_length})"

    __embedder: SentenceTransformer | SentenceEmbeddingGenerator | None = None

    @property
    def _embedder(self) -> SentenceTransformer | SentenceEmbeddingGenerator:
        if self.__embedder is None:
            if is_using_blob_storage(self.name):
                temp_dir = TemporaryDirectory()
                download_dir(self.name, temp_dir.name, recursive=False)
                location = temp_dir.name
            else:
                temp_dir = None
                location = self.name
            if self.config.auto_model_compatible:
                self.__embedder = SentenceEmbeddingGenerator(
                    base_model=location,
                    frozen=True,
                    normalize=True,
                    trust_remote_code=self.trust_remote_code,
                    max_sequence_length=self.max_seq_length_override,
                )
            else:
                embedder = SentenceTransformer(location, trust_remote_code=self.trust_remote_code)
                if self.max_seq_length_override is not None:
                    embedder.max_seq_length = self.max_seq_length_override
                self.__embedder = embedder
            if temp_dir is not None:
                temp_dir.cleanup()
        return self.__embedder

    def compute_context(self, values: InputTypeList) -> EmbeddingModelContext:
        """
        Compute the context for contextual embedding models like [CDE](https://huggingface.co/jxm/cde-small-v1)

        Args:
            values: the values of the corpus to construct the context from

        Returns:
            The context tensor for the embedding model that can be passed to the `embed` method
        """
        if self.config.transductive_context_length is None or not isinstance(self._embedder, SentenceTransformer):
            raise ValueError("Context can only be computed for contextual embedding models")
        if len(values) > self.config.transductive_context_length:
            sampled_values = random.sample(values, k=self.config.transductive_context_length)
        else:
            sampled_values = random.choices(values, k=self.config.transductive_context_length)
        embeddings = self._embedder.encode(
            sampled_values,  # type: ignore -- types are wrong, image is accepted here
            prompt=self.config.document_prompt,
            convert_to_tensor=True,
            show_progress_bar=False,
        )
        return EmbeddingModelContext(
            embeddings=embeddings, hash=hashlib.md5(embeddings.cpu().numpy().tobytes(order="C")).hexdigest()
        )

    def _embed(
        self,
        values: InputTypeList,
        show_progress_bar: bool,
        batch_size: int,
        value_kind: Literal["query", "document"],
        context_embeddings: Tensor | None,
    ) -> list[Vector]:
        try:
            if isinstance(self._embedder, SentenceTransformer):
                return list(
                    self._embedder.encode(
                        values,  # type: ignore -- types are wrong, image is accepted here
                        show_progress_bar=show_progress_bar,
                        normalize_embeddings=True,
                        batch_size=batch_size,
                        prompt=self.config.document_prompt if value_kind == "document" else self.config.query_prompt,
                        dataset_embeddings=context_embeddings,
                    )
                )
            else:
                if not isinstance(values[0], str):
                    raise ValueError(f"{self.name} embedding model only supports strings")
                if len(values) <= batch_size:
                    return [v for v in self._embedder.encode(cast(list[str], values)).cpu().numpy()]
                else:
                    batch_results: list[Vector] = []
                    for i in trange(
                        0,
                        len(values),
                        batch_size,
                        disable=not show_progress_bar,
                    ):
                        batch = cast(list[str], values[i : i + batch_size])
                        batch_results.extend(self._embedder.encode(batch).cpu().numpy())
                    return batch_results
        except Exception as e:
            if "The size of tensor" in str(e):
                raise RuntimeError(
                    f"{e}\n"
                    + f"    The data you are trying to embed is too large for the embedding model ({self.name}) you are using.\n"
                    + "    Please choose a different embedding model for your data.\n"
                    + (
                        "    Note: 'OrcaDB/clip-ViT-L-14' is only recommended for embedding data that contains images.\n"
                        if self.name == "OrcaDB/clip-ViT-L-14"
                        else ""
                    )
                )
            raise e

    @overload
    def embed(
        self,
        values: InputTypeList,
        show_progress_bar: bool = False,
        batch_size: int = 32,
        value_kind: Literal["query", "document"] = "query",
        context: EmbeddingModelContext | None = None,
        use_cache: bool = True,
    ) -> list[Vector]:
        pass

    @overload
    def embed(
        self,
        values: InputType,
        show_progress_bar: bool = False,
        batch_size: int = 32,
        value_kind: Literal["query", "document"] = "query",
        context: EmbeddingModelContext | None = None,
        use_cache: bool = True,
    ) -> Vector:
        pass

    def embed(
        self,
        values: InputTypeList | InputType,
        show_progress_bar: bool = False,
        batch_size: int = 32,
        value_kind: Literal["query", "document"] = "query",
        context: EmbeddingModelContext | None = None,
        use_cache: bool = True,
    ) -> list[Vector] | Vector:
        """
        Generate embeddings for the given input

        Args:
            values: the value or values to encode
            show_progress_bar: whether to show a progress bar
            batch_size: size of the batches to use
            value_kind: kind of values to embed, either "query" or "document" to determine potential
                prompts for the model, this is usually just used by memoryset internally
            context: optional context tensor to use for contextual embedding models

        Returns:
            list of embeddings or a single embedding if a scalar was given
        """

        scalar_value = not isinstance(values, list)
        if scalar_value:
            values = [values]
        elif len(values) == 0:
            return []

        # if caching is disabled, we can just embed the values directly
        if not use_cache:
            return self._embed(
                values,
                show_progress_bar=show_progress_bar,
                batch_size=batch_size,
                value_kind=value_kind,
                context_embeddings=context.embeddings if context is not None else None,
            )

        # otherwise we first resolve cache hits and collect the new values to embed
        new_values: InputTypeList = []
        new_values_indices: list[int] = []
        new_values_cache_keys: list[str] = []
        all_results: list[tuple[int, Vector]] = []  # (index, result)
        for i, value in enumerate(values):
            cache_key = self._get_cache_key(value, value_kind, context)
            result = self._cache.get(cache_key, None)
            if result is not None:
                all_results.append((i, result))
            else:
                new_values.append(value)
                new_values_indices.append(i)
                new_values_cache_keys.append(cache_key)

        # if everything was cached, we can just return the cached results
        if len(new_values) == 0:
            res = [r for _, r in all_results]
            return res[0] if scalar_value else res
        # otherwise, generate embeddings for the new values
        new_results = self._embed(
            new_values,
            show_progress_bar=show_progress_bar,
            batch_size=batch_size,
            value_kind=value_kind,
            context_embeddings=context.embeddings if context is not None else None,
        )

        # if nothing was cached, we can just store the new results and return them
        if len(all_results) == 0:
            for cache_key, result in zip(new_values_cache_keys, new_results):
                self._cache[cache_key] = result
            return new_results[0] if scalar_value else new_results

        # otherwise, store the new results and return the combined results in given order
        for i, cache_key, result in zip(new_values_indices, new_values_cache_keys, new_results):
            self._cache[cache_key] = result
            all_results.append((i, result))
        all_results.sort(key=lambda x: x[0])
        res = [r for _, r in all_results]
        return res[0] if scalar_value else res

    def finetune(
        self,
        save_dir: str,
        train_dataset: Dataset,
        value_column: str = "value",
        label_column: str = "label",
        eval_dataset: Dataset | None = None,
        training_args: EmbeddingTrainingArguments | None = None,  # filled with method specific defaults
        method: EmbeddingFinetuningMethod | str = EmbeddingFinetuningMethod.CLASSIFICATION,
        on_progress: OnProgressCallback | None = None,
        on_log: OnLogCallback | None = None,
    ) -> EmbeddingModel:
        """
        Finetune the embedding model on a given dataset

        Args:
            save_dir: The directory to save the finetuned model to.
            train_dataset: The data to finetune on.
            eval_dataset: The data to evaluate the finetuned model on, if this is `None` a 10%
                holdout from the training data will be used.
            training_args: The training arguments to use for the finetuning. If this is `None`
                sensible defaults will be used based on the finetuning method.
            method: The method to use for finetuning, "triplets" uses a contrastive triplet loss to
                pull embeddings together and push them apart based on labels. "classification" adds
                a logistic regression head to the model and fine-tunes it for classification.
            on_progress: Callback to report progress
            on_log: Callback to report logs

        Returns:
            New finetuned embedding model
        """
        if is_using_blob_storage(self.name):
            raise ValueError("Finetuning models is currently not supported for models in blob storage")

        train_dataset = parse_dataset(train_dataset, value_column=value_column, label_column=label_column)
        if eval_dataset is not None:
            eval_dataset = parse_dataset(eval_dataset, value_column=value_column, label_column=label_column)
        else:
            split_dataset = train_dataset.train_test_split(test_size=0.1)
            train_dataset = split_dataset["train"]
            eval_dataset = split_dataset["test"]

        match method:
            case EmbeddingFinetuningMethod.CLASSIFICATION:
                training_args = training_args or EmbeddingTrainingArguments.for_classification()
                finetune_fn = finetune_for_classification
            case EmbeddingFinetuningMethod.BATCH_TRIPLET_LOSS:
                training_args = training_args or EmbeddingTrainingArguments.for_triplet_loss()
                finetune_fn = finetune_with_batch_triplet_loss
            # TODO: bring these back once they properly support on progress and can be safely exposed via Lighthouse
            # case EmbeddingFinetuningMethod.TRIPLETS:
            #     training_args = training_args or EmbeddingTrainingArguments.for_triplet_loss()
            #     finetune_fn = finetune_with_batch_triplets
            # case EmbeddingFinetuningMethod.PROXY:
            #     training_args = training_args or EmbeddingTrainingArguments.for_proxy_loss()
            #     finetune_fn = finetune_with_proxy_loss
            case _:
                raise ValueError(f"Invalid finetuning method: {method}")

        if training_args.max_seq_length is None and self.max_seq_length_override is not None:
            # if the model has a custom max sequence length, use it for finetuning
            training_args.max_seq_length = self.max_seq_length_override

        if is_using_blob_storage(save_dir):
            temp_dir = TemporaryDirectory()
            # Model configs that contain custom classes get messed up if we don't provide a stable directory name
            location = temp_dir.name
        else:
            temp_dir = None
            location = save_dir

        try:
            finetune_fn(
                base_model_name=self.name,
                output_dir=location,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                training_args=training_args,
                on_progress=on_progress,
                on_log=on_log,
            )
        finally:
            # upload finetuned model to remote storage if necessary
            if temp_dir is not None:
                try:
                    upload_dir(location, save_dir, recursive=True)
                finally:
                    temp_dir.cleanup()

        # return a new embedding model by loading it from the saved directory
        return EmbeddingModel(
            save_dir,
            trust_remote_code=self.trust_remote_code,
            max_seq_length_override=self.max_seq_length_override,
        )
