{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# from-ipfs Demo: Load ML Models from IPFS\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexbakers/from_ipfs/blob/main/examples/from_ipfs_colab_demo.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use `from-ipfs` to load models directly from IPFS into:\n",
    "1. ðŸ¤— Hugging Face Transformers (Sentiment Analysis model)\n",
    "2. ðŸ¦™ llama.cpp (GPT-2 GGUF model)\n",
    "\n",
    "No setup required - just run each cell in order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "!pip install from-ipfs[all] torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transformers_example"
   },
   "source": [
    "## 2. Sentiment Analysis with Transformers\n",
    "\n",
    "Let's use a GPT-2 model fine-tuned for tweet sentiment analysis. The model is loaded directly from IPFS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transformers_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load model from IPFS\n",
    "model_uri = \"ipfs://bafybeichqdarufyutqc7yd43k77fkxbmeuhhetbihd3g32ghcqvijp6fxi\"\n",
    "\n",
    "print(\"Loading model from IPFS...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_uri)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_uri)\n",
    "print(\"Model loaded successfully!\\n\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Prepare input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    # Get model prediction\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert to sentiment\n",
    "    sentiment = \"positive\" if probabilities[0][1] > 0.5 else \"negative\"\n",
    "    confidence = float(probabilities[0][1] if sentiment == \"positive\" else probabilities[0][0])\n",
    "\n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test with example tweets\n",
    "example_tweets = [\n",
    "    \"I absolutely love this new update! The features are amazing! ðŸ˜Š\",\n",
    "    \"This is the worst service I've ever experienced. Very disappointed. ðŸ˜ \",\n",
    "    \"The weather is nice today, perfect for a walk in the park!\",\n",
    "    \"This product is not worth the money, don't buy it ðŸ‘Ž\"\n",
    "]\n",
    "\n",
    "for tweet in example_tweets:\n",
    "    sentiment, confidence = analyze_sentiment(tweet)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Sentiment: {sentiment} (confidence: {confidence:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llama_example"
   },
   "source": [
    "## 3. Text Generation with llama.cpp\n",
    "\n",
    "Now let's try text generation using a quantized GPT-2 model in GGUF format, loaded directly from IPFS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llama_code"
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load model from IPFS\n",
    "model_uri = \"ipfs://bafybeibawdd2fzph73hh3z2ip2js4wv3ct3kcc2lkyzet4bc75k5bg5xby/openai-community/gpt2.Q5_K_M.gguf\"\n",
    "\n",
    "print(\"Loading model from IPFS...\")\n",
    "llm = Llama(model_path=model_uri, n_ctx=512)\n",
    "print(\"Model loaded successfully!\\n\")\n",
    "\n",
    "# Test with different prompts\n",
    "prompts = [\n",
    "    \"Once upon a time in a magical forest,\",\n",
    "    \"The recipe for a perfect chocolate cake includes:\",\n",
    "    \"The future of artificial intelligence will\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    output = llm(prompt,\n",
    "                 max_tokens=64,\n",
    "                 temperature=0.7,\n",
    "                 top_p=0.95,\n",
    "                 repeat_penalty=1.1,\n",
    "                 stop=[\"\\n\\n\"])\n",
    "\n",
    "    generated_text = output['choices'][0]['text']\n",
    "    print(f\"Generated: {prompt}{generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cache_info"
   },
   "source": [
    "## 4. Cache Information\n",
    "\n",
    "Let's see what models are cached locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cache_code"
   },
   "outputs": [],
   "source": [
    "from from_ipfs.utils import list_cached_models, show_config\n",
    "\n",
    "# Show configuration\n",
    "print(\"Current configuration:\")\n",
    "show_config()\n",
    "print(\"\\nCached models:\")\n",
    "for model in list_cached_models():\n",
    "    print(f\"- {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## That's it!\n",
    "\n",
    "You've successfully:\n",
    "1. âœ… Loaded a sentiment analysis model from IPFS\n",
    "2. âœ… Analyzed tweet sentiments\n",
    "3. âœ… Loaded a GGUF model from IPFS\n",
    "4. âœ… Generated text with GPT-2\n",
    "\n",
    "Want to learn more? Check out:\n",
    "- ðŸ“š [from-ipfs Documentation](https://github.com/alexbakers/from_ipfs#readme)\n",
    "- ðŸ¤— [Original Sentiment Model](https://huggingface.co/riturajpandey739/gpt2-sentiment-analysis-tweets)\n",
    "- ðŸ¦™ [Original GGUF Model](https://huggingface.co/PrunaAI/openai-community-gpt2-GGUF-smashed)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "from_ipfs_colab_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
