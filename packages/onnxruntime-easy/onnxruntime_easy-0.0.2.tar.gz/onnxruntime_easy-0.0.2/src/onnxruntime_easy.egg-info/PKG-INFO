Metadata-Version: 2.2
Name: onnxruntime-easy
Version: 0.0.2
Summary: Simplified APIs for onnxruntime
Author-email: Justin Chu <justinchuby@users.noreply.github.com>
Project-URL: Documentation, https://github.com/justinchuby/onnxruntime-easy#readme
Project-URL: Issues, https://github.com/justinchuby/onnxruntime-easy/issues
Project-URL: Source, https://github.com/justinchuby/onnxruntime-easy
Classifier: Development Status :: 3 - Alpha
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: Operating System :: POSIX
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Operating System :: Microsoft :: Windows
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: onnxruntime>=1.18.0

# onnxruntime-easy

Simplified APIs for onnxruntime

## Usage

```py
import onnxruntime_easy as ort_easy
import numpy as np
import ml_dtypes

# Simple `load` method that handles setting up ONNX Runtime inference session
model = ort_easy.load("model.onnx", device="cpu")
# Supports all ONNX dtypes via ml_dtypes or dlpack
input = np.random.rand(1, 3, 299, 299).astype(ml_dtypes.bfloat16)
output = model(input)

# Works with torch tensors and any ndarray that implements the __array__ interface
import torch
input_tensor = torch.rand(1, 3, 299, 299)
output = model(input)

with model.set_outputs("output1"):
    output1 = model(input)
```
