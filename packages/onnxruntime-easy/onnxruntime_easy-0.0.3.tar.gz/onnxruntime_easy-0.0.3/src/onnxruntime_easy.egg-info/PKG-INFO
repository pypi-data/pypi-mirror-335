Metadata-Version: 2.4
Name: onnxruntime-easy
Version: 0.0.3
Summary: Simplified APIs for onnxruntime
Author-email: Justin Chu <justinchuby@users.noreply.github.com>
Project-URL: Documentation, https://github.com/justinchuby/onnxruntime-easy#readme
Project-URL: Issues, https://github.com/justinchuby/onnxruntime-easy/issues
Project-URL: Source, https://github.com/justinchuby/onnxruntime-easy
Classifier: Development Status :: 3 - Alpha
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: Operating System :: POSIX
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Operating System :: Microsoft :: Windows
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: onnxruntime>=1.18.0
Dynamic: license-file

# onnxruntime-easy

Simplified APIs for onnxruntime

## Usage

```py
import onnxruntime_easy as ort_easy
import numpy as np
import ml_dtypes

# Simple `load` method that handles setting up ONNX Runtime inference session
# All session options discoverable in the load function.
model = ort_easy.load("model.onnx", device="cpu")  # You can control the providers if the default is not what you need
# Supports all ONNX dtypes via ml_dtypes or dlpack
input = ort_easy.ort_value(np.random.rand(1, 3, 299, 299).astype(ml_dtypes.bfloat16))
output = model(input)

# Works with any ndarray that implements the __array__ interface
# Or automatically share data on device (like cuda) with dlpack
import torch
model = ort_easy.load("model.onnx", device="cuda")
input_tensor = ort_easy.ort_value(torch.rand(1, 3, 299, 299, device="cuda"))
output = model(input_tensor)

# Use a context manager to control the outputs you get
with model.set_outputs("output1"):
    output1 = model(input_tensor)
```
