"""
LLM Explainer module for the LingualLens framework.

This module provides tools for explaining black box language models by analyzing
their outputs, internal representations, and sensitivities to input changes.
"""

import numpy as np
from typing import Dict, List, Tuple, Union, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
from ..core.model_loader import ModelWrapper

class LLMExplainer:
    """Explain black box language models by analyzing their outputs and behaviors."""
    
    def __init__(self, model: ModelWrapper):
        """
        Initialize LLM explainer with a model.
        
        Args:
            model: ModelWrapper instance to explain
        """
        self.model = model
    
    def output_sensitivity(self, text: str, n_samples: int = 10, perturbation_std: float = 0.01) -> Dict:
        """
        Measure sensitivity of the model's output to perturbations in the input.
        
        Args:
            text: Input text to explain
            n_samples: Number of perturbation samples
            perturbation_std: Standard deviation of the perturbation
            
        Returns:
            Dictionary with sensitivity analysis results
        """
        # Get original embeddings
        original_embeddings = self.model.get_embeddings(text)
        original_output = self.model.generate(text)
        
        # Generate perturbed samples
        perturbed_outputs = []
        output_differences = []
        
        for _ in range(n_samples):
            # Add Gaussian noise to embeddings
            noise = np.random.normal(0, perturbation_std, size=original_embeddings.shape)
            perturbed_embedding = original_embeddings + noise
            
            # We'd normally use the model to generate from these perturbed embeddings,
            # but this is a simplified implementation. In a real implementation, we would
            # need to modify the model to accept perturbed embeddings.
            
            # For demonstration, we'll perturb the input text itself
            words = text.split()
            if words:
                idx = np.random.randint(0, len(words))
                # Randomly drop a word
                perturbed_words = words.copy()
                perturbed_words.pop(idx)
                perturbed_text = ' '.join(perturbed_words)
                
                # Generate from perturbed text
                perturbed_output = self.model.generate(perturbed_text)
                perturbed_outputs.append(perturbed_output)
                
                # Calculate difference (using simple word overlap)
                original_words = set(original_output.lower().split())
                perturbed_words = set(perturbed_output.lower().split())
                
                added = perturbed_words - original_words
                removed = original_words - perturbed_words
                
                output_differences.append({
                    "perturbed_text": perturbed_text,
                    "output": perturbed_output,
                    "words_added": list(added),
                    "words_removed": list(removed),
                    "similarity": len(original_words.intersection(perturbed_words)) / 
                                 len(original_words.union(perturbed_words)) if original_words or perturbed_words else 1.0
                })
        
        # Calculate overall sensitivity
        avg_similarity = np.mean([diff["similarity"] for diff in output_differences])
        sensitivity_score = 1.0 - avg_similarity
        
        return {
            "original_text": text,
            "original_output": original_output,
            "perturbed_samples": output_differences,
            "sensitivity_score": sensitivity_score,
            "analysis": f"The model's output has a sensitivity score of {sensitivity_score:.4f} "
                      f"(higher means more sensitive to input changes)."
        }
    
    def generate_counterfactuals(self, text: str, target_label: Optional[str] = None, 
                               n_attempts: int = 5) -> Dict:
        """
        Generate counterfactual examples by systematically modifying the input.
        
        Args:
            text: Input text to generate counterfactuals for
            target_label: Target label or class for counterfactual (for classification)
            n_attempts: Number of counterfactual generation attempts
            
        Returns:
            Dictionary with counterfactual examples
        """
        original_output = self.model.generate(text)
        
        # For a real implementation, we would use gradient-based methods to find
        # minimal changes that alter the output in desired ways. This is a simplified version.
        
        counterfactuals = []
        words = text.split()
        
        word_replacements = {
            'good': 'bad',
            'bad': 'good',
            'happy': 'sad',
            'sad': 'happy',
            'like': 'dislike',
            'dislike': 'like',
            'love': 'hate',
            'hate': 'love',
            'positive': 'negative',
            'negative': 'positive',
            'high': 'low',
            'low': 'high',
            'increase': 'decrease',
            'decrease': 'increase',
            'agree': 'disagree',
            'disagree': 'agree',
            'true': 'false',
            'false': 'true',
            'yes': 'no',
            'no': 'yes'
        }
        
        # Try different modifications
        for attempt in range(min(n_attempts, len(words))):
            # Try replacing a word with its opposite
            for i, word in enumerate(words):
                if word.lower() in word_replacements:
                    cf_words = words.copy()
                    cf_words[i] = word_replacements[word.lower()]
                    cf_text = ' '.join(cf_words)
                    
                    cf_output = self.model.generate(cf_text)
                    
                    # Calculate similarity
                    original_set = set(original_output.lower().split())
                    cf_set = set(cf_output.lower().split())
                    
                    similarity = len(original_set.intersection(cf_set)) / \
                                len(original_set.union(cf_set)) if original_set or cf_set else 1.0
                    
                    # If output is sufficiently different
                    if similarity < 0.8:
                        counterfactuals.append({
                            "modified_text": cf_text,
                            "modified_output": cf_output,
                            "change_description": f"Replaced '{word}' with '{cf_words[i]}'",
                            "similarity_to_original": similarity
                        })
                        break
        
        # If no good counterfactuals found by word replacement, try negation
        if not counterfactuals:
            negation_prefixes = ["I don't think that ", "It's not true that ", "Contrary to the statement that "]
            for prefix in negation_prefixes:
                cf_text = prefix + text
                cf_output = self.model.generate(cf_text)
                
                # Calculate similarity
                original_set = set(original_output.lower().split())
                cf_set = set(cf_output.lower().split())
                
                similarity = len(original_set.intersection(cf_set)) / \
                            len(original_set.union(cf_set)) if original_set or cf_set else 1.0
                
                if similarity < 0.8:
                    counterfactuals.append({
                        "modified_text": cf_text,
                        "modified_output": cf_output,
                        "change_description": f"Added negation: '{prefix}'",
                        "similarity_to_original": similarity
                    })
                    break
        
        return {
            "original_text": text,
            "original_output": original_output,
            "counterfactuals": counterfactuals,
            "analysis": f"Found {len(counterfactuals)} counterfactual examples that produce significantly different outputs."
        }
    
    def token_importance(self, text: str, method: str = "erasure") -> Dict:
        """
        Calculate importance of each token in the input.
        
        Args:
            text: Input text to analyze
            method: Method for calculating token importance
            
        Returns:
            Dictionary with token importance scores
        """
        tokens = text.split()
        original_output = self.model.generate(text)
        
        token_scores = []
        
        if method == "erasure":
            # Erasure method: remove one token at a time and see how output changes
            for i, token in enumerate(tokens):
                # Create text with this token removed
                removed_tokens = tokens.copy()
                removed_tokens.pop(i)
                modified_text = ' '.join(removed_tokens)
                
                # Get output with token removed
                modified_output = self.model.generate(modified_text)
                
                # Calculate output difference
                original_set = set(original_output.lower().split())
                modified_set = set(modified_output.lower().split())
                
                similarity = len(original_set.intersection(modified_set)) / \
                             len(original_set.union(modified_set)) if original_set or modified_set else 1.0
                
                # Importance is how much output changes when token is removed
                importance = 1.0 - similarity
                
                token_scores.append({
                    "token": token,
                    "position": i,
                    "importance": importance
                })
                
        elif method == "gradient":
            # Note: In a real implementation, this would use gradient-based attribution
            # This is a placeholder that assigns random importance scores
            for i, token in enumerate(tokens):
                # Random importance score
                importance = np.random.uniform(0, 1)
                
                token_scores.append({
                    "token": token,
                    "position": i,
                    "importance": importance
                })
        
        # Sort tokens by importance
        token_scores.sort(key=lambda x: x["importance"], reverse=True)
        
        # Format results for easy visualization
        result = {
            "original_text": text,
            "original_output": original_output,
            "token_scores": token_scores,
            "method": method,
            "all_tokens": tokens,
            "all_scores": [score["importance"] for score in token_scores]
        }
        
        return result
    
    def visualize_token_importance(self, token_importance_result: Dict):
        """
        Visualize importance of each token.
        
        Args:
            token_importance_result: Result from token_importance method
            
        Returns:
            Matplotlib figure
        """
        tokens = token_importance_result["all_tokens"]
        scores = token_importance_result["all_scores"]
        
        # Create colormap based on importance
        cmap = plt.cm.Reds
        norm = plt.Normalize(min(scores), max(scores))
        colors = cmap(norm(scores))
        
        fig, ax = plt.subplots(figsize=(12, 4))
        
        # Bar plot of token importance
        bars = ax.bar(range(len(tokens)), scores, color=colors)
        
        # Set x-tick positions and labels
        ax.set_xticks(range(len(tokens)))
        ax.set_xticklabels(tokens, rotation=45, ha='right')
        
        # Set labels and title
        ax.set_ylabel('Token Importance')
        ax.set_title(f'Token Importance Analysis ({token_importance_result["method"]} method)')
        
        # Add a colorbar
        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
        sm.set_array([])
        cbar = plt.colorbar(sm, ax=ax)
        cbar.set_label('Importance Score')
        
        plt.tight_layout()
        return fig
    
    def analyze_internal_representations(self, text: str, layer_indices: Optional[List[int]] = None) -> Dict:
        """
        Analyze internal representations of the model for the given input.
        
        Args:
            text: Input text to analyze
            layer_indices: Indices of layers to analyze (None for all layers)
            
        Returns:
            Dictionary with internal representation analysis
        """
        # Get attention patterns from model
        attention_data = self.model.get_attention(text)
        
        # In a real implementation, we would analyze the attention patterns
        # and other internal representations. This is a simplified version.
        
        # Extract tokens from input
        tokens = text.split()
        
        # If no layer indices specified, analyze all available layers
        if layer_indices is None and attention_data:
            if isinstance(attention_data, dict) and 'layers' in attention_data:
                layer_indices = list(range(len(attention_data['layers'])))
            else:
                layer_indices = [0]  # Default to first layer
        
        layer_analyses = []
        
        if attention_data:
            # Example analysis for demonstration purposes
            for layer_idx in layer_indices:
                # Extract attention for this layer (simplified)
                if isinstance(attention_data, dict) and 'layers' in attention_data:
                    layer_attention = attention_data['layers'][layer_idx]
                else:
                    # Simulate attention in simplified case
                    # In reality, attention is multi-headed and more complex
                    # This is just a placeholder
                    num_tokens = len(tokens)
                    layer_attention = np.random.rand(num_tokens, num_tokens)
                    # Normalize rows to sum to 1
                    layer_attention = layer_attention / layer_attention.sum(axis=1, keepdims=True)
                
                # Find token pairs with strongest attention
                token_pairs = []
                if isinstance(layer_attention, np.ndarray):
                    # Get top 3 attention pairs per token
                    for i in range(min(len(tokens), layer_attention.shape[0])):
                        attn_row = layer_attention[i]
                        # Get indices of top 3 attention values
                        top_indices = np.argsort(attn_row)[-3:][::-1]
                        for j in top_indices:
                            if j < len(tokens) and i != j:  # Avoid self-attention
                                token_pairs.append({
                                    "source": tokens[i],
                                    "target": tokens[j],
                                    "attention_score": float(attn_row[j])
                                })
                
                layer_analyses.append({
                    "layer_idx": layer_idx,
                    "token_pairs": token_pairs,
                    "summary": f"Layer {layer_idx} shows notable attention patterns between key tokens."
                })
        
        return {
            "input_text": text,
            "tokens": tokens,
            "layer_analyses": layer_analyses,
            "summary": "Analysis of internal representations reveals patterns of token relationships and model focus."
        }
    
    def visualize_attention_patterns(self, text: str, layer_idx: int = 0, head_idx: int = 0):
        """
        Visualize attention patterns for a specific layer and head.
        
        Args:
            text: Input text to analyze
            layer_idx: Index of the layer to visualize
            head_idx: Index of the attention head to visualize
            
        Returns:
            Matplotlib figure
        """
        tokens = text.split()
        
        # Get attention patterns
        attention_data = self.model.get_attention(text)
        
        # Extract attention weights for specified layer and head
        # This is a simplified version - in reality, the data structure may be more complex
        if attention_data:
            if isinstance(attention_data, dict) and 'layers' in attention_data:
                try:
                    layer_attention = attention_data['layers'][layer_idx]
                    if 'heads' in layer_attention:
                        attention_weights = layer_attention['heads'][head_idx]
                    else:
                        attention_weights = layer_attention
                except (IndexError, KeyError):
                    # Fallback to random weights for demonstration
                    attention_weights = np.random.rand(len(tokens), len(tokens))
            else:
                # Fallback to random weights for demonstration
                attention_weights = np.random.rand(len(tokens), len(tokens))
        else:
            # Fallback to random weights for demonstration
            attention_weights = np.random.rand(len(tokens), len(tokens))
        
        # Convert to numpy array if not already
        if not isinstance(attention_weights, np.ndarray):
            attention_weights = np.array(attention_weights)
        
        # Make sure attention_weights has the right shape
        if attention_weights.shape[0] != len(tokens) or attention_weights.shape[1] != len(tokens):
            # Resize to match tokens
            attention_weights = np.random.rand(len(tokens), len(tokens))
        
        # Create attention matrix visualization
        fig, ax = plt.subplots(figsize=(10, 8))
        im = ax.imshow(attention_weights, cmap='YlOrRd')
        
        # Set labels for axes
        ax.set_xticks(range(len(tokens)))
        ax.set_yticks(range(len(tokens)))
        ax.set_xticklabels(tokens, rotation=45, ha='right')
        ax.set_yticklabels(tokens)
        
        # Add colorbar
        cbar = ax.figure.colorbar(im, ax=ax)
        cbar.set_label('Attention Weight')
        
        # Set title
        ax.set_title(f'Attention Patterns (Layer {layer_idx}, Head {head_idx})')
        
        # Add grid to separate tokens
        ax.set_xticks(np.arange(-.5, len(tokens), 1), minor=True)
        ax.set_yticks(np.arange(-.5, len(tokens), 1), minor=True)
        ax.grid(which='minor', color='w', linestyle='-', linewidth=2)
        
        plt.tight_layout()
        return fig
    
    def explain_generation(self, prompt: str, max_length: int = 50) -> Dict:
        """
        Provide a comprehensive explanation of the model's generation process.
        
        Args:
            prompt: Input prompt for generation
            max_length: Maximum length of generation
            
        Returns:
            Dictionary with explanation of the generation process
        """
        # Generate output from the prompt
        output = self.model.generate(prompt, max_length=max_length)
        
        # Get token importance
        importance_result = self.token_importance(prompt)
        
        # Analyze internal representations
        internal_representations = self.analyze_internal_representations(prompt)
        
        # Generate counterfactuals
        counterfactuals = self.generate_counterfactuals(prompt, n_attempts=3)
        
        # Integrate all analyses into a comprehensive explanation
        token_strings = [f"'{t['token']}'" for t in importance_result['token_scores'][:3]]
        
        explanation = {
            "prompt": prompt,
            "generated_output": output,
            "most_important_tokens": importance_result["token_scores"][:3],
            "key_attention_patterns": internal_representations["layer_analyses"],
            "counterfactual_examples": counterfactuals["counterfactuals"],
            "summary": (
                f"The model generated: '{output}'. "
                f"The generation was most influenced by the tokens "
                f"{', '.join(token_strings)}. "
                f"Changing these tokens significantly alters the output, as shown by the counterfactual examples."
            ),
            "recommendations": [
                "If you want a different output, consider modifying the most important tokens.",
                "The model seems to focus on specific patterns of token relationships - this gives insight into its reasoning.",
                "Consider the counterfactual examples to understand how small changes can affect generation."
            ]
        }
        
        return explanation 