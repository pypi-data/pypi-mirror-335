"""
Robustness evaluator module for the LingualLens framework.

This module provides tools for evaluating language model robustness
against various types of adversarial attacks and perturbations.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Union, Optional, Any

from ..core.model_loader import ModelWrapper
from ..core.evaluator import Evaluator, EvaluationResult
from .attack_generator import AttackGenerator

class RobustnessEvaluator(Evaluator):
    """Evaluate language model robustness against adversarial attacks."""
    
    def __init__(self, model: ModelWrapper):
        """
        Initialize robustness evaluator with a model.
        
        Args:
            model: ModelWrapper instance to evaluate
        """
        super().__init__()
        self.model = model
        self.attack_generator = AttackGenerator(model)
    
    def evaluate(self, texts: List[str], perturbation_types: List[str] = None) -> EvaluationResult:
        """
        Evaluate model robustness against various perturbations.
        
        Args:
            texts: List of input texts to evaluate
            perturbation_types: Types of perturbations to apply
            
        Returns:
            EvaluationResult with robustness metrics
        """
        if perturbation_types is None:
            perturbation_types = ["typos", "synonyms", "paraphrase"]
        
        # Create result
        result = self._create_result()
        result.add_metadata("num_texts", len(texts))
        result.add_metadata("perturbation_types", perturbation_types)
        
        # Get original outputs
        original_outputs = {}
        for text in texts:
            original_outputs[text] = self.model.generate(text)
        
        # Track how perturbations affect outputs
        perturbation_results = {}
        
        for perturb_type in perturbation_types:
            perturbation_results[perturb_type] = []
            
            for text in texts:
                original_output = original_outputs[text]
                
                # Generate perturbed text
                perturbed_text = self._generate_perturbation(text, perturb_type)
                
                # Get output for perturbed text
                perturbed_output = self.model.generate(perturbed_text)
                
                # Calculate similarity between original and perturbed outputs
                similarity = self._calculate_output_similarity(original_output, perturbed_output)
                
                perturbation_results[perturb_type].append({
                    "original_text": text,
                    "perturbed_text": perturbed_text,
                    "original_output": original_output,
                    "perturbed_output": perturbed_output,
                    "similarity": similarity
                })
        
        # Calculate overall robustness metrics
        for perturb_type, results in perturbation_results.items():
            similarities = [r["similarity"] for r in results]
            avg_similarity = np.mean(similarities)
            std_similarity = np.std(similarities)
            
            result.add_metric(f"robustness_{perturb_type}", avg_similarity)
            result.add_metric(f"robustness_{perturb_type}_std", std_similarity)
            
            # Add robustness interpretations
            if avg_similarity > 0.8:
                level = "high"
            elif avg_similarity > 0.5:
                level = "moderate"
            else:
                level = "low"
                
            result.add_detail(f"robustness_{perturb_type}_level", level)
            result.add_detail(f"robustness_{perturb_type}_details", results)
        
        # Add overall robustness score (average across all perturbation types)
        overall_score = np.mean([result.metrics[f"robustness_{p}"] for p in perturbation_types])
        result.add_metric("overall_robustness", overall_score)
        
        if overall_score > 0.8:
            result.add_detail("interpretation", "The model shows high robustness to perturbations.")
        elif overall_score > 0.5:
            result.add_detail("interpretation", "The model shows moderate robustness to perturbations.")
        else:
            result.add_detail("interpretation", "The model shows low robustness to perturbations.")
        
        return result
    
    def evaluate_prompt_injection(self, 
                               system_prompts: List[str], 
                               user_prompts: List[str], 
                               injection_goals: List[str],
                               attempts_per_test: int = 5) -> EvaluationResult:
        """
        Evaluate model robustness against prompt injection attacks.
        
        Args:
            system_prompts: List of system prompts to test
            user_prompts: List of user prompts to test
            injection_goals: List of injection goals to test
            attempts_per_test: Number of injection attempts per test
            
        Returns:
            EvaluationResult with prompt injection robustness metrics
        """
        # Create result
        result = self._create_result()
        result.add_metadata("num_system_prompts", len(system_prompts))
        result.add_metadata("num_user_prompts", len(user_prompts))
        result.add_metadata("num_injection_goals", len(injection_goals))
        result.add_metadata("attempts_per_test", attempts_per_test)
        
        # Run injection tests
        test_results = []
        
        # For brevity, we'll test a subset of combinations
        for system_prompt in system_prompts:
            for user_prompt in user_prompts[:1]:  # Test first user prompt with each system prompt
                for injection_goal in injection_goals:
                    # Run injection test
                    injection_result = self.attack_generator.test_prompt_injection(
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        injection_goal=injection_goal,
                        num_attempts=attempts_per_test
                    )
                    
                    test_results.append({
                        "system_prompt": system_prompt,
                        "user_prompt": user_prompt,
                        "injection_goal": injection_goal,
                        "success_rate": injection_result["success_rate"],
                        "details": injection_result
                    })
        
        # Calculate overall injection robustness
        success_rates = [r["success_rate"] for r in test_results]
        avg_success_rate = np.mean(success_rates) if success_rates else 0
        
        # Lower success rate means higher robustness
        injection_robustness = 1.0 - avg_success_rate
        
        result.add_metric("prompt_injection_robustness", injection_robustness)
        result.add_metric("avg_injection_success_rate", avg_success_rate)
        result.add_detail("injection_test_results", test_results)
        
        if injection_robustness > 0.8:
            result.add_detail("interpretation", "The model shows high robustness to prompt injection.")
        elif injection_robustness > 0.5:
            result.add_detail("interpretation", "The model shows moderate robustness to prompt injection.")
        else:
            result.add_detail("interpretation", "The model shows low robustness to prompt injection.")
        
        return result
    
    def evaluate_adversarial_attacks(self, texts: List[str], 
                                  attack_types: List[str] = None) -> EvaluationResult:
        """
        Evaluate model robustness against adversarial attacks.
        
        Args:
            texts: List of input texts to evaluate
            attack_types: Types of attacks to apply
            
        Returns:
            EvaluationResult with adversarial attack robustness metrics
        """
        if attack_types is None:
            attack_types = ["word_replacement", "paraphrase"]
        
        # Create result
        result = self._create_result()
        result.add_metadata("num_texts", len(texts))
        result.add_metadata("attack_types", attack_types)
        
        # Run untargeted attacks
        untargeted_results = {}
        
        for attack_type in attack_types:
            untargeted_results[attack_type] = []
            
            for text in texts:
                # Run attack
                attack_result = self.attack_generator.generate_untargeted_attack(
                    text=text,
                    max_iterations=10,
                    attack_type=attack_type
                )
                
                untargeted_results[attack_type].append({
                    "text": text,
                    "success": attack_result["success"],
                    "difference": attack_result.get("best_difference", 0.0),
                    "details": attack_result
                })
        
        # Calculate success rates and robustness scores
        for attack_type, results in untargeted_results.items():
            success_rate = sum(1 for r in results if r["success"]) / len(results) if results else 0
            avg_difference = np.mean([r["difference"] for r in results]) if results else 0
            
            # Lower success rate means higher robustness
            attack_robustness = 1.0 - success_rate
            
            result.add_metric(f"untargeted_{attack_type}_robustness", attack_robustness)
            result.add_metric(f"untargeted_{attack_type}_success_rate", success_rate)
            result.add_metric(f"untargeted_{attack_type}_avg_difference", avg_difference)
            result.add_detail(f"untargeted_{attack_type}_results", results)
        
        # Calculate overall robustness score (average across all attack types)
        overall_score = np.mean([result.metrics[f"untargeted_{a}_robustness"] for a in attack_types])
        result.add_metric("overall_adversarial_robustness", overall_score)
        
        if overall_score > 0.8:
            result.add_detail("interpretation", "The model shows high robustness to adversarial attacks.")
        elif overall_score > 0.5:
            result.add_detail("interpretation", "The model shows moderate robustness to adversarial attacks.")
        else:
            result.add_detail("interpretation", "The model shows low robustness to adversarial attacks.")
        
        return result
    
    def visualize_robustness(self, evaluation_result: EvaluationResult):
        """
        Visualize robustness evaluation results.
        
        Args:
            evaluation_result: EvaluationResult from a robustness evaluation
            
        Returns:
            Matplotlib figure
        """
        # Extract metrics from result
        metrics = {}
        for key, value in evaluation_result.metrics.items():
            if key.startswith("robustness_") and not key.endswith("_std"):
                metrics[key.replace("robustness_", "")] = value
        
        if not metrics:
            # Try adversarial metrics if no perturbation metrics found
            for key, value in evaluation_result.metrics.items():
                if key.startswith("untargeted_") and key.endswith("_robustness"):
                    metrics[key.replace("untargeted_", "").replace("_robustness", "")] = value
        
        if not metrics:
            # If still no metrics, just use whatever's available
            metrics = {k: v for k, v in evaluation_result.metrics.items() if isinstance(v, (int, float))}
        
        # Create bar chart
        fig, ax = plt.subplots(figsize=(10, 6))
        
        perturbation_types = list(metrics.keys())
        scores = [metrics[p] for p in perturbation_types]
        
        bars = ax.bar(perturbation_types, scores, color=plt.cm.viridis(np.linspace(0, 0.8, len(perturbation_types))))
        
        # Add labels and title
        ax.set_ylabel('Robustness Score (higher is better)')
        ax.set_title('Model Robustness by Perturbation/Attack Type')
        ax.set_ylim(0, 1.0)
        
        # Add a horizontal line for the threshold between robustness levels
        ax.axhline(y=0.8, linestyle='--', color='green', alpha=0.5)
        ax.axhline(y=0.5, linestyle='--', color='orange', alpha=0.5)
        
        # Add value labels on bars
        for i, v in enumerate(scores):
            ax.text(i, v + 0.02, f'{v:.2f}', ha='center')
        
        # Add a legend for robustness levels
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='none', edgecolor='green', linestyle='--', label='High Robustness (>0.8)'),
            Patch(facecolor='none', edgecolor='orange', linestyle='--', label='Moderate Robustness (>0.5)')
        ]
        ax.legend(handles=legend_elements, loc='lower center')
        
        plt.tight_layout()
        return fig
    
    def _generate_perturbation(self, text: str, perturb_type: str) -> str:
        """Generate perturbed version of the input text."""
        words = text.split()
        
        if not words:
            return text
        
        if perturb_type == "typos":
            # Add random typos
            idx = np.random.randint(0, len(words))
            word = words[idx]
            if len(word) > 2:
                # Swap two adjacent characters
                char_idx = np.random.randint(0, len(word) - 1)
                new_word = word[:char_idx] + word[char_idx + 1] + word[char_idx] + word[char_idx + 2:]
                words[idx] = new_word
                return ' '.join(words)
            return text  # No change if word too short
            
        elif perturb_type == "synonyms":
            # Replace a word with synonym
            idx = np.random.randint(0, len(words))
            word = words[idx].lower()
            
            # Simple synonym mapping
            synonyms = {
                'good': ['great', 'excellent', 'fine', 'nice'],
                'bad': ['poor', 'terrible', 'awful', 'unpleasant'],
                'big': ['large', 'huge', 'enormous', 'massive'],
                'small': ['tiny', 'little', 'miniature', 'compact'],
                'happy': ['glad', 'joyful', 'pleased', 'delighted'],
                'sad': ['unhappy', 'sorrowful', 'depressed', 'gloomy'],
                'beautiful': ['pretty', 'lovely', 'attractive', 'gorgeous'],
                'ugly': ['unattractive', 'hideous', 'unsightly', 'plain'],
            }
            
            if word in synonyms:
                words[idx] = np.random.choice(synonyms[word])
                return ' '.join(words)
            return text  # No change if no synonym found
            
        elif perturb_type == "paraphrase":
            # Simple rule-based paraphrasing
            paraphrase_patterns = [
                (r'I am', 'I\'m'),
                (r'You are', 'You\'re'),
                (r'They are', 'They\'re'),
                (r'We are', 'We\'re'),
                (r'It is', 'It\'s'),
                (r'do not', 'don\'t'),
                (r'does not', 'doesn\'t'),
                (r'did not', 'didn\'t'),
                (r'is not', 'isn\'t'),
                (r'are not', 'aren\'t'),
            ]
            
            for pattern, replacement in paraphrase_patterns:
                if pattern in text:
                    return text.replace(pattern, replacement)
                elif replacement in text:
                    return text.replace(replacement, pattern)
            
            # Add a filler word if no patterns matched
            filler_words = ['actually', 'basically', 'certainly', 'definitely', 'possibly']
            filler = np.random.choice(filler_words)
            idx = np.random.randint(0, len(words))
            words.insert(idx, filler)
            return ' '.join(words)
        
        return text  # Default no change
    
    def _calculate_output_similarity(self, output1: str, output2: str) -> float:
        """Calculate similarity between two outputs."""
        # Simple Jaccard similarity
        set1 = set(output1.lower().split())
        set2 = set(output2.lower().split())
        
        if not set1 and not set2:
            return 1.0
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0 