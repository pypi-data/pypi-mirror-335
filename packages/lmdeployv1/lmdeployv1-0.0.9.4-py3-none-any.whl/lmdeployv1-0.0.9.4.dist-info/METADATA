Metadata-Version: 2.1
Name: lmdeployv1
Version: 0.0.9.4
Summary: LMDeploy api client
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: bceidaas ==0.0.1.dev2
Requires-Dist: openai ==1.55.3
Requires-Dist: pyrate-limiter ~=2.10.0
Requires-Dist: tenacity ==8.2.2

# 项目名称
LMDeploy API Client

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)

## 简介
LMDeploy API Client 是一个用于请求多模态大模型的部署服务的 Python API。它提供了一组方便的功能。

### 本地构建
1. 执行以下命令来构建项目：
cd windmill-endpoint/sdk/python/lmdeployv1 && make -f Makefile
2. 构建完成后，你可以在 `output/dist/` 目录下找到生成的lmdeployv1 whl文件
3. 执行以下命令来安装构建好的包：
pip install -r lmdeployv1-x.x.x-py3-none-any.whl

### 发布到pypi源
cd windmill-endpoint/sdk/python/lmdeployv1 && make -f Makefile publish_pypi

### 直接安装
你也可以直接安装已构建好的包：
- pip install lmdeployv1
- 如果需要再别的项目里引用本包 推荐使用：uv add lmdeployv1 或者 uv add lmdeployv1==0.0.9 [https://docs.astral.sh/uv/]

### 如何使用
```python
from lmdeployv1.client import LMDeployClient

client = LMDeployClient(
    endpoint="http://127.0.0.1:8312/ep-llm"
)
# 带鉴权版本，主要是需要有大模型使用权益的OrgID，UserID可填*或者等同于OrgID
auth_client = LMDeployClient(
    endpoint="http://127.0.0.1:8312/ep-llm",
    context={
        "OrgID":"XXXXXXXXXXX",
        "UserID":"XXXXXXXXXXX",
    }
)

from lmdeployv1.client import build_chat_messages
message = {"{image_url}": "Is there any cat in the image? only answer yes or no."}
client.chat_completion(
    messages=build_chat_messages(message),
    top_p=0.9,
    temperature=0.6,
    ....., # 其他参数
)

# 批量请求
from lmdeployv1.client import build_batch_chat_messages
from lmdeployv1.api import BatchChatCompletionRequest
client.batch_chat_completion(BatchChatCompletionRequest(
    messages=build_batch_chat_messages([message1, message2]),
    temperature=0.6,
    top_p=0.9,
    ....., # 其他参数
))
```



