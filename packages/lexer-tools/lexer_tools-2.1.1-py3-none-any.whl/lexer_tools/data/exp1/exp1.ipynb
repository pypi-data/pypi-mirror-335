{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f87fc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize Output: ['feline', 'mammal', 'usually', 'having', 'thick', 'soft', 'fur', 'and', 'no', 'ability', 'to', 'roar', ':', 'domestic', 'cats', ';', 'wildcats']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"word_tokenize Output:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067fc2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feline', 'mammal', 'usually', 'having', 'thick', 'soft', 'fur', 'and', 'no', 'ability', 'to', 'roar', ':', 'domestic', 'cats', ';', 'wildcats']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(\"feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\")\n",
    "print(tokens) # Output: ['I', \"'m\", 'learning', 'NLP', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0d15d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feline', 'mammal', 'usually', 'having', 'thick', 'soft', 'fur', 'and', 'no', 'ability', 'to', 'roar', ':', 'domestic', 'cats', ';', 'wildcats']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(\"feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\")\n",
    "print(tokens) # Output: ['I', \"'m\", 'learning', 'NLP', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96401de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a3df60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nltk.tokenize import PunktWordTokenizer\\npunkt_tokenizer = PunktWordTokenizer()\\ntext = \"feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats.\"\\ntokens_punkt = punkt_tokenizer.tokenize(text)\\nprint(\"PunktWordTokenizer Output:\", tokens_punkt)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from nltk.tokenize import PunktWordTokenizer\n",
    "punkt_tokenizer = PunktWordTokenizer()\n",
    "text = \"feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats.\"\n",
    "tokens_punkt = punkt_tokenizer.tokenize(text)\n",
    "print(\"PunktWordTokenizer Output:\", tokens_punkt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5678550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"won't\", 'let', 'you', 'down.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "text = \"I won't let you down.\"\n",
    "sentences = tokenizer.tokenize(text)\n",
    "word_tokens = []\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()  \n",
    "    word_tokens.extend(words)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7b56f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPunctTokenizer Output: ['feline', 'mammal', 'usually', 'having', 'thick', 'soft', 'fur', 'and', 'no', 'ability', 'to', 'roar', ':', 'domestic', 'cats', ';', 'wildcats', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "text = \"feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats.\"\n",
    "tokens_word_punct = word_punct_tokenizer.tokenize(text)\n",
    "print(\"WordPunctTokenizer Output:\", tokens_word_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d72d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'writer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = ['I', 'am', 'a', 'writer']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50690487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18f8ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat.n.01'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "syn = wn.synsets('cat')[0]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1aeee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da44333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dca84cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = syn.lemmas()\n",
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15830754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evil'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "syn1 =wn.synset('good.n.02') \n",
    "antonym1 =syn1.lemmas()[0].antonyms()[0] \n",
    "antonym1.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7332acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "Lanc_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "894ae7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b92b2d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "Reg_stemmer = RegexpStemmer('ing')\n",
    "Reg_stemmer.stem('ingeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e9153e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb180ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93ae000f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'wo', \"n't\", 'be', 'able', 'to', 'do', 'this', 'now']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from replacerRE import REReplacer\n",
    "rep_word = REReplacer()\n",
    "word_tokenize(\"I won't be able to do this now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acb82a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'will', 'not', 'be', 'able', 'to', 'do', 'this', 'now']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(rep_word.replace(\"I won't be able to do this now\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ddf31d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from removalrepeat import Rep_word_removal\n",
    "rep_word = Rep_word_removal()\n",
    "rep_word.replace (\"Youuuuuuuuuuuu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "564914ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'birthday'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replacesyn import word_syn_replacer\n",
    "rep_syn = word_syn_replacer ({'bday': 'birthday'})\n",
    "rep_syn.replace('bday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09cdf360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replacesyn import CSVword_syn_replacer\n",
    "rep_syn = CSVword_syn_replacer ('syn.csv')\n",
    "rep_syn.replace('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b8ae7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'birthday'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replacesyn import YAMLword_syn_replacer\n",
    "rep_syn = YAMLword_syn_replacer ('syn.yaml')\n",
    "rep_syn.replace('bday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "767b626c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let us', 'beautify', 'our', 'country']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replacerantonym import word_antonym_replacer\n",
    "rep_antonym = word_antonym_replacer ()\n",
    "rep_antonym.replace('uglify')\n",
    "sentence = [\"Let us\", 'not', 'uglify', 'our', 'country']\n",
    "rep_antonym.replace_negations(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5345fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1760196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elon', 'Musk', 'is', 'the', 'CEO', 'of', 'SpaceX', ',', 'which', 'is', 'based', 'in', 'California', '.']\n",
      "[('Elon', 'NNP'), ('Musk', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('CEO', 'NN'), ('of', 'IN'), ('SpaceX', 'NNP'), (',', ','), ('which', 'WDT'), ('is', 'VBZ'), ('based', 'VBN'), ('in', 'IN'), ('California', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON Elon/NNP)\n",
      "  (ORGANIZATION Musk/NNP)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  (ORGANIZATION CEO/NN of/IN SpaceX/NNP)\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  based/VBN\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Elon Musk is the CEO of SpaceX, which is based in California.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c415185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "named_entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60857dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(s (NP (Det the) (N dog)) (NP (Det the) (N cat)))\n",
      "             s             \n",
      "      _______|_______       \n",
      "     NP              NP    \n",
      "  ___|___         ___|___   \n",
      "Det      N      Det      N \n",
      " |       |       |       |  \n",
      "the     dog     the     cat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#Some trees to run tests on:\n",
    "dp1=Tree('NP',[Tree('Det',['the']),Tree('N',['dog'])]) #dp is the label\n",
    "dp2=Tree('NP',[Tree('Det',['the']),Tree('N',['cat'])]) #dp is the label\n",
    "tree=Tree('s',[dp1,dp2]) # s is the label\n",
    "print(tree)\n",
    "tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebf49b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Sentence: The cat chased the mouse\n",
      "\n",
      "Tokens: ['the', 'cat', 'chased', 'the', 'mouse'] \n",
      "\n",
      "[Tree('S', [Tree('NP', [Tree('Det', ['the']), Tree('N', ['cat'])]), Tree('VP', [Tree('V', ['chased']), Tree('NP', [Tree('Det', ['the']), Tree('N', ['mouse'])])])])] \n",
      "\n",
      "(S (NP (Det the) (N cat)) (VP (V chased) (NP (Det the) (N mouse))))\n",
      "              S                 \n",
      "      ________|_____             \n",
      "     |              VP          \n",
      "     |         _____|___         \n",
      "     NP       |         NP      \n",
      "  ___|___     |      ___|____    \n",
      "Det      N    V    Det       N  \n",
      " |       |    |     |        |   \n",
      "the     cat chased the     mouse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    VP -> V NP\n",
    "    NP -> Det N\n",
    "    V -> \"chased\"\n",
    "    Det -> \"the\"\n",
    "    N -> \"cat\" | \"mouse\"\n",
    "\"\"\")\n",
    "# Sentence to parse\n",
    "sentence = \"The cat chased the mouse\"\n",
    "print(f\"Parsing Sentence: {sentence}\\n\")\n",
    "# Tokenize the sentence and convert all tokens to lowercase\n",
    "sentence_tokens = [word.lower() for word in nltk.word_tokenize(sentence)]\n",
    "print(\"Tokens:\", sentence_tokens,\"\\n\")\n",
    "# Using ChartParser to parse the sentence\n",
    "parser = nltk.ChartParser(grammar)\n",
    "# Check for possible parse trees\n",
    "try:\n",
    "    trees = list(parser.parse(sentence_tokens))\n",
    "    print(trees,\"\\n\")\n",
    "    if trees:\n",
    "        for tree in trees:\n",
    "            print(tree)\n",
    "            tree.pretty_print()\n",
    "    else:\n",
    "        print(\"No valid parse tree found.\")\n",
    "except ValueError as e:\n",
    "    print(\"Parsing error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36109208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Sentence: John eats an apple\n",
      "\n",
      "Tokens: ['John', 'eats', 'an', 'apple'] \n",
      "\n",
      "           S               \n",
      "  _________|___             \n",
      " |             VP          \n",
      " |     ________|___         \n",
      " NP   |            NP      \n",
      " |    |         ___|____    \n",
      " N    V       Det       N  \n",
      " |    |        |        |   \n",
      "John eats      an     apple\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "# Define grammar for the sentence \"John eats an apple.\"\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> N\n",
    "V -> \"eats\"\n",
    "NP -> Det N\n",
    "Det -> \"an\"\n",
    "N -> \"John\" | \"apple\"\n",
    "\"\"\")\n",
    "# Sentence to parse\n",
    "sentence = \"John eats an apple\"\n",
    "print(f\"Parsing Sentence: {sentence}\\n\")\n",
    "# Tokenize the sentence\n",
    "sentence_tokens = nltk.word_tokenize(sentence)\n",
    "print(\"Tokens:\", sentence_tokens,\"\\n\")\n",
    "# Using ChartParser\n",
    "parser = nltk.ChartParser(grammar)\n",
    "# Check for possible parse trees\n",
    "try:\n",
    "    trees = list(parser.parse(sentence_tokens))\n",
    "    if trees:\n",
    "        for tree in trees:\n",
    "            tree.pretty_print()\n",
    "    else:\n",
    "        print(\"No valid parse tree found.\")\n",
    "except ValueError as e:\n",
    "    print(\"Error:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "346476a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Sentence: The dog sleeps on the mat.\n",
      "\n",
      "Tokens: ['the', 'dog', 'sleeps', 'on', 'the', 'mat'] \n",
      "\n",
      "              S                       \n",
      "      ________|_________               \n",
      "     |                  VP            \n",
      "     |         _________|___           \n",
      "     |        |             PP        \n",
      "     |        |      _______|___       \n",
      "     NP       |     |           NP    \n",
      "  ___|___     |     |        ___|___   \n",
      "Det      N    V     P      Det      N \n",
      " |       |    |     |       |       |  \n",
      "the     dog sleeps  on     the     mat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "# Define grammar for the sentence \"The dog sleeps on the mat.\"\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V PP | V\n",
    "NP -> Det N\n",
    "PP ->P NP\n",
    "V -> \"sleeps\"\n",
    "Det -> \"the\"\n",
    "N -> \"dog\" | \"mat\"\n",
    "P -> \"on\"\n",
    "\"\"\")\n",
    "# Sentence to parse\n",
    "sentence = \"The dog sleeps on the mat.\"\n",
    "print(f\"Parsing Sentence: {sentence}\\n\")\n",
    "# Tokenize the sentence\n",
    "sentence_tokens = nltk.word_tokenize(sentence.lower().strip('.'))\n",
    "print(\"Tokens:\", sentence_tokens,\"\\n\")\n",
    "# Using ChartParser\n",
    "parser = nltk.ChartParser(grammar)\n",
    "# Check for possible parse trees\n",
    "try:\n",
    "    trees = list(parser.parse(sentence_tokens))\n",
    "    if trees:\n",
    "        for tree in trees:\n",
    "            tree.pretty_print()\n",
    "    else:\n",
    "        print(\"No valid parse tree found.\")\n",
    "except ValueError as e:\n",
    "    print(\"Error:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24407515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The dog sleeps on the mat ---\n",
      "Parsing Sentence (The dog sleeps on the mat): The dog sleeps on the mat\n",
      "\n",
      "Tokens: ['the', 'dog', 'sleeps', 'on', 'the', 'mat'] \n",
      "\n",
      "              S                       \n",
      "      ________|_________               \n",
      "     |                  VP            \n",
      "     |         _________|___           \n",
      "     |        |             PP        \n",
      "     |        |      _______|___       \n",
      "     NP       |     |           NP    \n",
      "  ___|___     |     |        ___|___   \n",
      "Det      N    V     P      Det      N \n",
      " |       |    |     |       |       |  \n",
      "the     dog sleeps  on     the     mat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "# Define grammar for the sentence \"The dog sleeps on the mat.\"\n",
    "grammar_case1 = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V PP | V\n",
    "NP -> Det N\n",
    "PP ->P NP\n",
    "V -> \"sleeps\"\n",
    "Det -> \"the\"\n",
    "N -> \"dog\" | \"mat\"\n",
    "P -> \"on\"\n",
    "\"\"\")\n",
    "# Sentence to parse\n",
    "sentence = \"The dog sleeps on the mat\"\n",
    "\n",
    "def parse_sentence_with_grammer(sentence,grammer,case_description):\n",
    "    print(f\"\\n--- {case_description} ---\")\n",
    "    print(f\"Parsing Sentence ({case_description}): {sentence}\\n\")\n",
    "    sentence_tokens = nltk.word_tokenize(sentence.lower())\n",
    "    print(\"Tokens:\", sentence_tokens,\"\\n\")\n",
    "    parser = nltk.ChartParser(grammar)\n",
    "    try:\n",
    "        trees = list(parser.parse(sentence_tokens))\n",
    "        if trees:\n",
    "            for tree in trees:\n",
    "                tree.pretty_print()\n",
    "        else:\n",
    "            print(\"No valid parse tree found.\")\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\",e)\n",
    "\n",
    "parse_sentence_with_grammer(sentence,grammar_case1,\"The dog sleeps on the mat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
