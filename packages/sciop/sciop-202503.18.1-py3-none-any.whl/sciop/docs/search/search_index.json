{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SciOp Documentation","text":"<p>What up, this is just a placeholder and an invitation for our dear collaborators to help fill this baby in.</p> <p>For further info on how mkdocs works, see: - the main docs at mkdocs.org. - the material theme docs</p>"},{"location":"sciop/error_handling/","title":"Error Handling","text":"<p>(stub file... todo)</p>"},{"location":"sciop/error_handling/#modals","title":"Modals","text":"<p>(todo: link to code when we figure out how autodocs in mkdocs works)</p> <p>When requests are made from the web interface using HTMX, they have a <code>HX-Request: true</code> header.  When the server returns an HTTP status code above 400,  an exception handler middleware uses the <code>HX-Reswap</code> and <code>HX-Retarget</code> response headers to retarget that to a model alert to a hidden dom element <code>#error-modal-container</code>.</p> <p>Clientside, error events are handled  - First with a <code>htmx:beforeOnLoad</code> handler that forces htmx to reswap the error   (by default, errors are not swapped) - Second, an <code>htmx:afterSwap</code> handler adds event listeners for the close buttons   and set the modal as being in focus for screen readers.</p> <p>Once the modal has no more children, it returns to being hidden using CSS selectors.</p>"},{"location":"sciop/error_handling/#form-validation-and-422s","title":"Form Validation and 422's","text":"<p>422 errors are handled as a special case by the client, as a result, 422 errors should not be used for reporting errors with an <code>HTTPException</code>. </p> <p>(todo: document how the form validation error handling works)</p>"},{"location":"sciop/jobs/","title":"Scheduled Jobs","text":"<p>Scheduled jobs have three components:</p> <ul> <li><code>jobs</code> - the top-level operation that defines when a job is executed by calling a <code>service</code></li> <li><code>services</code> - the code for the job</li> <li><code>scheduler</code> - the long-running process that executes jobs according to their scheduling configuration</li> </ul>"},{"location":"sciop/jobs/#jobs","title":"Jobs","text":"<p>The <code>jobs.py</code> file is intended as a shorthand index of available jobs. Each top-level job in <code>jobs.py</code> should contain as little code as possible, ideally just calling a single service function. </p> <p>Each job should have its own configuration in the <code>config</code> object that allows it to be enabled/disabled and otherwise have its scheduling configured.</p>"},{"location":"sciop/jobs/#services","title":"Services","text":"<p>Services should be relatively self-contained, such that disabling a service does not cause the site to become unusable.</p> <p>Services should be <code>async</code> and make careful use of resources, such as limiting the number of concurrent <code>async</code> calls with a <code>Semaphore</code> or otherwise. Avoid long-running processes, and split jobs into small, atomic units when possible so that failure of any part of them doesn't break the whole thing.</p> <p>Services should not have side effects except for mutating the database.</p> <p>Services should export a single function with no parameters up to <code>sciop.services</code> to be called from <code>jobs.py</code></p>"},{"location":"sciop/jobs/#scheduler","title":"Scheduler","text":"<p>The scheduler module wraps APScheduler with a handful of convenience methods.  It is started and torn down with starlette's lifespan contextmanager methods. </p> <p>The scheduler uses the database as its job store,  so jobs can be persisted in the case of server crashes, though currently they are removed when the server is requested to do an orderly shutdown.</p>"},{"location":"scraping/checksums/","title":"Computing checksums for files","text":"<p>Checksums are a way to generate a fixed-size \"fingerprint\" of a file. They are used to verify that two copies of a file are the same without comparing the files themselves. For example, if we have a large file and you have a large file, we can check that they are the same (with very high probability) by running a command like,</p> <pre><code>$ sha512sum TMY-User-Manual.pdf \n2693e3bd683b3d7283c60b2c83e2e... TMY-User-Manual.pdf\n</code></pre> <p>(where we have elided the complete checksum because it is quite long).</p> <p>There are several checksum algorithms and they vary in how expensive they are to compute and how likely it is that two different files have the same checksum, called a \"collision\". CRC32 is not appropriate for this use because it has a quite high chance of collisions, but it is used on some communication links. MD5 and SHA1 are quite old and collisions are known to be possible. SHA256 and SHA512 are thought to be robust enough that the chance of a collision is essentially zero.</p> <p>Our convention is to store SHA512 checksums in a file called <code>SHA512.sums</code> in the top level directory of a dataset, together with a <code>README.txt</code> (or <code>README.nfo</code> or <code>README.md</code>) file. The actual data should be in a subdirectory.</p> <p>To create a <code>SHA512.sums</code> file, suppose that the data is in a subdirectory called <code>tmy</code>. We would then do,</p> <pre><code>$ find ./tmy -type f -print0 | xargs -0 sha512sum &gt; SHA512.sums\n</code></pre> <p>The first part of that command, with <code>find</code>, descends into the <code>./tmy</code> directory looking for regular files (<code>-type f</code>). It will skip directories and any special files. It then prints the filenames that it finds to the standard output. The reason for <code>-print0</code> as opposed to <code>-print</code> is to correctly handle files with spaces, quotes, or other special characters in their names. It does this by using a <code>NULL</code> character, or 0 as a delimiter. We hope that there are no files with <code>NULL</code> characters in their names. This is possible, but rare and should be corrected before packaging and distribution of the data.</p> <p>The second part of that command, with <code>xargs</code>, reads filenames from its standard input, separated by a <code>NULL</code> character (<code>-0</code>) and runs the <code>sha512sum</code> command on them. The output is then redirected and saved in the <code>SHA512.sums</code> file.</p> <p>There are similar commands for computing the other kinds of checksum that are used in exactly the same way: <code>sha256sum</code>, and even <code>sha1sum</code> or <code>md5sum</code> for situations where those have been used by someone else. Checksums made with <code>sha256sum</code> should be stored in a file called <code>SHA256.sums</code> and similarly with the others.</p>"},{"location":"scraping/webpages/","title":"Scraping Web Pages","text":"<p>There are several ways to scrape web pages. The <code>wget(1)</code> tool is a quick and dirty way but it does not record much metadata. Archival standard copies of web sites is possible by using a tool such as Heretrix from the Internet Archive or Browsertrix. These tools make good archives but are not super helpful for producing browsable copies. For that, the warc2zim tool is helpful. It produces <code>.zim</code> files that can be read by the Kiwix software for offline reading of web pages.</p>"},{"location":"scraping/webpages/#using-zimit","title":"Using zimit","text":"<p>A convenient way to archive web pages, produce WARC files and <code>.zim</code> files is using the Zimit tool which bundles both <code>Browsertrix</code> and <code>warc2zim</code> in a Docker image. Whilst we have opinions about the Docker strategy and the software development patterns that produced it, in this case it is an easy way to get going.</p> <p>The steps are:</p> <ol> <li>Install docker in whatever way your operating system wants you to. Debian or Ubuntu systems might do <code>apt install docker.io</code></li> <li>Obtain the <code>zimit</code> image: <code>docker pull ghcr.io/openzim/zimit</code></li> </ol> <p>Now we assume you are working in a particular directory, say, <code>/home/name/scraping</code> that we will call <code>$SCRAPE</code></p> <p>First run the scrape. We will use the https://maps.org/ web site as an example.</p> <pre><code>docker run \\\n    -v ${SCRAPE}:/output \\\nghcr.io/openzim/zimit zimit \\\n-w 12 \\\n--seeds https://maps.org \\\n--name maps.org-20250310 \\\n--title MAPS \\\n--description \"Multidisciplinary Association for Psychedelic Studies\" \\\n    --scopeExcludeRx '.*add-to-cart=[0-9]*' \\\n--keep\n</code></pre> <p>This needs some explanation.</p> <ul> <li><code>-v ${SCRAPE}:/output</code> says to bind what Docker thinks of as the output directory to the working directory.</li> <li><code>-w 12</code> means to run 12 scraping threads concurrently. On our machine, this is the number of CPU cores.</li> <li><code>--seeds https://maps.org/</code> is the web site to scrape. It is possible to have multiple web sites, comma separated</li> <li><code>--name maps.org-20250310</code> is the filename for the output <code>.zim</code> file</li> <li><code>--title</code> and <code>--description</code> go in the <code>.zim</code> file metadata</li> <li><code>--scopeExcludeRx</code> is a regular expression to exclude certain URLs. Necessary in this case so that the shopping cart section of the web site does not create an infinitely recursive scrape</li> <li><code>--keep</code> causes <code>zimit</code> to keep intermediate files. In particular, it keeps the WARC files which we also want.</li> </ul> <p>Doing this archived the web site but failed at the very end. The reason is yet undiagnosed but we suspect it to have to do with <code>zimit</code>'s management of concurrency. No matter, the WARC files are saved in a temporary directory that starts with <code>.tmp</code> followed by some random characters, in this case <code>.tmptp8i9y5f</code></p> <p>We can work around this by looking in the temporary directory for the WARC files, and running <code>warc2zim</code>:</p> <pre><code>ls .tmptp8i9y5f/collections/crawl-20250310121334268/archive/*.warc.gz | sed s@^@/output/@ &gt; /tmp/scrape.$$\n\ndocker run \\\n    -v ${SCRAPE}:/output \\\nghcr.io/openzim/zimit warc2zim \\\n--name maps.org-20250310 \\\n--title MAPS \\\n--description \"Multidisciplinary Association for Psychedelic Studies\" \\\n--zim-file /output/maps.org-20250310.zim \\\n`cat /tmp/scrape.$$`\n\nrm /tmp/scrape.$$\n</code></pre> <p>Now we can assemble the archive, ready for uploading,</p> <pre><code>mkdir archive\nmv maps.org-20250310.zim archive\nmv .tmptp8i9y5f/collections/crawl-20250310121334268/archive/* archive\nmv .tmptp8i9y5f/collections/crawl-20250310121334268/crawls/* archive\nmv .tmptp8i9y5f/collections/crawl-20250310121334268/pages/* archive\nmv .tmptp8i9y5f/collections/crawl-20250310121334268/warc-cdx/* archive\n</code></pre>"},{"location":"uploading/","title":"Uploading","text":""},{"location":"uploading/trackers/","title":"Trackers","text":"<p>(incomplete stubfile to add a default tracker list so it exists when a failed upload with no trackers directs someone here)</p>"},{"location":"uploading/trackers/#default-trackers","title":"Default Trackers","text":"<p>For public torrents, there is relatively little downside to adding more trackers.</p> <p>Public trackers don't require a torrent to be registered in advance, and will handle coordinating any peers that either declare that they have a torrent or ask for one. </p> <p>It is common to then automatically add trackers to torrents, and include a default list of public trackers to created torrents. Especially privacy-minded people can strip out trackers they don't trust when downloading - the risk of announcing to a tracker is the same as seeding, you are letting others know that you either have or want a torrent.</p> <p>There are a few lists of public trackers that are reasonably accurate and up to date: (note that we do not endorse these sites, nor guarantee security or privacy when visiting them)</p> <ul> <li>https://github.com/ngosang/trackerslist</li> <li>https://newtrackon.com/</li> <li>https://cf.trackerslist.com/best.txt</li> </ul> <p>This set of trackers can be copy/pasted when creating new torrents if you aren't sure which to include.  These are trackers that have been running for a long time and have a good record of uptime, but again due to the nature of bittorrent we can't guarantee privacy when using them:</p> <pre><code>udp://opentracker.io:6969/announce\nudp://tracker.opentrackr.org:1337/announce\nudp://open.demonii.com:1337/announce\nudp://open.stealth.si:80/announce\nudp://tracker.torrent.eu.org:451/announce\nudp://tracker-udp.gbitt.info:80/announce\nudp://explodie.org:6969/announce\nudp://discord.heihachi.pw:6969/announce\nudp://bt.ktrackers.com:6666/announce\n</code></pre>"}]}