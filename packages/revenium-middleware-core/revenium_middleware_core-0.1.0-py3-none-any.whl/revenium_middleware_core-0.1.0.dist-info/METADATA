Metadata-Version: 2.4
Name: revenium-middleware-core
Version: 0.1.0
Summary: A Python library that provides core functionality to send AI metering data to Revenium.
Author-email: Revenium <info@revenium.io>
License: GNU Lesser General Public License v3 or later (LGPLv3+)
Project-URL: Homepage, https://github.com/revenium/revenium-middleware-core-python
Project-URL: Bug Tracker, https://github.com/revenium/revenium-middleware-core-python/issues
Project-URL: Documentation, https://github.com/revenium/revenium-middleware-core-python/blob/main/README.md
Keywords: revenium,middleware,logging,token-usage,metering
Classifier: Development Status :: 4 - Beta
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: revenium_metering
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: freezegun; extra == "dev"

# ğŸ”„ Revenium Core Middleware

[![PyPI version](https://img.shields.io/pypi/v/revenium-middleware-core.svg)](https://pypi.org/project/revenium-middleware-core/)
[![Python Versions](https://img.shields.io/pypi/pyversions/revenium-middleware-core.svg)](https://pypi.org/project/revenium-middleware-core/)
[![License: LGPL v3](https://img.shields.io/badge/License-LGPL%20v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)

A foundational library that provides core metering functionality shared across all Revenium AI provider-specific middleware implementations (OpenAI, Anthropic, Ollama, etc). ğŸâœ¨

## âœ¨ Features

- **ğŸ§  Shared Core Functionality**: Provides the essential metering infrastructure used by all Revenium middleware implementations
- **ğŸ”„ Asynchronous Processing**: Background thread management for non-blocking metering operations
- **ğŸ›‘ Graceful Shutdown**: Ensures all metering data is properly sent even during application shutdown
- **ğŸ”Œ Provider Agnostic**: Designed to work with any AI provider through specific middleware implementations

## ğŸ“¥ Installation

```bash
pip install revenium-middleware-core
```

## ğŸ”§ Usage

### ğŸ”„ Direct Usage

While this package is primarily intended as a dependency for provider-specific middleware, you can use it directly:

```python
from revenium_middleware_core import client, run_async_in_thread, shutdown_event

# Record usage directly
client.record_usage(
    model="gpt-4",
    prompt_tokens=500,
    completion_tokens=200,
    user_id="user123",
    session_id="session456"
)

# Run async metering tasks in background threads
async def async_metering_task():
    await client.async_record_usage(
        model="gpt-3.5-turbo",
        prompt_tokens=300,
        completion_tokens=150,
        user_id="user789"
    )

thread = run_async_in_thread(async_metering_task())

# Application continues while metering happens in background
```

### ğŸ—ï¸ Building Provider-Specific Middleware

This library is designed to be extended by provider-specific middleware implementations:

```python
from revenium_middleware_core import client, run_async_in_thread

# Example of how a provider-specific middleware might use the core
def record_provider_usage(response_data, metadata):
    # Extract token counts from provider-specific response format
    prompt_tokens = response_data.usage.prompt_tokens
    completion_tokens = response_data.usage.completion_tokens
    
    # Use the core client to record the usage
    run_async_in_thread(
        client.async_record_usage(
            model=response_data.model,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            **metadata
        )
    )
```

## ğŸ”„ Compatibility

- ğŸ Python 3.8+
- ğŸ¤ Compatible with all Revenium provider-specific middleware implementations

## ğŸ“š Documentation

For more detailed documentation, please refer to the docstrings in the code or visit our GitHub repository.

## ğŸ‘¥ Contributing

Contributions are welcome! Please check out our contributing guidelines for details.

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create your feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some amazing feature'`)
4. ğŸš€ Push to the branch (`git push origin feature/amazing-feature`)
5. ğŸ” Open a Pull Request

## ğŸ“„ License

This project is licensed under the GNU Lesser General Public License v3.0 (LGPL-3.0) - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- ğŸ’– Built with â¤ï¸ by the Revenium team
